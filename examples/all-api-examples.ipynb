{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import csv, json\n",
    "import time\n",
    "import nucleus_client\n",
    "from nucleus_client.rest import ApiException\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configure API host and key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "configuration = nucleus_client.Configuration()\n",
    "configuration.host = 'UPDATE-WITH-API-HOST'\n",
    "configuration.api_key['x-api-key'] = 'UPDATE-WITH-API-KEY'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create API instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_instance = nucleus_client.DatasetsApi(nucleus_client.ApiClient(configuration))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Append json from csv to dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add documents to dataset\n",
    "csv_file = 'trump_tweets.csv'\n",
    "dataset = 'dataset_from_json'   \n",
    "\n",
    "api_instance = nucleus_client.DatasetsApi(nucleus_client.ApiClient(configuration))\n",
    "doc_cnt = 0\n",
    "with open(csv_file, encoding='utf-8-sig') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for row in reader:\n",
    "        if doc_cnt < 10:\n",
    "            payload = nucleus_client.Appendjsonparams(dataset=dataset, \n",
    "                                                  language='english', \n",
    "                                                  document={'time': row['time'],\n",
    "                                                            'title': row['title'],\n",
    "                                                            'content': row['content']}\n",
    "                                                 )\n",
    "\n",
    "            try:\n",
    "                response = api_instance.post_append_json_to_dataset(payload)\n",
    "            except ApiException as e:\n",
    "                print(\"Exception when calling DatasetsApi->post_append_json_to_dataset: %s\\n\" % e)\n",
    "        \n",
    "        doc_cnt = doc_cnt + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Append file from local drive to dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'quarles20181109a.pdf' # file | \n",
    "dataset = 'dataset_from_file' # str | Destination dataset where the file will be inserted.\n",
    "\n",
    "try:\n",
    "    api_instance.post_upload_file(file, dataset)\n",
    "except ApiException as e:\n",
    "    print(\"Exception when calling DatasetsApi->post_upload_file: %s\\n\" % e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Append file from URL to dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'success': 'https://www.federalreserve.gov/newsevents/speech/files/quarles20181109a.pdf'}\n"
     ]
    }
   ],
   "source": [
    "dataset = 'dataset_from_url'\n",
    "file_url = 'https://www.federalreserve.gov/newsevents/speech/files/quarles20181109a.pdf'\n",
    "payload = nucleus_client.UploadURLModel(\n",
    "                dataset=dataset,\n",
    "                file_url=file_url\n",
    "            ) # UploadURLModel | \n",
    "\n",
    "try:\n",
    "    api_response = api_instance.post_upload_url(payload)\n",
    "    pprint(api_response)\n",
    "except ApiException as e:\n",
    "    print(\"Exception when calling DatasetsApi->post_upload_url: %s\\n\" % e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List available datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'list_datasets': \"['dataset_from_file', 'dataset_from_json', \"\n",
      "                  \"'dataset_from_url', 'dataset_json', 'dataset_test_delete', \"\n",
      "                  \"'trump_tweets_test', 'trump_tweets_test_1108', \"\n",
      "                  \"'trump_tweets_test1', 'trump_tweets_test111', \"\n",
      "                  \"'trump_tweets_test2']\"}\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    api_response = api_instance.get_list_datasets()\n",
    "    pprint(api_response)\n",
    "except ApiException as e:\n",
    "    print(\"Exception when calling DatasetsApi->get_list_datasets: %s\\n\" % e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get dataset information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dataset': 'dataset_from_file',\n",
      " 'detected_language': 'en',\n",
      " 'metadata': '{}',\n",
      " 'num_documents': '1',\n",
      " 'time_range': '[1541823874.0, 1541823874.0]'}\n"
     ]
    }
   ],
   "source": [
    "dataset = 'dataset_from_file' # str | Dataset name.\n",
    "query = '' # str | Fulltext query, using mysql MATCH boolean query format. (optional)\n",
    "metadata_selection = '' # str | json object of {\\\"metadata_field\\\":[\\\"selected_values\\\"]} (optional)\n",
    "time_period = '' # str | Time period selection (optional)\n",
    "\n",
    "try:\n",
    "    api_response = api_instance.get_dataset_info(dataset, \n",
    "                                                 query=query, \n",
    "                                                 metadata_selection=metadata_selection, \n",
    "                                                 time_period=time_period)\n",
    "    pprint(api_response)\n",
    "except ApiException as e:\n",
    "    print(\"Exception when calling DatasetsApi->get_dataset_info: %s\\n\" % e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete a document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exception when calling DatasetsApi->post_delete_document: (500)\n",
      "Reason: Internal Server Error\n",
      "HTTP response headers: HTTPHeaderDict({'Date': 'Fri, 09 Nov 2018 20:24:38 GMT', 'Content-Type': 'application/json', 'Content-Length': '58', 'Connection': 'keep-alive', 'x-amzn-RequestId': '7ad65c00-e45d-11e8-ac7d-c3ace5710c06', 'x-amzn-Remapped-Content-Length': '58', 'x-amz-apigw-id': 'QHH46HP_vHcFXcg=', 'x-amzn-Remapped-Server': 'Werkzeug/0.14.1 Python/3.5.2', 'x-amzn-Remapped-Date': 'Fri, 09 Nov 2018 20:24:38 GMT'})\n",
      "HTTP response body: {\n",
      "    \"message\": \"IndexError : list index out of range\"\n",
      "}\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "payload = nucleus_client.Deletedocumentmodel(dataset=dataset,\n",
    "                                             docid='2') # Deletedocumentmodel | \n",
    "\n",
    "try:\n",
    "    api_response = api_instance.post_delete_document(payload)\n",
    "    pprint(api_response)\n",
    "except ApiException as e:\n",
    "    print(\"Exception when calling DatasetsApi->post_delete_document: %s\\n\" % e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'success': 'Dataset deleted'}\n",
      "{'list_datasets': \"['dataset_from_json', 'dataset_from_url', 'dataset_json', \"\n",
      "                  \"'dataset_test_delete', 'trump_tweets_test', \"\n",
      "                  \"'trump_tweets_test_1108', 'trump_tweets_test1', \"\n",
      "                  \"'trump_tweets_test111', 'trump_tweets_test2']\"}\n"
     ]
    }
   ],
   "source": [
    "payload = nucleus_client.Deletedatasetmodel(dataset=dataset) # Deletedatasetmodel | \n",
    "\n",
    "try:\n",
    "    api_response = api_instance.post_delete_dataset(payload)\n",
    "    pprint(api_response)\n",
    "except ApiException as e:\n",
    "    print(\"Exception when calling DatasetsApi->post_delete_dataset: %s\\n\" % e)\n",
    "    \n",
    "# List datasets again to check if the specified dataset has been deleted\n",
    "try:\n",
    "    api_response = api_instance.get_list_datasets()\n",
    "    pprint(api_response)\n",
    "except ApiException as e:\n",
    "    print(\"Exception when calling DatasetsApi->get_list_datasets: %s\\n\" % e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a full dataset for testing other APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add documents to dataset\n",
    "csv_file = 'trump_tweets.csv'\n",
    "dataset = 'trump_tweets_test'   \n",
    "\n",
    "api_instance = nucleus_client.DatasetsApi(nucleus_client.ApiClient(configuration))\n",
    "\n",
    "with open(csv_file, encoding='utf-8-sig') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for row in reader:\n",
    "        payload = nucleus_client.Appendjsonparams(dataset=dataset, \n",
    "                                                  language='english', \n",
    "                                                  document={'time': row['time'],\n",
    "                                                            'title': row['title'],\n",
    "                                                            'content': row['content']}\n",
    "                                                 )\n",
    "\n",
    "        try:\n",
    "            response = api_instance.post_append_json_to_dataset(payload)\n",
    "        except ApiException as e:\n",
    "            print(\"Exception when calling DatasetsApi->post_append_json_to_dataset: %s\\n\" % e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create API Instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_instance = nucleus_client.TopicsApi(nucleus_client.ApiClient(configuration))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get list of topics from dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'results': [{'doc_topic_exposure': '[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, '\n",
      "                                    '0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, '\n",
      "                                    '0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, '\n",
      "                                    '0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, '\n",
      "                                    '0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, '\n",
      "                                    '0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, '\n",
      "                                    '0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, '\n",
      "                                    '0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, '\n",
      "                                    '0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, '\n",
      "                                    '0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, '\n",
      "                                    '0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, '\n",
      "                                    '0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, '\n",
      "                                    '0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, '\n",
      "                                    '0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, '\n",
      "                                    '0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, '\n",
      "                                    '0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, '\n",
      "                                    '0.0, 0.0, 0.0, 0.0]',\n",
      "              'keywords_weight': '[0.14905536596293506, 0.14796290553543182, '\n",
      "                                 '0.166984715331087, 0.10719940263410926, '\n",
      "                                 '0.10719940263410926, 0.10719940263410926, '\n",
      "                                 '0.10719940263410926, 0.10719940263410926]',\n",
      "              'strength': '0.17198933454298942',\n",
      "              'topic': 'america great;great great;andrew cuomo;taxed '\n",
      "                       'andrew;highest taxed;great believe;governor '\n",
      "                       'highest;believe governor'},\n",
      "             {'doc_topic_exposure': '[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, '\n",
      "                                    '0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, '\n",
      "                                    '0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, '\n",
      "                                    '0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, '\n",
      "                                    '0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, '\n",
      "                                    '0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, '\n",
      "                                    '0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, '\n",
      "                                    '0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, '\n",
      "                                    '0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, '\n",
      "                                    '0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, '\n",
      "                                    '0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, '\n",
      "                                    '0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, '\n",
      "                                    '0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, '\n",
      "                                    '0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, '\n",
      "                                    '0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, '\n",
      "                                    '0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, '\n",
      "                                    '0.0, 0.0, 0.0, 0.0]',\n",
      "              'keywords_weight': '[0.223240222493608, 0.09955039260441553, '\n",
      "                                 '0.1316798600203613, 0.1316798600203613, '\n",
      "                                 '0.1316798600203613, 0.1316798600203613, '\n",
      "                                 '0.0752449724102656, 0.0752449724102656]',\n",
      "              'strength': '0.14958355366606918',\n",
      "              'topic': 'bruce ohr;wife nelly;ohr justice;justice '\n",
      "                       'department;fusion gps;christopher steele;gps '\n",
      "                       'fake;accused helping'},\n",
      "             {'doc_topic_exposure': '[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, '\n",
      "                                    '0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, '\n",
      "                                    '0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, '\n",
      "                                    '0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, '\n",
      "                                    '0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, '\n",
      "                                    '0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, '\n",
      "                                    '0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, '\n",
      "                                    '0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, '\n",
      "                                    '0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, '\n",
      "                                    '0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, '\n",
      "                                    '0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, '\n",
      "                                    '0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, '\n",
      "                                    '0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, '\n",
      "                                    '0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, '\n",
      "                                    '0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, '\n",
      "                                    '0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, '\n",
      "                                    '0.0, 0.0, 0.0, 0.0]',\n",
      "              'keywords_weight': '[0.1111111111111111, 0.1111111111111111, '\n",
      "                                 '0.1111111111111111, 0.1111111111111111, '\n",
      "                                 '0.1111111111111111, 0.1111111111111111, '\n",
      "                                 '0.1111111111111111, 0.2222222222222222]',\n",
      "              'strength': '0.12329207602145682',\n",
      "              'topic': 'illegal aliens;schumer left;permission illegal;legally '\n",
      "                       'chuck;left haywire;enter united;aliens treated;aliens '\n",
      "                       'illegal'},\n",
      "             {'doc_topic_exposure': '[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, '\n",
      "                                    '0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, '\n",
      "                                    '0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, '\n",
      "                                    '0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, '\n",
      "                                    '0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, '\n",
      "                                    '0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, '\n",
      "                                    '0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, '\n",
      "                                    '0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, '\n",
      "                                    '0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, '\n",
      "                                    '0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, '\n",
      "                                    '0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, '\n",
      "                                    '0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, '\n",
      "                                    '0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, '\n",
      "                                    '0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, '\n",
      "                                    '0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, '\n",
      "                                    '0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, '\n",
      "                                    '0.0, 0.0, 0.0, 0.0]',\n",
      "              'keywords_weight': '[0.08643573885666589, 0.10665711016827946, '\n",
      "                                 '0.08933682389603051, 0.21700699319899389, '\n",
      "                                 '0.146659327511078, 0.18103252865562047, '\n",
      "                                 '0.08643573885666589, 0.08643573885666589]',\n",
      "              'strength': '0.14427371626302876',\n",
      "              'topic': 'witch hunt;donald trump;frame donald;hillary '\n",
      "                       'clinton;rigged witch;jarrett foxandfriends;gregg '\n",
      "                       'jarrett;foxandfriends real'},\n",
      "             {'doc_topic_exposure': '[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, '\n",
      "                                    '0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, '\n",
      "                                    '0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, '\n",
      "                                    '0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, '\n",
      "                                    '0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, '\n",
      "                                    '0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, '\n",
      "                                    '0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, '\n",
      "                                    '0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, '\n",
      "                                    '0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, '\n",
      "                                    '0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, '\n",
      "                                    '0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, '\n",
      "                                    '0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, '\n",
      "                                    '0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, '\n",
      "                                    '0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, '\n",
      "                                    '0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, '\n",
      "                                    '0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, '\n",
      "                                    '0.0, 0.0, 0.0, 0.0]',\n",
      "              'keywords_weight': '[0.17978654560850796, 0.13599314314064154, '\n",
      "                                 '0.17978654560850796, 0.08116180500845868, '\n",
      "                                 '0.17978654560850796, 0.08116180500845868, '\n",
      "                                 '0.08116180500845868, 0.08116180500845868]',\n",
      "              'strength': '0.11170492840503222',\n",
      "              'topic': 'peter strzok;fired fbi;agent peter;fbi agent;received '\n",
      "                       'documents;nelly fusion;disgraced fired;department '\n",
      "                       'wife'},\n",
      "             {'doc_topic_exposure': '[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, '\n",
      "                                    '0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, '\n",
      "                                    '0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, '\n",
      "                                    '0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, '\n",
      "                                    '0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, '\n",
      "                                    '0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, '\n",
      "                                    '0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, '\n",
      "                                    '0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, '\n",
      "                                    '0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, '\n",
      "                                    '0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, '\n",
      "                                    '0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, '\n",
      "                                    '0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, '\n",
      "                                    '0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, '\n",
      "                                    '0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, '\n",
      "                                    '0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, '\n",
      "                                    '0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, '\n",
      "                                    '0.0, 0.0, 0.0, 0.0]',\n",
      "              'keywords_weight': '[0.18180528180141983, 0.14262476425556084, '\n",
      "                                 '0.13720328827832373, 0.10767333313293909, '\n",
      "                                 '0.10767333313293909, 0.10767333313293909, '\n",
      "                                 '0.10767333313293909, 0.10767333313293909]',\n",
      "              'strength': '0.09898125859272065',\n",
      "              'topic': 'fake news;news media;wacky omarosa;presidential '\n",
      "                       'lowlife;overtime wacky;omarosa legitimate;form '\n",
      "                       'communication;communication fake'},\n",
      "             {'doc_topic_exposure': '[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, '\n",
      "                                    '0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, '\n",
      "                                    '0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, '\n",
      "                                    '0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, '\n",
      "                                    '0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, '\n",
      "                                    '0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, '\n",
      "                                    '0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, '\n",
      "                                    '0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, '\n",
      "                                    '0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, '\n",
      "                                    '0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, '\n",
      "                                    '0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, '\n",
      "                                    '0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, '\n",
      "                                    '0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, '\n",
      "                                    '0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, '\n",
      "                                    '0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, '\n",
      "                                    '0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, '\n",
      "                                    '0.0, 0.0, 0.0, 0.0]',\n",
      "              'keywords_weight': '[0.14009766684753122, 0.1228431904503527, '\n",
      "                                 '0.1228431904503527, 0.1228431904503527, '\n",
      "                                 '0.1228431904503527, 0.1228431904503527, '\n",
      "                                 '0.1228431904503527, 0.1228431904503527]',\n",
      "              'strength': '0.11500773399233298',\n",
      "              'topic': 'trump campaign;lou dobbs;dobbs forward;democrats '\n",
      "                       'evidence;conflicts angry;collusion trump;campaign '\n",
      "                       'russia;angry democrats'},\n",
      "             {'doc_topic_exposure': '[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, '\n",
      "                                    '0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, '\n",
      "                                    '0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, '\n",
      "                                    '0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, '\n",
      "                                    '0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, '\n",
      "                                    '0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, '\n",
      "                                    '0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, '\n",
      "                                    '0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, '\n",
      "                                    '0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, '\n",
      "                                    '0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, '\n",
      "                                    '0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, '\n",
      "                                    '0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, '\n",
      "                                    '0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, '\n",
      "                                    '0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, '\n",
      "                                    '0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, '\n",
      "                                    '0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, '\n",
      "                                    '0.0, 0.0, 0.0, 0.0]',\n",
      "              'keywords_weight': '[0.16996717050785298, 0.02582823898072689, '\n",
      "                                 '0.17431650476619812, 0.12597761714904437, '\n",
      "                                 '0.12597761714904437, 0.12597761714904437, '\n",
      "                                 '0.12597761714904437, 0.12597761714904437]',\n",
      "              'strength': '0.08516739851637',\n",
      "              'topic': 'strzok firing;judicial watch;watch strzok;tom '\n",
      "                       'fitton;mueller operation;fitton judicial;firing '\n",
      "                       'mueller;assistant director'}]}\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset\n",
    "#query = '(\"Trump\" OR \"president\")' # str | Fulltext query, using mysql MATCH boolean query format. Example, (\\\"word1\\\" OR \\\"word2\\\") AND (\\\"word3\\\" OR \\\"word4\\\") (optional)\n",
    "query = ''\n",
    "custom_stop_words = [\"real\",\"hillary\"] # ERRORUNKNOWN | List of stop words. (optional)\n",
    "num_topics = 8 # int | Number of topics to be extracted from the dataset. (optional) (default to 8)\n",
    "metadata_selection =\"\" # str | json object of {\\\"metadata_field\\\":[\\\"selected_values\\\"]} (optional)\n",
    "time_period =\"\"# str | Time period selection (optional)\n",
    "\n",
    "try:\n",
    "    api_response = api_instance.get_topic_api(dataset, \n",
    "                                              query=query, \n",
    "                                              custom_stop_words=custom_stop_words, \n",
    "                                              num_topics=num_topics, \n",
    "                                              metadata_selection=metadata_selection,\n",
    "                                              time_period=time_period)\n",
    "    pprint(api_response)\n",
    "except ApiException as e:\n",
    "    print(\"Exception when calling TopicsApi->get_topic_api: %s\\n\" % e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get topic summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'results': [{'summary': \"[{'title': 'D_Trump2018_8_17_19_25', 'sourceid': 16, \"\n",
      "                         \"'attribute': {'time': 1534559100.0, 'counts': None}, \"\n",
      "                         '\\'sentences\\': [\"Which is worse Hightax Andrew '\n",
      "                         \"Cuomo's statement “WE’RE NOT GOING TO MAKE AMERICA \"\n",
      "                         'GREAT AGAIN IT WAS NEVER THAT GREAT” or Hillary '\n",
      "                         'Clinton’s “DEPLORABLES” statement...\"]}, {\\'title\\': '\n",
      "                         \"'D_Trump2018_8_17_11_44', 'sourceid': 24, \"\n",
      "                         \"'attribute': {'time': 1534531440.0, 'counts': None}, \"\n",
      "                         \"'sentences': ['How does a politician Cuomo known for \"\n",
      "                         'pushing people and businesses out of his state not '\n",
      "                         'to mention having the highest taxes in the U.S. '\n",
      "                         'survive making the statement WE’RE NOT GOING TO MAKE '\n",
      "                         \"AMERICA GREAT AGAIN IT WAS NEVER THAT GREAT?']}, \"\n",
      "                         \"{'title': 'D_Trump2018_8_16_2_2', 'sourceid': 44, \"\n",
      "                         \"'attribute': {'time': 1534410120.0, 'counts': None}, \"\n",
      "                         \"'sentences': ['“WE’RE NOT GOING TO MAKE AMERICA \"\n",
      "                         'GREAT AGAIN IT WAS NEVER THAT GREAT.” Can you '\n",
      "                         'believe this is the Governor of the Highest Taxed '\n",
      "                         'State in the U.S. Andrew Cuomo having a total '\n",
      "                         \"meltdown!']}, {'title': 'D_Trump2018_8_16_1_53', \"\n",
      "                         \"'sourceid': 45, 'attribute': {'time': 1534409580.0, \"\n",
      "                         \"'counts': None}, 'sentences': ['“WE’RE NOT GONG TO \"\n",
      "                         'MAKE AMERICA GREAT AGAIN IT WAS NEVER THAT GREAT.”  '\n",
      "                         'Can you believe this is the Governor of the Highest '\n",
      "                         'Taxed State in the U.S. Andrew Cuomo having a total '\n",
      "                         \"meltdown!']}]\",\n",
      "              'topic': 'america great;great great;andrew cuomo;taxed '\n",
      "                       'andrew;highest taxed;great believe;governor '\n",
      "                       'highest;believe governor'},\n",
      "             {'summary': \"[{'title': 'D_Trump2018_8_17_22_29', 'sourceid': 13, \"\n",
      "                         \"'attribute': {'time': 1534570140.0, 'counts': None}, \"\n",
      "                         \"'sentences': ['“Fox News has learned that Bruce Ohr \"\n",
      "                         'wrote Christopher Steele following the firing of '\n",
      "                         'James Comey saying that he was afraid the anti-Trump '\n",
      "                         'Russia probe will be exposed.”  Charles Payne  '\n",
      "                         '@FoxBusiness   How much more does Mueller have to '\n",
      "                         \"see?']}, {'title': 'D_Trump2018_8_16_23_37', \"\n",
      "                         \"'sourceid': 33, 'attribute': {'time': 1534487820.0, \"\n",
      "                         \"'counts': None}, 'sentences': ['“The FBI received \"\n",
      "                         'documents from Bruce Ohr (of the Justice Department  '\n",
      "                         'whose wife Nelly worked for Fusion GPS).” Disgraced '\n",
      "                         \"and fired FBI Agent Peter Strzok.']}, {'title': \"\n",
      "                         \"'D_Trump2018_8_14_11_55', 'sourceid': 70, \"\n",
      "                         \"'attribute': {'time': 1534272900.0, 'counts': None}, \"\n",
      "                         \"'sentences': ['Bruce Ohr of the “Justice” Department \"\n",
      "                         '(can you believe he is still there) is accused of '\n",
      "                         'helping disgraced Christopher Steele “find dirt on '\n",
      "                         'Trump.” Ohr’s wife Nelly was in on the act big time '\n",
      "                         \"- worked for Fusion GPS on Fake Dossier.']}]\",\n",
      "              'topic': 'bruce ohr;wife nelly;ohr justice;justice '\n",
      "                       'department;fusion gps;christopher steele;gps '\n",
      "                       'fake;accused helping'},\n",
      "             {'summary': \"[{'title': 'D_Trump2018_8_15_12_44', 'sourceid': 63, \"\n",
      "                         \"'attribute': {'time': 1534362240.0, 'counts': None}, \"\n",
      "                         \"'sentences': ['“People who enter the United States \"\n",
      "                         'without our permission are illegal aliens and '\n",
      "                         'illegal aliens should not be treated the same as '\n",
      "                         'people who enters the U.S. legally.”  Chuck Schumer '\n",
      "                         \"in 2009 before he went left and haywire!']}]\",\n",
      "              'topic': 'illegal aliens;schumer left;permission illegal;legally '\n",
      "                       'chuck;left haywire;enter united;aliens treated;aliens '\n",
      "                       'illegal'},\n",
      "             {'summary': \"[{'title': 'D_Trump2018_8_16_1_14', 'sourceid': 46, \"\n",
      "                         \"'attribute': {'time': 1534407240.0, 'counts': None}, \"\n",
      "                         \"'sentences': ['We have the unfortunate situation \"\n",
      "                         'where they then decided they were going to frame '\n",
      "                         \"Donald Trump” concerning the Rigged Witch Hunt.']}, \"\n",
      "                         \"{'title': 'D_Trump2018_8_14_13_10', 'sourceid': 67, \"\n",
      "                         \"'attribute': {'time': 1534277400.0, 'counts': None}, \"\n",
      "                         \"'sentences': ['Strzok started the illegal Rigged \"\n",
      "                         'Witch Hunt - why isn’t this so-called “probe” ended '\n",
      "                         \"immediately?']}, {'title': 'D_Trump2018_8_14_12_6', \"\n",
      "                         \"'sourceid': 69, 'attribute': {'time': 1534273560.0, \"\n",
      "                         \"'counts': None}, 'sentences': ['“They were all in on \"\n",
      "                         'it clear Hillary Clinton and FRAME Donald Trump for '\n",
      "                         'things he didn’t do.” Gregg Jarrett on '\n",
      "                         '@foxandfriends  If we had a real Attorney General '\n",
      "                         \"this Witch Hunt would never have been started!']}]\",\n",
      "              'topic': 'witch hunt;donald trump;frame donald;hillary '\n",
      "                       'clinton;rigged witch;jarrett foxandfriends;gregg '\n",
      "                       'jarrett;foxandfriends real'},\n",
      "             {'summary': \"[{'title': 'D_Trump2018_8_16_23_37', 'sourceid': 33, \"\n",
      "                         \"'attribute': {'time': 1534487820.0, 'counts': None}, \"\n",
      "                         \"'sentences': ['“The FBI received documents from \"\n",
      "                         'Bruce Ohr (of the Justice Department  whose wife '\n",
      "                         'Nelly worked for Fusion GPS).” Disgraced and fired '\n",
      "                         \"FBI Agent Peter Strzok.']}, {'title': \"\n",
      "                         \"'D_Trump2018_8_14_13_1', 'sourceid': 68, \"\n",
      "                         \"'attribute': {'time': 1534276860.0, 'counts': None}, \"\n",
      "                         \"'sentences': ['Fired FBI Agent Peter Strzok is a \"\n",
      "                         \"fraud as is the rigged investigation he started.']}, \"\n",
      "                         \"{'title': 'D_Trump2018_8_13_16_4', 'sourceid': 91, \"\n",
      "                         \"'attribute': {'time': 1534201440.0, 'counts': None}, \"\n",
      "                         \"'sentences': ['Agent Peter Strzok was just fired \"\n",
      "                         \"from the FBI - finally.']}]\",\n",
      "              'topic': 'peter strzok;fired fbi;agent peter;fbi agent;received '\n",
      "                       'documents;nelly fusion;disgraced fired;department '\n",
      "                       'wife'},\n",
      "             {'summary': \"[{'title': 'D_Trump2018_8_13_14_21', 'sourceid': 93, \"\n",
      "                         \"'attribute': {'time': 1534195260.0, 'counts': None}, \"\n",
      "                         \"'sentences': ['While I know it’s “not presidential” \"\n",
      "                         'to take on a lowlife like Omarosa and while I would '\n",
      "                         'rather not be doing so this is a modern day form of '\n",
      "                         'communication and I know the Fake News Media will be '\n",
      "                         'working overtime to make even Wacky Omarosa look '\n",
      "                         \"legitimate as possible.']}]\",\n",
      "              'topic': 'fake news;news media;wacky omarosa;presidential '\n",
      "                       'lowlife;overtime wacky;omarosa legitimate;form '\n",
      "                       'communication;communication fake'},\n",
      "             {'summary': \"[{'title': 'D_Trump2018_8_14_13_15', 'sourceid': 66, \"\n",
      "                         \"'attribute': {'time': 1534277700.0, 'counts': None}, \"\n",
      "                         \"'sentences': ['Lou Dobbs: “This cannot go \"\n",
      "                         'forward...this Special Counsel with all of his '\n",
      "                         'conflicts with his 17 Angry Democrats without any '\n",
      "                         'evidence of collusion by the Trump Campaign and '\n",
      "                         \"Russia.']}, {'title': 'D_Trump2018_8_14_11_21', \"\n",
      "                         \"'sourceid': 73, 'attribute': {'time': 1534270860.0, \"\n",
      "                         \"'counts': None}, 'sentences': ['Lou Dobbs: “This \"\n",
      "                         'cannot go forward...this Special Councel with all of '\n",
      "                         'his conflicts with his 17 Angry Democrats without '\n",
      "                         'any evidence of collusion by the Trump Campaign and '\n",
      "                         \"Russia.']}]\",\n",
      "              'topic': 'trump campaign;lou dobbs;dobbs forward;democrats '\n",
      "                       'evidence;conflicts angry;collusion trump;campaign '\n",
      "                       'russia;angry democrats'},\n",
      "             {'summary': \"[{'title': 'D_Trump2018_8_15_14_15', 'sourceid': 52, \"\n",
      "                         \"'attribute': {'time': 1534367700.0, 'counts': None}, \"\n",
      "                         \"'sentences': ['“The action (the Strzok firing) was a \"\n",
      "                         'decisive step in the right direction in correcting '\n",
      "                         'the wrongs committed by what has been described as '\n",
      "                         'Comey’s skinny inner circle.”  Chris Swecker former '\n",
      "                         \"FBI Assistant Director.']}, {'title': \"\n",
      "                         \"'D_Trump2018_8_14_10_59', 'sourceid': 76, \"\n",
      "                         \"'attribute': {'time': 1534269540.0, 'counts': None}, \"\n",
      "                         \"'sentences': ['Tom Fitton of Judicial Watch: “The \"\n",
      "                         'Strzok firing is as much about the Mueller operation '\n",
      "                         \"as anything else.']}]\",\n",
      "              'topic': 'strzok firing;judicial watch;watch strzok;tom '\n",
      "                       'fitton;mueller operation;fitton judicial;firing '\n",
      "                       'mueller;assistant director'}]}\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset # str | Dataset name.\n",
    "#query = '(\"Trump\" OR \"president\")' # str | Fulltext query, using mysql MATCH boolean query format. Example, (\\\"word1\\\" OR \\\"word2\\\") AND (\\\"word3\\\" OR \\\"word4\\\") (optional)\n",
    "query = ''\n",
    "custom_stop_words = [\"real\",\"hillary\"] # ERRORUNKNOWN | List of stop words. (optional)\n",
    "num_topics = 8 # int | Number of topics to be extracted from the dataset. (optional) (default to 8)\n",
    "num_keywords = 8 # int | Number of keywords per topic that is extracted from the dataset. (optional) (default to 8)\n",
    "summary_length = 6 # int | The maximum number of bullet points a user wants to see in each topic summary. (optional) (default to 6)\n",
    "context_amount = 0 # int | The number of sentences surrounding key summary sentences in the documents that they come from. (optional) (default to 0)\n",
    "num_docs = 20 # int | The maximum number of key documents to use for summarization. (optional) (default to 20)\n",
    "excluded_docs = '' # str | List of document IDs that should be excluded from the analysis. Example, \\\"docid1, docid2, ..., docidN\\\"  (optional)\n",
    "\n",
    "try:\n",
    "    api_response = api_instance.get_topic_summary_api(dataset, \n",
    "                                                      query=query, \n",
    "                                                      custom_stop_words=custom_stop_words, \n",
    "                                                      num_topics=num_topics, \n",
    "                                                      num_keywords=num_keywords, \n",
    "                                                      summary_length=summary_length, \n",
    "                                                      context_amount=context_amount, \n",
    "                                                      num_docs=num_docs)\n",
    "    pprint(api_response)\n",
    "except ApiException as e:\n",
    "    print(\"Exception when calling TopicsApi->get_topic_summary_api: %s\\n\" % e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get topic sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'results': [{'document_scores': '[[0.13842981263820506, 0.080463311097383, '\n",
      "                                 '0.07558211992548376, 0.09428682676944669, '\n",
      "                                 '0.31289973656753545, 0.2983381930019461]]',\n",
      "              'document_sentiments': '[0.2727272727272727, '\n",
      "                                     '-0.2666666666666667, '\n",
      "                                     '0.11764705882352941, '\n",
      "                                     '0.45454545454545453, 0.6, '\n",
      "                                     '0.5454545454545454]',\n",
      "              'sentiment': '0.41851613034223817',\n",
      "              'strength': '0.17198933454298942',\n",
      "              'topic': 'america great;great great;andrew cuomo;taxed '\n",
      "                       'andrew;highest taxed;great believe;governor '\n",
      "                       'highest;believe governor'},\n",
      "             {'document_scores': '[[0.11735514798169955, 0.11735514798169955, '\n",
      "                                 '0.0929418313169472, 0.1355100525560452, '\n",
      "                                 '0.24142555452081566, 0.2954122656427928]]',\n",
      "              'document_sentiments': '[-0.3333333333333333, '\n",
      "                                     '-0.3333333333333333, -0.1, -0.3125, '\n",
      "                                     '-0.25, -0.3157894736842105]',\n",
      "              'sentiment': '-0.2835223123939935',\n",
      "              'strength': '0.14958355366606918',\n",
      "              'topic': 'bruce ohr;wife nelly;ohr justice;justice '\n",
      "                       'department;fusion gps;christopher steele;gps '\n",
      "                       'fake;accused helping'},\n",
      "             {'document_scores': '[[0.5, 0.5]]',\n",
      "              'document_sentiments': '[-0.46153846153846156, '\n",
      "                                     '-0.46153846153846156]',\n",
      "              'sentiment': '-0.46153846153846156',\n",
      "              'strength': '0.12329207602145682',\n",
      "              'topic': 'illegal aliens;schumer left;permission illegal;legally '\n",
      "                       'chuck;left haywire;enter united;aliens treated;aliens '\n",
      "                       'illegal'},\n",
      "             {'document_scores': '[[0.034322258755361566, 0.23924952894578164, '\n",
      "                                 '0.06685986536565615, 0.12357837799645073, '\n",
      "                                 '0.2805753697160322, 0.03794468474967494, '\n",
      "                                 '0.09455412837855011, 0.07302809786729958, '\n",
      "                                 '0.049887688225193204]]',\n",
      "              'document_sentiments': '[0.2727272727272727, -0.375, '\n",
      "                                     '-0.8333333333333334, '\n",
      "                                     '-0.8571428571428571, '\n",
      "                                     '-0.3333333333333333, '\n",
      "                                     '-0.2222222222222222, '\n",
      "                                     '-0.16666666666666666, '\n",
      "                                     '-0.2857142857142857, -0.4]',\n",
      "              'sentiment': '-0.4005353787098191',\n",
      "              'strength': '0.14427371626302876',\n",
      "              'topic': 'witch hunt;donald trump;frame donald;hillary '\n",
      "                       'clinton;rigged witch;jarrett foxandfriends;gregg '\n",
      "                       'jarrett;foxandfriends real'},\n",
      "             {'document_scores': '[[0.34247409118819916, 0.32709462800260947, '\n",
      "                                 '0.3304312808091913]]',\n",
      "              'document_sentiments': '[-0.25, -0.625, -0.4]',\n",
      "              'sentiment': '-0.42222517762235723',\n",
      "              'strength': '0.11170492840503222',\n",
      "              'topic': 'peter strzok;fired fbi;agent peter;fbi agent;received '\n",
      "                       'documents;nelly fusion;disgraced fired;department '\n",
      "                       'wife'},\n",
      "             {'document_scores': '[[0.09415980058720225, 0.09415980058720225, '\n",
      "                                 '0.23762662508220356, 0.10049363411313801, '\n",
      "                                 '0.3915074309718749, 0.08205270865837908]]',\n",
      "              'document_sentiments': '[-0.5, -0.375, -0.5, -0.5, '\n",
      "                                     '0.2857142857142857, -0.6666666666666666]',\n",
      "              'sentiment': '-0.19429249489176126',\n",
      "              'strength': '0.09898125859272065',\n",
      "              'topic': 'fake news;news media;wacky omarosa;presidential '\n",
      "                       'lowlife;overtime wacky;omarosa legitimate;form '\n",
      "                       'communication;communication fake'},\n",
      "             {'document_scores': '[[0.07525269365611574, 0.04759398240324591, '\n",
      "                                 '0.4385766619703192, 0.4385766619703192]]',\n",
      "              'document_sentiments': '[-0.5, -0.3, -0.3333333333333333, '\n",
      "                                     '-0.16666666666666666]',\n",
      "              'sentiment': '-0.2711928725341912',\n",
      "              'strength': '0.11500773399233298',\n",
      "              'topic': 'trump campaign;lou dobbs;dobbs forward;democrats '\n",
      "                       'evidence;conflicts angry;collusion trump;campaign '\n",
      "                       'russia;angry democrats'},\n",
      "             {'document_scores': '[[0.1280441131104907, 0.08428761623610356, '\n",
      "                                 '0.16741924586562784, 0.620249024787778]]',\n",
      "              'document_sentiments': '[-0.4, -0.375, -0.75, '\n",
      "                                     '-0.5714285714285714]',\n",
      "              'sentiment': '-0.5628179498964005',\n",
      "              'strength': '0.08516739851637',\n",
      "              'topic': 'strzok firing;judicial watch;watch strzok;tom '\n",
      "                       'fitton;mueller operation;fitton judicial;firing '\n",
      "                       'mueller;assistant director'}]}\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset # str | Dataset name.\n",
    "#query = '(\"Trump\" OR \"president\")' # str | Fulltext query, using mysql MATCH boolean query format. Example, (\\\"word1\\\" OR \\\"word2\\\") AND (\\\"word3\\\" OR \\\"word4\\\") (optional)\n",
    "query = ''\n",
    "custom_stop_words = [\"real\",\"hillary\"] # ERRORUNKNOWN | List of stop words. (optional)\n",
    "num_topics = 8 # int | Number of topics to be extracted from the dataset. (optional) (default to 8)\n",
    "num_keywords = 8 # int | Number of keywords per topic that is extracted from the dataset. (optional) (default to 8)\n",
    "excluded_docs = '' # str | List of document IDs that should be excluded from the analysis. Example, \\\"docid1, docid2, ..., docidN\\\"  (optional)\n",
    "\n",
    "try:\n",
    "    api_response = api_instance.get_topic_sentiment_api(dataset, \n",
    "                                                        query=query, \n",
    "                                                        custom_stop_words=custom_stop_words, \n",
    "                                                        num_topics=num_topics, \n",
    "                                                        num_keywords=num_keywords)\n",
    "    pprint(api_response)\n",
    "except ApiException as e:\n",
    "    print(\"Exception when calling TopicsApi->get_topic_sentiment_api: %s\\n\" % e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get topic consensus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'results': [{'consensus': '1.0',\n",
      "              'strength': '0.18791715552843313',\n",
      "              'topic': 'trump campaign;lou dobbs;democrats evidence;conflicts '\n",
      "                       'angry;collusion trump;campaign russia;angry democrats'},\n",
      "             {'consensus': '1.0',\n",
      "              'strength': '0.16169879505464493',\n",
      "              'topic': 'donald trump;witch hunt;frame donald;unfortunate '\n",
      "                       'situation;situation decided;rigged witch;decided '\n",
      "                       'frame'},\n",
      "             {'consensus': '1.0',\n",
      "              'strength': '0.12934265999548608',\n",
      "              'topic': 'forward special;evidence collusion;dobbs '\n",
      "                       'forward;special counsel;counsel conflicts;special '\n",
      "                       'councel;councel conflicts'},\n",
      "             {'consensus': '1.0',\n",
      "              'strength': '0.13715894403433687',\n",
      "              'topic': 'bruce ohr;christopher steele;fake dossier;time '\n",
      "                       'fusion;helping disgraced;gps fake;disgraced '\n",
      "                       'christopher'},\n",
      "             {'consensus': '1.0',\n",
      "              'strength': '0.08083916249717878',\n",
      "              'topic': 'trump presidency;scandal american;responsible '\n",
      "                       'greatest;mark levin;interfering election;history '\n",
      "                       'interfering;chinese north'},\n",
      "             {'consensus': '1.0',\n",
      "              'strength': '0.1010489531214735',\n",
      "              'topic': 'trade deals;unfair trade;tariffs leading;opposed '\n",
      "                       'horrible;leading great;deals opposed;built tariffs'},\n",
      "             {'consensus': '1.0',\n",
      "              'strength': '0.08083916249717878',\n",
      "              'topic': 'trump dossier;phony discredited;notes bruce;emails '\n",
      "                       'notes;doj emails;discredited trump;connection phony'},\n",
      "             {'consensus': '1.0',\n",
      "              'strength': '0.12115516727126796',\n",
      "              'topic': 'heard heard;heard trump;claims heard;undermine '\n",
      "                       'trump;russians chinese;levin power;greatest scandal'}]}\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset # str | Dataset name.\n",
    "query = '(\"Trump\" OR \"president\")' # str | Fulltext query, using mysql MATCH boolean query format. Example, (\\\"word1\\\" OR \\\"word2\\\") AND (\\\"word3\\\" OR \\\"word4\\\") (optional)\n",
    "custom_stop_words = [\"real\",\"hillary\"] # ERRORUNKNOWN | List of stop words. (optional)\n",
    "num_topics = 8 # int | Number of topics to be extracted from the dataset. (optional) (default to 8)\n",
    "num_keywords = 8 # int | Number of keywords per topic that is extracted from the dataset. (optional) (default to 8)\n",
    "excluded_docs = '' # str | List of document IDs that should be excluded from the analysis. Example, \\\"docid1, docid2, ..., docidN\\\"  (optional)\n",
    "\n",
    "try:\n",
    "    api_response = api_instance.get_topic_consensus_api(dataset, \n",
    "                                                        query=query, \n",
    "                                                        custom_stop_words=custom_stop_words, \n",
    "                                                        num_topics=num_topics, \n",
    "                                                        num_keywords=num_keywords)\n",
    "    pprint(api_response)\n",
    "except ApiException as e:\n",
    "    print(\"Exception when calling TopicsApi->get_topic_consensus_api: %s\\n\" % e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create API instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_instance = nucleus_client.DocumentsApi(nucleus_client.ApiClient(configuration))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get document information without content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'results': [{'attribute': {'author': None,\n",
      "                            'source': None,\n",
      "                            'time': '1534582020.0'},\n",
      "              'sourceid': '10',\n",
      "              'title': 'D_Trump2018_8_18_1_47'},\n",
      "             {'attribute': {'author': None,\n",
      "                            'source': None,\n",
      "                            'time': '1534581960.0'},\n",
      "              'sourceid': '11',\n",
      "              'title': 'D_Trump2018_8_18_1_46'},\n",
      "             {'attribute': {'author': None,\n",
      "                            'source': None,\n",
      "                            'time': '1534581420.0'},\n",
      "              'sourceid': '12',\n",
      "              'title': 'D_Trump2018_8_18_1_37'},\n",
      "             {'attribute': {'author': None,\n",
      "                            'source': None,\n",
      "                            'time': '1534534680.0'},\n",
      "              'sourceid': '20',\n",
      "              'title': 'D_Trump2018_8_17_12_38'}]}\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset # str | Dataset name.\n",
    "doc_titles = ['D_Trump2018_8_18_1_47']   # str | The title of the document to retrieve. Example: \\\" \\\"title 1\\\" \\\"  (optional)\n",
    "doc_ids = ['11', '12', '20']      # int | The docid of the document to retrieve. Example: \\\"docid1\\\"  (optional)\n",
    "\n",
    "try:\n",
    "    api_response = api_instance.get_doc_info(dataset, doc_titles=doc_titles, doc_ids=doc_ids)\n",
    "    pprint(api_response)\n",
    "except ApiException as e:\n",
    "    print(\"Exception when calling DocumentsApi->get_doc_info: %s\\n\" % e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display document details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'results': [{'attribute': {'author': None,\n",
      "                            'source': None,\n",
      "                            'time': '1534582020.0'},\n",
      "              'content': ' financial gain is a Federal Gratuity Statute '\n",
      "                         'Violation Bribery Statute Violation Honest Services '\n",
      "                         'Violation all Major Crimes because the DOJ is run by '\n",
      "                         'BLANK Jeff Sessions ”  Gregg Jarrett. So when does '\n",
      "                         'Mueller do what must be done? Probably never! '\n",
      "                         '@FoxNews',\n",
      "              'sourceid': '10',\n",
      "              'title': 'D_Trump2018_8_18_1_47'},\n",
      "             {'attribute': {'author': None,\n",
      "                            'source': None,\n",
      "                            'time': '1534581960.0'},\n",
      "              'content': '“Bruce Ohr of DOJ is in legal jeopardy it’s '\n",
      "                         'astonishing that he’s still employed. Bruce  Nelly '\n",
      "                         'Ohr’s bank account is getting fatter  fatter because '\n",
      "                         'of the Dossier that they are both peddling. He '\n",
      "                         'doesn’t disclose it under Fed Regs. Using your '\n",
      "                         'Federal office for personal ',\n",
      "              'sourceid': '11',\n",
      "              'title': 'D_Trump2018_8_18_1_46'}]}\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset # str | Dataset name.\n",
    "doc_titles = ['D_Trump2018_8_18_1_47']   # str | The title of the document to retrieve. Example: \\\" \\\"title 1\\\" \\\"  (optional)\n",
    "doc_ids = ['11']      # int | The docid of the document to retrieve. Example: \\\"docid1\\\"  (optional)\n",
    "\n",
    "try:\n",
    "    api_response = api_instance.get_doc_display(dataset, doc_titles=doc_titles, doc_ids=doc_ids)\n",
    "    pprint(api_response)\n",
    "except ApiException as e:\n",
    "    print(\"Exception when calling DocumentsApi->get_doc_display_api: %s\\n\" % e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get document recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'results': [{'recommendations': [{'attribute': {'author': None,\n",
      "                                                 'source': None,\n",
      "                                                 'time': '1534270860.0'},\n",
      "                                   'sourceid': '73',\n",
      "                                   'title': 'D_Trump2018_8_14_11_21'},\n",
      "                                  {'attribute': {'author': None,\n",
      "                                                 'source': None,\n",
      "                                                 'time': '1534277700.0'},\n",
      "                                   'sourceid': '66',\n",
      "                                   'title': 'D_Trump2018_8_14_13_15'}],\n",
      "              'topic': 'trump campaign;lou dobbs;democrats evidence;conflicts '\n",
      "                       'angry;collusion trump;campaign russia;angry democrats'},\n",
      "             {'recommendations': [{'attribute': {'author': None,\n",
      "                                                 'source': None,\n",
      "                                                 'time': '1534273560.0'},\n",
      "                                   'sourceid': '69',\n",
      "                                   'title': 'D_Trump2018_8_14_12_6'},\n",
      "                                  {'attribute': {'author': None,\n",
      "                                                 'source': None,\n",
      "                                                 'time': '1534407240.0'},\n",
      "                                   'sourceid': '46',\n",
      "                                   'title': 'D_Trump2018_8_16_1_14'}],\n",
      "              'topic': 'donald trump;witch hunt;frame donald;unfortunate '\n",
      "                       'situation;situation decided;rigged witch;decided '\n",
      "                       'frame'},\n",
      "             {'recommendations': [{'attribute': {'author': None,\n",
      "                                                 'source': None,\n",
      "                                                 'time': '1534270860.0'},\n",
      "                                   'sourceid': '73',\n",
      "                                   'title': 'D_Trump2018_8_14_11_21'},\n",
      "                                  {'attribute': {'author': None,\n",
      "                                                 'source': None,\n",
      "                                                 'time': '1534277700.0'},\n",
      "                                   'sourceid': '66',\n",
      "                                   'title': 'D_Trump2018_8_14_13_15'}],\n",
      "              'topic': 'forward special;evidence collusion;dobbs '\n",
      "                       'forward;special counsel;counsel conflicts;special '\n",
      "                       'councel;councel conflicts'},\n",
      "             {'recommendations': [{'attribute': {'author': None,\n",
      "                                                 'source': None,\n",
      "                                                 'time': '1534570140.0'},\n",
      "                                   'sourceid': '13',\n",
      "                                   'title': 'D_Trump2018_8_17_22_29'},\n",
      "                                  {'attribute': {'author': None,\n",
      "                                                 'source': None,\n",
      "                                                 'time': '1534272900.0'},\n",
      "                                   'sourceid': '70',\n",
      "                                   'title': 'D_Trump2018_8_14_11_55'}],\n",
      "              'topic': 'bruce ohr;christopher steele;fake dossier;time '\n",
      "                       'fusion;helping disgraced;gps fake;disgraced '\n",
      "                       'christopher'},\n",
      "             {'recommendations': [{'attribute': {'author': None,\n",
      "                                                 'source': None,\n",
      "                                                 'time': '1534411860.0'},\n",
      "                                   'sourceid': '43',\n",
      "                                   'title': 'D_Trump2018_8_16_2_31'}],\n",
      "              'topic': 'trump presidency;scandal american;responsible '\n",
      "                       'greatest;mark levin;interfering election;history '\n",
      "                       'interfering;chinese north'},\n",
      "             {'recommendations': [{'attribute': {'author': None,\n",
      "                                                 'source': None,\n",
      "                                                 'time': '1534370640.0'},\n",
      "                                   'sourceid': '50',\n",
      "                                   'title': 'D_Trump2018_8_15_15_4'}],\n",
      "              'topic': 'trade deals;unfair trade;tariffs leading;opposed '\n",
      "                       'horrible;leading great;deals opposed;built tariffs'},\n",
      "             {'recommendations': [{'attribute': {'author': None,\n",
      "                                                 'source': None,\n",
      "                                                 'time': '1534488780.0'},\n",
      "                                   'sourceid': '31',\n",
      "                                   'title': 'D_Trump2018_8_16_23_53'}],\n",
      "              'topic': 'trump dossier;phony discredited;notes bruce;emails '\n",
      "                       'notes;doj emails;discredited trump;connection phony'},\n",
      "             {'recommendations': [{'attribute': {'author': None,\n",
      "                                                 'source': None,\n",
      "                                                 'time': '1534411860.0'},\n",
      "                                   'sourceid': '43',\n",
      "                                   'title': 'D_Trump2018_8_16_2_31'},\n",
      "                                  {'attribute': {'author': None,\n",
      "                                                 'source': None,\n",
      "                                                 'time': '1534205640.0'},\n",
      "                                   'sourceid': '86',\n",
      "                                   'title': 'D_Trump2018_8_13_17_14'}],\n",
      "              'topic': 'heard heard;heard trump;claims heard;undermine '\n",
      "                       'trump;russians chinese;levin power;greatest scandal'}]}\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset # str | Dataset name.\n",
    "query = '(\"Trump\" OR \"president\")' # str | Fulltext query, using mysql MATCH boolean query format. Example, (\\\"word1\\\" OR \\\"word2\\\") AND (\\\"word3\\\" OR \\\"word4\\\") (optional)\n",
    "custom_stop_words = [\"real\",\"hillary\"] # ERRORUNKNOWN | List of stop words. (optional)\n",
    "num_topics = 8 # int | Number of topics to be extracted from the dataset. (optional) (default to 8)\n",
    "num_keywords = 8 # int | Number of keywords per topic that is extracted from the dataset. (optional) (default to 8)\n",
    "excluded_docs = '' # str | List of document IDs that should be excluded from the analysis. Example, \\\"docid1, docid2, ..., docidN\\\"  (optional)\n",
    "\n",
    "try:\n",
    "    api_response = api_instance.get_doc_recommend_api(dataset, \n",
    "                                                      query=query, \n",
    "                                                      custom_stop_words=custom_stop_words, \n",
    "                                                      num_topics=num_topics, \n",
    "                                                      num_keywords=num_keywords)\n",
    "    pprint(api_response)\n",
    "except ApiException as e:\n",
    "    print(\"Exception when calling DocumentsApi->get_doc_recommend_api: %s\\n\" % e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get document summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'doc_title': 'D_Trump2018_8_15_15_4',\n",
      " 'summary': {'sentences': \"['Our Country was built on Tariffs and Tariffs are \"\n",
      "                          'now leading us to great new Trade Deals - as '\n",
      "                          'opposed to the horrible and unfair Trade Deals that '\n",
      "                          \"I inherited as your President.', 'Other Countries \"\n",
      "                          'should not be allowed to come in and steal the '\n",
      "                          \"wealth of our great U.S.A. No longer!']\",\n",
      "             'sourceid': '50'}}\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset # str | Dataset name.\n",
    "doc_title = 'D_Trump2018_8_15_15_4' # str | The title of the document to be summarized.\n",
    "custom_stop_words = [\"real\",\"hillary\"] # ERRORUNKNOWN | List of stop words. (optional)\n",
    "summary_length = 6 # int | The maximum number of bullet points a user wants to see in the document summary. (optional) (default to 6)\n",
    "context_amount = 0 # int | The number of sentences surrounding key summary sentences in the documents that they come from. (optional) (default to 0)\n",
    "\n",
    "try:\n",
    "    api_response = api_instance.get_doc_summary_api(dataset, doc_title, custom_stop_words=custom_stop_words, summary_length=summary_length, context_amount=context_amount)\n",
    "    pprint(api_response)\n",
    "except ApiException as e:\n",
    "    print(\"Exception when calling DocumentsApi->get_doc_summary_api: %s\\n\" % e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
