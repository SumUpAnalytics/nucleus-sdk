{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import csv, json, datetime\n",
    "import time\n",
    "import nucleus_client\n",
    "from nucleus_client.rest import ApiException\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configure API host and key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "configuration = nucleus_client.Configuration()\n",
    "configuration.host = 'UPDATE-WITH-API-HOST'\n",
    "configuration.api_key['x-api-key'] = 'UPDATE-WITH-API-KEY'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create API instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------\n",
      "--                Dataset API Examples                     --\n",
      "-------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print('-------------------------------------------------------------')\n",
    "print('--                Dataset API Examples                     --')\n",
    "print('-------------------------------------------------------------')\n",
    "api_instance_dataset = nucleus_client.DatasetsApi(nucleus_client.ApiClient(configuration))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Append file from local drive to dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- Append file from local drive to dataset -----------\n",
      "\n",
      "quarles20181109a.pdf has been added to dataset dataset_test\n",
      "-------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print('--------- Append file from local drive to dataset -----------')\n",
    "print('')\n",
    "file = 'quarles20181109a.pdf' # file | \n",
    "dataset = 'dataset_test' # str | Destination dataset where the file will be inserted.\n",
    "\n",
    "try:\n",
    "    api_instance_dataset.post_upload_file(file, dataset)\n",
    "except ApiException as e:\n",
    "    print(\"Exception when calling DatasetsApi->post_upload_file: %s\\n\" % e)\n",
    "\n",
    "print(file, 'has been added to dataset', dataset)\n",
    "print('-------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Append file from URL to dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------ Append file from URL to dataset ---------------\n",
      "https://www.federalreserve.gov/newsevents/speech/files/quarles20181109a.pdf has been added to dataset dataset_test\n",
      "-------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print('------------ Append file from URL to dataset ---------------')\n",
    "\n",
    "dataset = dataset\n",
    "file_url = 'https://www.federalreserve.gov/newsevents/speech/files/quarles20181109a.pdf'\n",
    "payload = nucleus_client.UploadURLModel(\n",
    "                dataset=dataset,\n",
    "                file_url=file_url\n",
    "            ) # UploadURLModel | \n",
    "\n",
    "try:\n",
    "    api_response = api_instance_dataset.post_upload_url(payload)\n",
    "    #pprint(api_response)\n",
    "except ApiException as e:\n",
    "    print(\"Exception when calling DatasetsApi->post_upload_url: %s\\n\" % e)\n",
    "    \n",
    "print(file_url, 'has been added to dataset', dataset)\n",
    "print('-------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Append json from csv to dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- Append json from CSV to dataset -----------------\n",
      "Dataset dataset_test now has 12 documents.\n",
      "-------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print('----------- Append json from CSV to dataset -----------------')\n",
    "# add documents to dataset\n",
    "csv_file = 'trump-tweets-100.csv'\n",
    "dataset = dataset   \n",
    "\n",
    "doc_cnt = 0\n",
    "with open(csv_file, encoding='utf-8-sig') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for row in reader:\n",
    "        if doc_cnt < 10:\n",
    "            payload = nucleus_client.Appendjsonparams(dataset=dataset, \n",
    "                                                  language='english', \n",
    "                                                  document={'time'   : row['time'],\n",
    "                                                            'title'  : row['title'],\n",
    "                                                            'content': row['content'],\n",
    "                                                            'author' : row['author']}\n",
    "                                                 )\n",
    "\n",
    "            try:\n",
    "                response = api_instance_dataset.post_append_json_to_dataset(payload)\n",
    "            except ApiException as e:\n",
    "                print(\"Exception when calling DatasetsApi->post_append_json_to_dataset: %s\\n\" % e)\n",
    "        \n",
    "        doc_cnt = doc_cnt + 1\n",
    "        \n",
    "print('Dataset', dataset, 'now has', response.success, 'documents.')\n",
    "print('-------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List available datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------- List available datasets ---------------------\n",
      "7 datasets in the database:\n",
      "     aloxTest\n",
      "     dataset_from_file\n",
      "     dataset_from_url\n",
      "     dataset_test\n",
      "     trump_tweets\n",
      "     trump_tweets_2\n",
      "     trump_tweets_test\n",
      "-------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print('---------------- List available datasets ---------------------')\n",
    "try:\n",
    "    api_response = api_instance_dataset.get_list_datasets()\n",
    "except ApiException as e:\n",
    "    print(\"Exception when calling DatasetsApi->get_list_datasets: %s\\n\" % e)\n",
    "\n",
    "list_datasets = api_response.to_dict()['list_datasets']\n",
    "\n",
    "print(len(list_datasets), 'datasets in the database:')\n",
    "for ds in list_datasets:\n",
    "    print('    ', ds)\n",
    "\n",
    "    \n",
    "print('-------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get dataset information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------- Get dataset information -------------------\n",
      "Information about dataset dataset_test\n",
      "    Language: en\n",
      "    Number of documents: 12\n",
      "    Time range: 2018-08-18 01:47:00 to 2018-11-18 16:57:40\n",
      "-------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print('--------------- Get dataset information -------------------')\n",
    "dataset = dataset # str | Dataset name.\n",
    "query = '' # str | Fulltext query, using mysql MATCH boolean query format. (optional)\n",
    "metadata_selection = '' # str | json object of {\\\"metadata_field\\\":[\\\"selected_values\\\"]} (optional)\n",
    "time_period = '' # str | Time period selection (optional)\n",
    "\n",
    "try:\n",
    "    api_response = api_instance_dataset.get_dataset_info(\n",
    "        dataset, \n",
    "        query=query, \n",
    "        metadata_selection=metadata_selection, \n",
    "        time_period=time_period)\n",
    "except ApiException as e:\n",
    "    print(\"Exception when calling DatasetsApi->get_dataset_info: %s\\n\" % e)\n",
    "\n",
    "print('Information about dataset', dataset)\n",
    "print('    Language:', api_response.detected_language)\n",
    "print('    Number of documents:', api_response.num_documents)\n",
    "print('    Time range:', datetime.datetime.fromtimestamp(float(api_response.time_range[0])),\n",
    "             'to', datetime.datetime.fromtimestamp(float(api_response.time_range[1])))\n",
    "\n",
    "#pprint(api_response) # raw API response\n",
    "print('-------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------- Delete document -----------------------\n",
      "Document 1 from dataset dataset_test has been deleted.\n",
      "-------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print('--------------------- Delete document -----------------------')\n",
    "dataset = dataset\n",
    "docid = '1'\n",
    "payload = nucleus_client.Deletedocumentmodel(dataset=dataset,\n",
    "                                             docid=docid) # Deletedocumentmodel | \n",
    "\n",
    "try:\n",
    "    api_response = api_instance_dataset.post_delete_document(payload)\n",
    "except ApiException as e:\n",
    "    print(\"Exception when calling DatasetsApi->post_delete_document: %s\\n\" % e)\n",
    "\n",
    "print('Document', docid, 'from dataset', dataset, 'has been deleted.')\n",
    "print('-------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------- Delete dataset ------------------------\n",
      "{'success': 'Dataset deleted'}\n",
      "{'list_datasets': ['aloxTest',\n",
      "                   'dataset_from_file',\n",
      "                   'dataset_from_url',\n",
      "                   'trump_tweets',\n",
      "                   'trump_tweets_2',\n",
      "                   'trump_tweets_test']}\n",
      "-------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print('--------------------- Delete dataset ------------------------')\n",
    "\n",
    "dataset = dataset  \n",
    "payload = nucleus_client.Deletedatasetmodel(dataset=dataset) # Deletedatasetmodel | \n",
    "\n",
    "try:\n",
    "    api_response = api_instance_dataset.post_delete_dataset(payload)\n",
    "    pprint(api_response)\n",
    "except ApiException as e:\n",
    "    print(\"Exception when calling DatasetsApi->post_delete_dataset: %s\\n\" % e)\n",
    "    \n",
    "# List datasets again to check if the specified dataset has been deleted\n",
    "try:\n",
    "    api_response = api_instance_dataset.get_list_datasets()\n",
    "    pprint(api_response)\n",
    "except ApiException as e:\n",
    "    print(\"Exception when calling DatasetsApi->get_list_datasets: %s\\n\" % e)\n",
    "    \n",
    "print('-------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a full dataset for testing other APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- Create a full dataset for testing other APIs ---------\n",
      "Dataset trump_tweets now has 2800 documents.\n",
      "-------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print('--------- Create a full dataset for testing other APIs ---------')\n",
    "# add documents to dataset\n",
    "csv_file = 'trump-tweets-100.csv'\n",
    "dataset = 'trump_tweets'   \n",
    "\n",
    "with open(csv_file, encoding='utf-8-sig') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for row in reader:\n",
    "        payload = nucleus_client.Appendjsonparams(dataset=dataset, \n",
    "                                                  language='english', \n",
    "                                                  document={'time'   : row['time'],\n",
    "                                                            'title'  : row['title'],\n",
    "                                                            'content': row['content'],\n",
    "                                                            'author' : row['author']}\n",
    "                                                 )\n",
    "\n",
    "        try:\n",
    "            response = api_instance_dataset.post_append_json_to_dataset(payload)\n",
    "        except ApiException as e:\n",
    "            print(\"Exception when calling DatasetsApi->post_append_json_to_dataset: %s\\n\" % e)\n",
    "            \n",
    "print('Dataset', dataset, 'now has', response.success, 'documents.')\n",
    "print('-------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create API Instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------\n",
      "--                Topic API Examples                     --\n",
      "-------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print('-------------------------------------------------------------')\n",
    "print('--                Topic API Examples                     --')\n",
    "print('-------------------------------------------------------------')\n",
    "api_instance_topic = nucleus_client.TopicsApi(nucleus_client.ApiClient(configuration))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get list of topics from dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------- Get list of topics from dataset --------------\n",
      "Topic 1 keywords:\n",
      "    Keywords: america great;great great;andrew cuomo;taxed andrew;highest taxed;great believe;governor highest;believe governor\n",
      "    Keyword weights: 0.14795797393756516;0.16695785166789107;0.14904268327507322;0.10720829822389409;0.10720829822389409;0.10720829822389409;0.10720829822389409;0.10720829822389409\n",
      "    Strength: 0.17477316796369655\n",
      "    Document IDs: 16 17 18 24 44 45\n",
      "    Document exposures: 0.13842219371178488 0.0804584723966716 0.07557757475759962 0.09427887714883187 0.312912509996506 0.2983503719886059\n",
      "---------------\n",
      "Topic 2 keywords:\n",
      "    Keywords: illegal aliens;united permission;permission illegal;legally chuck;enter united;chuck schumer;aliens treated;aliens illegal\n",
      "    Keyword weights: 0.1111111111111111;0.1111111111111111;0.1111111111111111;0.1111111111111111;0.2222222222222222;0.1111111111111111;0.1111111111111111;0.1111111111111111\n",
      "    Strength: 0.12529311552435418\n",
      "    Document IDs: 57 63\n",
      "    Document exposures: 0.5 0.5\n",
      "---------------\n",
      "Topic 3 keywords:\n",
      "    Keywords: bruce ohr;peter strzok;fired fbi;agent peter;wife nelly;ohr justice;justice department;fusion gps\n",
      "    Keyword weights: 0.17704642562739586;0.11485991166063131;0.11485991166063131;0.11485991166063131;0.11485991166063131;0.12117130924335964;0.12117130924335964;0.12117130924335964\n",
      "    Strength: 0.16651861972651685\n",
      "    Document IDs: 11 12 13 31 33 68 70 91\n",
      "    Document exposures: 0.05799296722944725 0.08858572070134517 0.048520397500979105 0.10228997938655726 0.30640242163327774 0.10221071807041876 0.17896342760444403 0.11503436787353073\n",
      "---------------\n",
      "Topic 4 keywords:\n",
      "    Keywords: witch hunt;donald trump;frame donald;hillary clinton;rigged witch;trump gregg;gregg jarrett;clinton frame\n",
      "    Keyword weights: 0.08644792328377869;0.10668662079612214;0.08930771280814101;0.21697599423263134;0.14665673578828484;0.18102916652348455;0.08644792328377869;0.08644792328377869\n",
      "    Strength: 0.14660981702170603\n",
      "    Document IDs: 16 46 53 67 69 90 91 96 99\n",
      "    Document exposures: 0.03843228001612903 0.2677971675231634 0.0628738072730804 0.11033418347614264 0.3140968410154378 0.03406657517653554 0.06693427915859546 0.055845149187054514 0.04961971717386124\n",
      "---------------\n",
      "Topic 5 keywords:\n",
      "    Keywords: fake news;news media;wacky omarosa;presidential lowlife;omarosa modern;modern form;lowlife omarosa;form communication\n",
      "    Keyword weights: 0.18180528180141983;0.14262476425556084;0.13720328827832373;0.10767333313293909;0.10767333313293909;0.10767333313293909;0.10767333313293909;0.10767333313293909\n",
      "    Strength: 0.100587731732617\n",
      "    Document IDs: 8 38 40 89 93 95\n",
      "    Document exposures: 0.07823291401602575 0.0958153602151467 0.24180468243319012 0.1022605580282864 0.3983910893134939 0.08349539599385718\n",
      "---------------\n",
      "Topic 6 keywords:\n",
      "    Keywords: trump campaign;lou dobbs;forward special;evidence collusion;dobbs forward;democrats evidence;conflicts angry;angry democrats\n",
      "    Keyword weights: 0.14007373683036595;0.12284660902423344;0.12284660902423344;0.12284660902423344;0.12284660902423344;0.12284660902423344;0.12284660902423344;0.12284660902423344\n",
      "    Strength: 0.11687186488497478\n",
      "    Document IDs: 27 43 66 73\n",
      "    Document exposures: 0.05089655322295002 0.052153447503261105 0.4162759433607732 0.4806740559130157\n",
      "---------------\n",
      "Topic 7 keywords:\n",
      "    Keywords: strzok firing;judicial watch;watch strzok;tom fitton;mueller operation;fitton judicial;firing mueller;firing decisive\n",
      "    Keyword weights: 0.169967170507853;0.17431650476619812;0.025828238980726895;0.1259776171490444;0.1259776171490444;0.1259776171490444;0.1259776171490444;0.1259776171490444\n",
      "    Strength: 0.08654967168663112\n",
      "    Document IDs: 28 52 74 76\n",
      "    Document exposures: 0.1128563149776027 0.11508948362847467 0.22860053963526916 0.5434536617586534\n",
      "---------------\n",
      "Topic 8 keywords:\n",
      "    Keywords: john brennan;cia director;serving cia;mistakes john;looked mistakes;brennan serving;tuckercarlson speaking;speaking john\n",
      "    Keyword weights: 0.11354335520734882;0.11354335520734882;0.21461784150703836;0.11354335520734882;0.11354335520734882;0.1860984826927547;0.07255512748540587;0.07255512748540587\n",
      "    Strength: 0.08279601145950366\n",
      "    Document IDs: 4 30 47\n",
      "    Document exposures: 0.5387633645951668 0.33371686842898046 0.12751976697585257\n",
      "---------------\n",
      "-------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print('------------- Get list of topics from dataset --------------')\n",
    "dataset = dataset\n",
    "#query = '(\"Trump\" OR \"president\")' # str | Fulltext query, using mysql MATCH boolean query format. Example, (\\\"word1\\\" OR \\\"word2\\\") AND (\\\"word3\\\" OR \\\"word4\\\") (optional)\n",
    "query = ''\n",
    "custom_stop_words = [\"real\",\"hillary\"] # ERRORUNKNOWN | List of stop words. (optional)\n",
    "num_topics = 8 # int | Number of topics to be extracted from the dataset. (optional) (default to 8)\n",
    "metadata_selection =\"\" # str | json object of {\\\"metadata_field\\\":[\\\"selected_values\\\"]} (optional)\n",
    "time_period =\"\"# str | Time period selection (optional)\n",
    "\n",
    "try:\n",
    "    api_response = api_instance_topic.get_topic_api(\n",
    "        dataset,                                \n",
    "        query=query,                   \n",
    "        custom_stop_words=custom_stop_words,     \n",
    "        num_topics=num_topics,\n",
    "        metadata_selection=metadata_selection,\n",
    "        time_period=time_period)\n",
    "except ApiException as e:\n",
    "    print(\"Exception when calling TopicsApi->get_topic_api: %s\\n\" % e)\n",
    "    \n",
    "#print(api_response)\n",
    "i = 1\n",
    "for res in api_response.results:\n",
    "    print('Topic', i, 'keywords:')\n",
    "    print('    Keywords:', res.topic)\n",
    "    keywords_weight_str = \";\".join(str(x) for x in res.keywords_weight)\n",
    "    print('    Keyword weights:', keywords_weight_str)\n",
    "    print('    Strength:', res.strength)\n",
    "    doc_topic_exposure_sel = []  # list of non-zero doc_topic_exposure\n",
    "    doc_id_sel = []        # list of doc ids matching doc_topic_exposure_sel\n",
    "    for j in range(len(res.doc_topic_exposure)):\n",
    "        doc_topic_exp = float(res.doc_topic_exposure[j])\n",
    "        if doc_topic_exp != 0:\n",
    "            doc_topic_exposure_sel.append(doc_topic_exp)\n",
    "            doc_id_sel.append(res.doc_id[j])\n",
    "    \n",
    "    doc_id_sel_str = ' '.join(str(x) for x in doc_id_sel)\n",
    "    doc_topic_exposure_sel_str = ' '.join(str(x) for x in doc_topic_exposure_sel)\n",
    "    print('    Document IDs:', doc_id_sel_str)\n",
    "    print('    Document exposures:', doc_topic_exposure_sel_str)\n",
    "\n",
    "    print('---------------')\n",
    "    i = i + 1\n",
    "    \n",
    "print('-------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get topic summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------- Get topic summary -----------------------\n",
      "Topic 1 summary:\n",
      "    Keywords: america great;great great;andrew cuomo;taxed andrew;highest taxed;great believe;governor highest;believe governor\n",
      "    Document ID: 16\n",
      "        Title: D_Trump2018_8_17_19_25\n",
      "        Sentences: [\"Which is worse Hightax Andrew Cuomo's statement “WE’RE NOT GOING TO MAKE AMERICA GREAT AGAIN IT WAS NEVER THAT GREAT” or Hillary Clinton’s “DEPLORABLES” statement...\"]\n",
      "        Author: D_Trump16\n",
      "        Source: None\n",
      "        Time: 2018-08-17 19:25:00\n",
      "    Document ID: 24\n",
      "        Title: D_Trump2018_8_17_11_44\n",
      "        Sentences: ['How does a politician Cuomo known for pushing people and businesses out of his state not to mention having the highest taxes in the U.S. survive making the statement WE’RE NOT GOING TO MAKE AMERICA GREAT AGAIN IT WAS NEVER THAT GREAT?']\n",
      "        Author: D_Trump24\n",
      "        Source: None\n",
      "        Time: 2018-08-17 11:44:00\n",
      "    Document ID: 44\n",
      "        Title: D_Trump2018_8_16_2_2\n",
      "        Sentences: ['“WE’RE NOT GOING TO MAKE AMERICA GREAT AGAIN IT WAS NEVER THAT GREAT.” Can you believe this is the Governor of the Highest Taxed State in the U.S. Andrew Cuomo having a total meltdown!']\n",
      "        Author: D_Trump44\n",
      "        Source: None\n",
      "        Time: 2018-08-16 02:02:00\n",
      "    Document ID: 45\n",
      "        Title: D_Trump2018_8_16_1_53\n",
      "        Sentences: ['“WE’RE NOT GONG TO MAKE AMERICA GREAT AGAIN IT WAS NEVER THAT GREAT.”  Can you believe this is the Governor of the Highest Taxed State in the U.S. Andrew Cuomo having a total meltdown!']\n",
      "        Author: D_Trump45\n",
      "        Source: None\n",
      "        Time: 2018-08-16 01:53:00\n",
      "---------------\n",
      "Topic 2 summary:\n",
      "    Keywords: illegal aliens;united permission;permission illegal;legally chuck;enter united;chuck schumer;aliens treated;aliens illegal\n",
      "    Document ID: 63\n",
      "        Title: D_Trump2018_8_15_12_44\n",
      "        Sentences: ['“People who enter the United States without our permission are illegal aliens and illegal aliens should not be treated the same as people who enters the U.S. legally.”  Chuck Schumer in 2009 before he went left and haywire!']\n",
      "        Author: D_Trump63\n",
      "        Source: None\n",
      "        Time: 2018-08-15 12:44:00\n",
      "---------------\n",
      "Topic 3 summary:\n",
      "    Keywords: bruce ohr;peter strzok;fired fbi;agent peter;wife nelly;ohr justice;justice department;fusion gps\n",
      "    Document ID: 33\n",
      "        Title: D_Trump2018_8_16_23_37\n",
      "        Sentences: ['“The FBI received documents from Bruce Ohr (of the Justice Department  whose wife Nelly worked for Fusion GPS).” Disgraced and fired FBI Agent Peter Strzok.']\n",
      "        Author: D_Trump33\n",
      "        Source: None\n",
      "        Time: 2018-08-16 23:37:00\n",
      "    Document ID: 70\n",
      "        Title: D_Trump2018_8_14_11_55\n",
      "        Sentences: ['Bruce Ohr of the “Justice” Department (can you believe he is still there) is accused of helping disgraced Christopher Steele “find dirt on Trump.” Ohr’s wife Nelly was in on the act big time - worked for Fusion GPS on Fake Dossier.']\n",
      "        Author: D_Trump70\n",
      "        Source: None\n",
      "        Time: 2018-08-14 11:55:00\n",
      "    Document ID: 91\n",
      "        Title: D_Trump2018_8_13_16_4\n",
      "        Sentences: ['Agent Peter Strzok was just fired from the FBI - finally.']\n",
      "        Author: D_Trump91\n",
      "        Source: None\n",
      "        Time: 2018-08-13 16:04:00\n",
      "---------------\n",
      "Topic 4 summary:\n",
      "    Keywords: witch hunt;donald trump;frame donald;hillary clinton;rigged witch;trump gregg;gregg jarrett;clinton frame\n",
      "    Document ID: 46\n",
      "        Title: D_Trump2018_8_16_1_14\n",
      "        Sentences: ['We have the unfortunate situation where they then decided they were going to frame Donald Trump” concerning the Rigged Witch Hunt.']\n",
      "        Author: D_Trump46\n",
      "        Source: None\n",
      "        Time: 2018-08-16 01:14:00\n",
      "    Document ID: 67\n",
      "        Title: D_Trump2018_8_14_13_10\n",
      "        Sentences: ['Strzok started the illegal Rigged Witch Hunt - why isn’t this so-called “probe” ended immediately?']\n",
      "        Author: D_Trump67\n",
      "        Source: None\n",
      "        Time: 2018-08-14 13:10:00\n",
      "    Document ID: 69\n",
      "        Title: D_Trump2018_8_14_12_6\n",
      "        Sentences: ['“They were all in on it clear Hillary Clinton and FRAME Donald Trump for things he didn’t do.” Gregg Jarrett on @foxandfriends  If we had a real Attorney General this Witch Hunt would never have been started!']\n",
      "        Author: D_Trump69\n",
      "        Source: None\n",
      "        Time: 2018-08-14 12:06:00\n",
      "---------------\n",
      "Topic 5 summary:\n",
      "    Keywords: fake news;news media;wacky omarosa;presidential lowlife;omarosa modern;modern form;lowlife omarosa;form communication\n",
      "    Document ID: 93\n",
      "        Title: D_Trump2018_8_13_14_21\n",
      "        Sentences: ['While I know it’s “not presidential” to take on a lowlife like Omarosa and while I would rather not be doing so this is a modern day form of communication and I know the Fake News Media will be working overtime to make even Wacky Omarosa look legitimate as possible.']\n",
      "        Author: D_Trump93\n",
      "        Source: None\n",
      "        Time: 2018-08-13 14:21:00\n",
      "---------------\n",
      "Topic 6 summary:\n",
      "    Keywords: trump campaign;lou dobbs;forward special;evidence collusion;dobbs forward;democrats evidence;conflicts angry;angry democrats\n",
      "    Document ID: 73\n",
      "        Title: D_Trump2018_8_14_11_21\n",
      "        Sentences: ['Lou Dobbs: “This cannot go forward...this Special Councel with all of his conflicts with his 17 Angry Democrats without any evidence of collusion by the Trump Campaign and Russia.']\n",
      "        Author: D_Trump73\n",
      "        Source: None\n",
      "        Time: 2018-08-14 11:21:00\n",
      "---------------\n",
      "Topic 7 summary:\n",
      "    Keywords: strzok firing;judicial watch;watch strzok;tom fitton;mueller operation;fitton judicial;firing mueller;firing decisive\n",
      "    Document ID: 52\n",
      "        Title: D_Trump2018_8_15_14_15\n",
      "        Sentences: ['“The action (the Strzok firing) was a decisive step in the right direction in correcting the wrongs committed by what has been described as Comey’s skinny inner circle.”  Chris Swecker former FBI Assistant Director.']\n",
      "        Author: D_Trump52\n",
      "        Source: None\n",
      "        Time: 2018-08-15 14:15:00\n",
      "    Document ID: 76\n",
      "        Title: D_Trump2018_8_14_10_59\n",
      "        Sentences: ['Tom Fitton of Judicial Watch: “The Strzok firing is as much about the Mueller operation as anything else.']\n",
      "        Author: D_Trump76\n",
      "        Source: None\n",
      "        Time: 2018-08-14 10:59:00\n",
      "---------------\n",
      "Topic 8 summary:\n",
      "    Keywords: john brennan;cia director;serving cia;mistakes john;looked mistakes;brennan serving;tuckercarlson speaking;speaking john\n",
      "    Document ID: 4\n",
      "        Title: D_Trump2018_8_18_13_12\n",
      "        Sentences: ['Has anyone looked at the mistakes that John Brennan made while serving as CIA Director?']\n",
      "        Author: D_Trump4\n",
      "        Source: None\n",
      "        Time: 2018-08-18 13:12:00\n",
      "    Document ID: 30\n",
      "        Title: D_Trump2018_8_17_0_45\n",
      "        Sentences: ['@TuckerCarlson speaking of John Brennan: “How did somebody so obviously limited intellectually get to be CIA Director in the first place?” Now that is a really good question!']\n",
      "        Author: D_Trump30\n",
      "        Source: None\n",
      "        Time: 2018-08-17 00:45:00\n",
      "---------------\n",
      "-------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print('------------------- Get topic summary -----------------------')\n",
    "dataset = dataset # str | Dataset name.\n",
    "#query = '(\"Trump\" OR \"president\")' # str | Fulltext query, using mysql MATCH boolean query format. Example, (\\\"word1\\\" OR \\\"word2\\\") AND (\\\"word3\\\" OR \\\"word4\\\") (optional)\n",
    "query = ''\n",
    "custom_stop_words = [\"real\",\"hillary\"] # ERRORUNKNOWN | List of stop words. (optional)\n",
    "num_topics = 8 # int | Number of topics to be extracted from the dataset. (optional) (default to 8)\n",
    "num_keywords = 8 # int | Number of keywords per topic that is extracted from the dataset. (optional) (default to 8)\n",
    "metadata_selection =\"\" # str | json object of {\\\"metadata_field\\\":[\\\"selected_values\\\"]} (optional)\n",
    "summary_length = 6 # int | The maximum number of bullet points a user wants to see in each topic summary. (optional) (default to 6)\n",
    "context_amount = 0 # int | The number of sentences surrounding key summary sentences in the documents that they come from. (optional) (default to 0)\n",
    "num_docs = 20 # int | The maximum number of key documents to use for summarization. (optional) (default to 20)\n",
    "excluded_docs = '' # str | List of document IDs that should be excluded from the analysis. Example, \\\"docid1, docid2, ..., docidN\\\"  (optional)\n",
    "\n",
    "try:\n",
    "    api_response = api_instance_topic.get_topic_summary_api(\n",
    "        dataset, \n",
    "        query=query, \n",
    "        custom_stop_words=custom_stop_words, \n",
    "        num_topics=num_topics, \n",
    "        num_keywords=num_keywords,\n",
    "        metadata_selection=metadata_selection,\n",
    "        summary_length=summary_length, \n",
    "        context_amount=context_amount, \n",
    "        num_docs=num_docs)\n",
    "except ApiException as e:\n",
    "    print(\"Exception when calling TopicsApi->get_topic_summary_api: %s\\n\" % e)\n",
    "\n",
    "i = 1\n",
    "for res in api_response.results:\n",
    "    print('Topic', i, 'summary:')\n",
    "    print('    Keywords:', res.topic)\n",
    "    for j in range(len(res.summary)):\n",
    "        print('    Document ID:', res.summary[j].sourceid)\n",
    "        print('        Title:', res.summary[j].title)\n",
    "        print('        Sentences:', res.summary[j].sentences)\n",
    "        print('        Author:', res.summary[j].attribute.author)\n",
    "        print('        Source:', res.summary[j].attribute.source)\n",
    "        print('        Time:', datetime.datetime.fromtimestamp(float(res.summary[j].attribute.time)))\n",
    "\n",
    "        #print(type(res.summary[j].attribute))\n",
    "        \n",
    "    print('---------------')\n",
    "    i = i + 1\n",
    "    \n",
    "    \n",
    "#pprint(api_response)  # raw API response\n",
    "print('-------------------------------------------------------------')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get topic sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------- Get topic sentiment ------------------------\n",
      "Topic 1 sentiment:\n",
      "    Keywords: america great;great great;andrew cuomo;taxed andrew;highest taxed;great believe;governor highest;believe governor\n",
      "    Sentiment: 0.41852550172486996\n",
      "    Strength: 0.17477316796369655\n",
      "    Doucment IDs: 16 17 18 24 44 45\n",
      "    Doucment Sentiments: 0.2727272727272727 -0.2666666666666667 0.11764705882352941 0.45454545454545453 0.6 0.5454545454545454\n",
      "    Doucment Scores: [0.1384221937117849, 0.08045847239667162, 0.07557757475759962, 0.09427887714883189, 0.31291250999650605, 0.2983503719886059]\n",
      "---------------\n",
      "Topic 2 sentiment:\n",
      "    Keywords: illegal aliens;united permission;permission illegal;legally chuck;enter united;chuck schumer;aliens treated;aliens illegal\n",
      "    Sentiment: -0.46153846153846156\n",
      "    Strength: 0.12529311552435418\n",
      "    Doucment IDs: 57 63\n",
      "    Doucment Sentiments: -0.46153846153846156 -0.46153846153846156\n",
      "    Doucment Scores: [0.5, 0.5]\n",
      "---------------\n",
      "Topic 3 sentiment:\n",
      "    Keywords: bruce ohr;peter strzok;fired fbi;agent peter;wife nelly;ohr justice;justice department;fusion gps\n",
      "    Sentiment: -0.347947217991058\n",
      "    Strength: 0.16651861972651685\n",
      "    Doucment IDs: 11 12 13 31 33 68 70 91\n",
      "    Doucment Sentiments: -0.3333333333333333 -0.3333333333333333 -0.1 -0.3125 -0.25 -0.625 -0.3157894736842105 -0.4\n",
      "    Doucment Scores: [0.07570715190115822, 0.07570715190115822, 0.041466514860734466, 0.08741908905942716, 0.2618577180816574, 0.13461747240857874, 0.15294577152089295, 0.17027913026639271]\n",
      "---------------\n",
      "Topic 4 sentiment:\n",
      "    Keywords: witch hunt;donald trump;frame donald;hillary clinton;rigged witch;trump gregg;gregg jarrett;clinton frame\n",
      "    Sentiment: -0.40051525545005806\n",
      "    Strength: 0.14660981702170603\n",
      "    Doucment IDs: 16 46 53 67 69 90 91 96 99\n",
      "    Doucment Sentiments: 0.2727272727272727 -0.375 -0.8333333333333334 -0.8571428571428571 -0.3333333333333333 -0.2222222222222222 -0.16666666666666666 -0.2857142857142857 -0.4\n",
      "    Doucment Scores: [0.034333319262838405, 0.2392354980344115, 0.06685335995670713, 0.12355975826466116, 0.2805971208969716, 0.03795691260077215, 0.09454492834098563, 0.07303006833307407, 0.04988903430957834]\n",
      "---------------\n",
      "Topic 5 sentiment:\n",
      "    Keywords: fake news;news media;wacky omarosa;presidential lowlife;omarosa modern;modern form;lowlife omarosa;form communication\n",
      "    Sentiment: -0.19429249489176126\n",
      "    Strength: 0.100587731732617\n",
      "    Doucment IDs: 8 38 40 89 93 95\n",
      "    Doucment Sentiments: -0.5 -0.375 -0.5 -0.5 0.2857142857142857 -0.6666666666666666\n",
      "    Doucment Scores: [0.09415980058720225, 0.09415980058720225, 0.23762662508220356, 0.10049363411313801, 0.3915074309718749, 0.08205270865837908]\n",
      "---------------\n",
      "Topic 6 sentiment:\n",
      "    Keywords: trump campaign;lou dobbs;forward special;evidence collusion;dobbs forward;democrats evidence;conflicts angry;angry democrats\n",
      "    Sentiment: -0.2711896972210555\n",
      "    Strength: 0.11687186488497478\n",
      "    Doucment IDs: 27 43 66 73\n",
      "    Doucment Sentiments: -0.5 -0.3 -0.3333333333333333 -0.16666666666666666\n",
      "    Doucment Scores: [0.07524141859813199, 0.047586851430450085, 0.438585864985709, 0.438585864985709]\n",
      "---------------\n",
      "Topic 7 sentiment:\n",
      "    Keywords: strzok firing;judicial watch;watch strzok;tom fitton;mueller operation;fitton judicial;firing mueller;firing decisive\n",
      "    Sentiment: -0.5628179498964005\n",
      "    Strength: 0.08654967168663112\n",
      "    Doucment IDs: 28 52 74 76\n",
      "    Doucment Sentiments: -0.4 -0.375 -0.75 -0.5714285714285714\n",
      "    Doucment Scores: [0.1280441131104907, 0.08428761623610359, 0.16741924586562787, 0.620249024787778]\n",
      "---------------\n",
      "Topic 8 sentiment:\n",
      "    Keywords: john brennan;cia director;serving cia;mistakes john;looked mistakes;brennan serving;tuckercarlson speaking;speaking john\n",
      "    Sentiment: -0.1773119152040573\n",
      "    Strength: 0.08279601145950366\n",
      "    Doucment IDs: 4 30 47\n",
      "    Doucment Sentiments: -0.3333333333333333 0.1111111111111111 -0.14285714285714285\n",
      "    Doucment Scores: [0.5932385414019006, 0.3092632399354499, 0.09749821866264961]\n",
      "---------------\n",
      "-------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print('---------------- Get topic sentiment ------------------------')\n",
    "dataset = dataset # str | Dataset name.\n",
    "#query = '(\"Trump\" OR \"president\")' # str | Fulltext query, using mysql MATCH boolean query format. Example, (\\\"word1\\\" OR \\\"word2\\\") AND (\\\"word3\\\" OR \\\"word4\\\") (optional)\n",
    "query = ''\n",
    "custom_stop_words = [\"real\",\"hillary\"] # ERRORUNKNOWN | List of stop words. (optional)\n",
    "num_topics = 8 # int | Number of topics to be extracted from the dataset. (optional) (default to 8)\n",
    "num_keywords = 8 # int | Number of keywords per topic that is extracted from the dataset. (optional) (default to 8)\n",
    "excluded_docs = '' # str | List of document IDs that should be excluded from the analysis. Example, \\\"docid1, docid2, ..., docidN\\\"  (optional)\n",
    "\n",
    "try:\n",
    "    api_response = api_instance_topic.get_topic_sentiment_api(\n",
    "        dataset, \n",
    "        query=query, \n",
    "        custom_stop_words=custom_stop_words, \n",
    "        num_topics=num_topics, \n",
    "        num_keywords=num_keywords)\n",
    "    \n",
    "except ApiException as e:\n",
    "    print(\"Exception when calling TopicsApi->get_topic_sentiment_api: %s\\n\" % e)\n",
    "\n",
    "i = 1\n",
    "for res in api_response.results:\n",
    "    print('Topic', i, 'sentiment:')\n",
    "    print('    Keywords:', res.topic)\n",
    "    print('    Sentiment:', res.sentiment)\n",
    "    print('    Strength:', res.strength)\n",
    "    \n",
    "    doc_id_str = ' '.join(str(x) for x in res.doc_id)\n",
    "    doc_sentiment_str = ' '.join(str(x) for x in res.doc_sentiment)\n",
    "    doc_score_str = ' '.join(str(x) for x in res.doc_score)\n",
    "    print('    Doucment IDs:', doc_id_str)\n",
    "    print('    Doucment Sentiments:', doc_sentiment_str)\n",
    "    print('    Doucment Scores:', doc_score_str)\n",
    "    \n",
    "    print('---------------')\n",
    "    i = i + 1\n",
    "    \n",
    "#pprint(api_response)\n",
    "print('-------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get topic consensus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------- Get topic consensus ------------------------\n",
      "Topic 1 consensus:\n",
      "    Keywords: america great;great great;andrew cuomo;taxed andrew;highest taxed;great believe;governor highest;believe governor\n",
      "    Consensus: 0.9195415276033284\n",
      "    Strength: 0.17477316796369655\n",
      "---------------\n",
      "Topic 2 consensus:\n",
      "    Keywords: illegal aliens;united permission;permission illegal;legally chuck;enter united;chuck schumer;aliens treated;aliens illegal\n",
      "    Consensus: 1.0\n",
      "    Strength: 0.12529311552435418\n",
      "---------------\n",
      "Topic 3 consensus:\n",
      "    Keywords: bruce ohr;peter strzok;fired fbi;agent peter;wife nelly;ohr justice;justice department;fusion gps\n",
      "    Consensus: 0.9999999999999999\n",
      "    Strength: 0.16651861972651685\n",
      "---------------\n",
      "Topic 4 consensus:\n",
      "    Keywords: witch hunt;donald trump;frame donald;hillary clinton;rigged witch;trump gregg;gregg jarrett;clinton frame\n",
      "    Consensus: 0.9656666807371614\n",
      "    Strength: 0.14660981702170603\n",
      "---------------\n",
      "Topic 5 consensus:\n",
      "    Keywords: fake news;news media;wacky omarosa;presidential lowlife;omarosa modern;modern form;lowlife omarosa;form communication\n",
      "    Consensus: 0.6084925690281252\n",
      "    Strength: 0.100587731732617\n",
      "---------------\n",
      "Topic 6 consensus:\n",
      "    Keywords: trump campaign;lou dobbs;forward special;evidence collusion;dobbs forward;democrats evidence;conflicts angry;angry democrats\n",
      "    Consensus: 1.0\n",
      "    Strength: 0.11687186488497478\n",
      "---------------\n",
      "Topic 7 consensus:\n",
      "    Keywords: strzok firing;judicial watch;watch strzok;tom fitton;mueller operation;fitton judicial;firing mueller;firing decisive\n",
      "    Consensus: 1.0\n",
      "    Strength: 0.08654967168663112\n",
      "---------------\n",
      "Topic 8 consensus:\n",
      "    Keywords: john brennan;cia director;serving cia;mistakes john;looked mistakes;brennan serving;tuckercarlson speaking;speaking john\n",
      "    Consensus: 0.6907367600645502\n",
      "    Strength: 0.08279601145950366\n",
      "---------------\n",
      "-------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print('---------------- Get topic consensus ------------------------')\n",
    "dataset = dataset # str | Dataset name.\n",
    "query = '' # str | Fulltext query, using mysql MATCH boolean query format. Example, (\\\"word1\\\" OR \\\"word2\\\") AND (\\\"word3\\\" OR \\\"word4\\\") (optional)\n",
    "custom_stop_words = [\"real\",\"hillary\"] # ERRORUNKNOWN | List of stop words. (optional)\n",
    "num_topics = 8 # int | Number of topics to be extracted from the dataset. (optional) (default to 8)\n",
    "num_keywords = 8 # int | Number of keywords per topic that is extracted from the dataset. (optional) (default to 8)\n",
    "excluded_docs = [''] # str | List of document IDs that should be excluded from the analysis. Example, \\\"docid1, docid2, ..., docidN\\\"  (optional)\n",
    "\n",
    "try:\n",
    "    api_response = api_instance_topic.get_topic_consensus_api(\n",
    "        dataset, \n",
    "        query=query, \n",
    "        custom_stop_words=custom_stop_words, \n",
    "        num_topics=num_topics, \n",
    "        num_keywords=num_keywords)\n",
    "except ApiException as e:\n",
    "    print(\"Exception when calling TopicsApi->get_topic_consensus_api: %s\\n\" % e)\n",
    "    \n",
    "i = 1\n",
    "for res in api_response.results:\n",
    "    print('Topic', i, 'consensus:')\n",
    "    print('    Keywords:', res.topic)\n",
    "    print('    Consensus:', res.consensus)\n",
    "    print('    Strength:', res.strength)\n",
    "    \n",
    "    print('---------------')\n",
    "    i = i + 1\n",
    "    \n",
    "#pprint(api_response) # raw API response\n",
    "print('-------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get author connectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------\n",
      "Mainstream connections:\n",
      "    Topic: worse hightax;hightax andrew;andrew cuomo\n",
      "    Authors: D_Trump44\n",
      "Niche connections:\n",
      "    Topic: statement america;cuomo statement;america great\n",
      "    Authors: D_Trump24\n",
      "-------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print('-------------------------------------------------------------')\n",
    "dataset = dataset # str | Dataset name.\n",
    "target_author = 'D_Trump16' # str | Name of the author to be analyzed.\n",
    "query = '' # str | Fulltext query, using mysql MATCH boolean query format. Subject covered by the author, on which to focus the analysis of connectivity. Example, (\\\"word1\\\" OR \\\"word2\\\") AND (\\\"word3\\\" OR \\\"word4\\\") (optional)\n",
    "custom_stop_words = [\"real\",\"hillary\"] # str | List of words possibly used by the target author that are considered not information-bearing. (optional)\n",
    "time_period = '12M' # str | Time period selection. Required. Valid values: \"1M\",\"3M\",\"6M\",\"12M\",\"3Y\",\"5Y\"\n",
    "metadata_selection = '' # str | json object of {\\\"metadata_field\\\":[\\\"selected_values\\\"]} (optional)\n",
    "excluded_docs = [''] # str | List of document IDs that should be excluded from the analysis. Example, \\\"docid1, docid2, ..., docidN\\\"  (optional)\n",
    "\n",
    "try:\n",
    "    api_response = api_instance_topic.get_author_connectivity_api(\n",
    "        dataset, \n",
    "        target_author, \n",
    "        query=query, \n",
    "        custom_stop_words=custom_stop_words, \n",
    "        time_period=time_period, \n",
    "        metadata_selection=metadata_selection, \n",
    "        excluded_docs=excluded_docs)\n",
    "    \n",
    "except ApiException as e:\n",
    "    print(\"Exception when calling TopicsApi->get_author_connectivity_api: %s\\n\" % e)\n",
    "\n",
    "res = api_response.results\n",
    "print('Mainstream connections:')\n",
    "for mc in res.mainstream_connection:\n",
    "    print('    Topic:', mc.topic)\n",
    "    print('    Authors:', \" \".join(str(x) for x in mc.authors))\n",
    "    \n",
    "print('Niche connections:')\n",
    "for nc in res.niche_connection:\n",
    "    print('    Topic:', nc.topic)\n",
    "    print('    Authors:', \" \".join(str(x) for x in nc.authors))  \n",
    "    \n",
    "#pprint(api_response)   # raw API response\n",
    "print('-------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create API instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------\n",
      "--                Document API examples                    --\n",
      "-------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print('-------------------------------------------------------------')\n",
    "print('--                Document API examples                    --')\n",
    "print('-------------------------------------------------------------')\n",
    "\n",
    "api_instance_doc = nucleus_client.DocumentsApi(nucleus_client.ApiClient(configuration))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get document information without content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document ID: 10\n",
      "    Title: D_Trump2018_8_18_1_47\n",
      "    Author: D_Trump10\n",
      "    Source: None\n",
      "    Time: 2018-08-18 01:47:00\n",
      "---------------\n",
      "Document ID: 11\n",
      "    Title: D_Trump2018_8_18_1_46\n",
      "    Author: D_Trump11\n",
      "    Source: None\n",
      "    Time: 2018-08-18 01:46:00\n",
      "---------------\n",
      "Document ID: 12\n",
      "    Title: D_Trump2018_8_18_1_37\n",
      "    Author: D_Trump12\n",
      "    Source: None\n",
      "    Time: 2018-08-18 01:37:00\n",
      "---------------\n",
      "Document ID: 20\n",
      "    Title: D_Trump2018_8_17_12_38\n",
      "    Author: D_Trump20\n",
      "    Source: None\n",
      "    Time: 2018-08-17 12:38:00\n",
      "---------------\n",
      "-------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset # str | Dataset name.\n",
    "doc_titles = ['D_Trump2018_8_18_1_47']   # str | The title of the document to retrieve. Example: \\\" \\\"title 1\\\" \\\"  (optional)\n",
    "doc_ids = ['11', '12', '20']      # int | The docid of the document to retrieve. Example: \\\"docid1\\\"  (optional)\n",
    "\n",
    "try:\n",
    "    api_response = api_instance_doc.get_doc_info(dataset, doc_titles=doc_titles, doc_ids=doc_ids)\n",
    "    \n",
    "except ApiException as e:\n",
    "    print(\"Exception when calling DocumentsApi->get_doc_info: %s\\n\" % e)\n",
    "    \n",
    "for res in api_response.results:\n",
    "    print('Document ID:', res.sourceid)\n",
    "    print('    Title:', res.title)\n",
    "    print('    Author:', res.attribute.author)\n",
    "    print('    Source:', res.attribute.source)\n",
    "    print('    Time:', datetime.datetime.fromtimestamp(float(res.attribute.time)))\n",
    "\n",
    "    print('---------------')\n",
    "    \n",
    "    \n",
    "#pprint(api_response)  # raw response from API server\n",
    "print('-------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display document details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------\n",
      "Document ID: 10\n",
      "    Title: D_Trump2018_8_18_1_47\n",
      "    Author: D_Trump10\n",
      "    Source: None\n",
      "    Time: 2018-08-18 01:47:00\n",
      "    Content  financial gain is a Federal Gratuity Statute Violation Bribery Statute Violation Honest Services Violation all Major Crimes because the DOJ is run by BLANK Jeff Sessions ”  Gregg Jarrett. So when does Mueller do what must be done? Probably never! @FoxNews\n",
      "---------------\n",
      "Document ID: 11\n",
      "    Title: D_Trump2018_8_18_1_46\n",
      "    Author: D_Trump11\n",
      "    Source: None\n",
      "    Time: 2018-08-18 01:46:00\n",
      "    Content “Bruce Ohr of DOJ is in legal jeopardy it’s astonishing that he’s still employed. Bruce  Nelly Ohr’s bank account is getting fatter  fatter because of the Dossier that they are both peddling. He doesn’t disclose it under Fed Regs. Using your Federal office for personal \n",
      "---------------\n",
      "-------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print('-------------------------------------------------------------')\n",
    "\n",
    "dataset = dataset # str | Dataset name.\n",
    "doc_titles = ['D_Trump2018_8_18_1_47']   # str | The title of the document to retrieve. Example: \\\" \\\"title 1\\\" \\\"  (optional)\n",
    "doc_ids = ['11']      # int | The docid of the document to retrieve. Example: \\\"docid1\\\"  (optional)\n",
    "\n",
    "try:\n",
    "    api_response = api_instance_doc.get_doc_display(dataset, doc_titles=doc_titles, doc_ids=doc_ids)\n",
    "    \n",
    "except ApiException as e:\n",
    "    print(\"Exception when calling DocumentsApi->get_doc_display_api: %s\\n\" % e)\n",
    "\n",
    "for res in api_response.results:\n",
    "    print('Document ID:', res.sourceid)\n",
    "    print('    Title:', res.title)\n",
    "    print('    Author:', res.attribute.author)\n",
    "    print('    Source:', res.attribute.source)\n",
    "    print('    Time:', datetime.datetime.fromtimestamp(float(res.attribute.time)))\n",
    "    print('    Content', res.content)\n",
    "\n",
    "    print('---------------')\n",
    "\n",
    "\n",
    "#pprint(api_response) # raw response from API server\n",
    "print('-------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get document recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------- Get document recommendations -----------------\n",
      "Document recommendations for topic 1 :\n",
      "    Keywords: america great;great great;andrew cuomo;taxed andrew;highest taxed;great believe;governor highest;believe governor\n",
      "    Recommendation 1 :\n",
      "        Document ID: 44\n",
      "        Title: D_Trump2018_8_16_2_2\n",
      "        Attribute: {'author': 'D_Trump44', 'source': None, 'time': '1534410120.0'}\n",
      "        Author: D_Trump44\n",
      "        Source: None\n",
      "        Time: 2018-08-16 02:02:00\n",
      "    Recommendation 2 :\n",
      "        Document ID: 45\n",
      "        Title: D_Trump2018_8_16_1_53\n",
      "        Attribute: {'author': 'D_Trump45', 'source': None, 'time': '1534409580.0'}\n",
      "        Author: D_Trump45\n",
      "        Source: None\n",
      "        Time: 2018-08-16 01:53:00\n",
      "    Recommendation 3 :\n",
      "        Document ID: 16\n",
      "        Title: D_Trump2018_8_17_19_25\n",
      "        Attribute: {'author': 'D_Trump16', 'source': None, 'time': '1534559100.0'}\n",
      "        Author: D_Trump16\n",
      "        Source: None\n",
      "        Time: 2018-08-17 19:25:00\n",
      "    Recommendation 4 :\n",
      "        Document ID: 24\n",
      "        Title: D_Trump2018_8_17_11_44\n",
      "        Attribute: {'author': 'D_Trump24', 'source': None, 'time': '1534531440.0'}\n",
      "        Author: D_Trump24\n",
      "        Source: None\n",
      "        Time: 2018-08-17 11:44:00\n",
      "    Recommendation 5 :\n",
      "        Document ID: 17\n",
      "        Title: D_Trump2018_8_17_14_17\n",
      "        Attribute: {'author': 'D_Trump17', 'source': None, 'time': '1534540620.0'}\n",
      "        Author: D_Trump17\n",
      "        Source: None\n",
      "        Time: 2018-08-17 14:17:00\n",
      "    Recommendation 6 :\n",
      "        Document ID: 18\n",
      "        Title: D_Trump2018_8_17_14_10\n",
      "        Attribute: {'author': 'D_Trump18', 'source': None, 'time': '1534540200.0'}\n",
      "        Author: D_Trump18\n",
      "        Source: None\n",
      "        Time: 2018-08-17 14:10:00\n",
      "---------------\n",
      "Document recommendations for topic 2 :\n",
      "    Keywords: illegal aliens;united permission;permission illegal;legally chuck;enter united;chuck schumer;aliens treated;aliens illegal\n",
      "    Recommendation 1 :\n",
      "        Document ID: 63\n",
      "        Title: D_Trump2018_8_15_12_44\n",
      "        Attribute: {'author': 'D_Trump63', 'source': None, 'time': '1534362240.0'}\n",
      "        Author: D_Trump63\n",
      "        Source: None\n",
      "        Time: 2018-08-15 12:44:00\n",
      "    Recommendation 2 :\n",
      "        Document ID: 57\n",
      "        Title: D_Trump2018_8_15_13_18\n",
      "        Attribute: {'author': 'D_Trump57', 'source': None, 'time': '1534364280.0'}\n",
      "        Author: D_Trump57\n",
      "        Source: None\n",
      "        Time: 2018-08-15 13:18:00\n",
      "---------------\n",
      "Document recommendations for topic 3 :\n",
      "    Keywords: bruce ohr;peter strzok;fired fbi;agent peter;wife nelly;ohr justice;justice department;fusion gps\n",
      "    Recommendation 1 :\n",
      "        Document ID: 33\n",
      "        Title: D_Trump2018_8_16_23_37\n",
      "        Attribute: {'author': 'D_Trump33', 'source': None, 'time': '1534487820.0'}\n",
      "        Author: D_Trump33\n",
      "        Source: None\n",
      "        Time: 2018-08-16 23:37:00\n",
      "    Recommendation 2 :\n",
      "        Document ID: 70\n",
      "        Title: D_Trump2018_8_14_11_55\n",
      "        Attribute: {'author': 'D_Trump70', 'source': None, 'time': '1534272900.0'}\n",
      "        Author: D_Trump70\n",
      "        Source: None\n",
      "        Time: 2018-08-14 11:55:00\n",
      "    Recommendation 3 :\n",
      "        Document ID: 91\n",
      "        Title: D_Trump2018_8_13_16_4\n",
      "        Attribute: {'author': 'D_Trump91', 'source': None, 'time': '1534201440.0'}\n",
      "        Author: D_Trump91\n",
      "        Source: None\n",
      "        Time: 2018-08-13 16:04:00\n",
      "    Recommendation 4 :\n",
      "        Document ID: 68\n",
      "        Title: D_Trump2018_8_14_13_1\n",
      "        Attribute: {'author': 'D_Trump68', 'source': None, 'time': '1534276860.0'}\n",
      "        Author: D_Trump68\n",
      "        Source: None\n",
      "        Time: 2018-08-14 13:01:00\n",
      "---------------\n",
      "Document recommendations for topic 4 :\n",
      "    Keywords: witch hunt;donald trump;frame donald;hillary clinton;rigged witch;trump gregg;gregg jarrett;clinton frame\n",
      "    Recommendation 1 :\n",
      "        Document ID: 69\n",
      "        Title: D_Trump2018_8_14_12_6\n",
      "        Attribute: {'author': 'D_Trump69', 'source': None, 'time': '1534273560.0'}\n",
      "        Author: D_Trump69\n",
      "        Source: None\n",
      "        Time: 2018-08-14 12:06:00\n",
      "    Recommendation 2 :\n",
      "        Document ID: 46\n",
      "        Title: D_Trump2018_8_16_1_14\n",
      "        Attribute: {'author': 'D_Trump46', 'source': None, 'time': '1534407240.0'}\n",
      "        Author: D_Trump46\n",
      "        Source: None\n",
      "        Time: 2018-08-16 01:14:00\n",
      "    Recommendation 3 :\n",
      "        Document ID: 67\n",
      "        Title: D_Trump2018_8_14_13_10\n",
      "        Attribute: {'author': 'D_Trump67', 'source': None, 'time': '1534277400.0'}\n",
      "        Author: D_Trump67\n",
      "        Source: None\n",
      "        Time: 2018-08-14 13:10:00\n",
      "---------------\n",
      "Document recommendations for topic 5 :\n",
      "    Keywords: fake news;news media;wacky omarosa;presidential lowlife;omarosa modern;modern form;lowlife omarosa;form communication\n",
      "    Recommendation 1 :\n",
      "        Document ID: 93\n",
      "        Title: D_Trump2018_8_13_14_21\n",
      "        Attribute: {'author': 'D_Trump93', 'source': None, 'time': '1534195260.0'}\n",
      "        Author: D_Trump93\n",
      "        Source: None\n",
      "        Time: 2018-08-13 14:21:00\n",
      "    Recommendation 2 :\n",
      "        Document ID: 40\n",
      "        Title: D_Trump2018_8_16_12_50\n",
      "        Attribute: {'author': 'D_Trump40', 'source': None, 'time': '1534449000.0'}\n",
      "        Author: D_Trump40\n",
      "        Source: None\n",
      "        Time: 2018-08-16 12:50:00\n",
      "---------------\n",
      "Document recommendations for topic 6 :\n",
      "    Keywords: trump campaign;lou dobbs;forward special;evidence collusion;dobbs forward;democrats evidence;conflicts angry;angry democrats\n",
      "    Recommendation 1 :\n",
      "        Document ID: 73\n",
      "        Title: D_Trump2018_8_14_11_21\n",
      "        Attribute: {'author': 'D_Trump73', 'source': None, 'time': '1534270860.0'}\n",
      "        Author: D_Trump73\n",
      "        Source: None\n",
      "        Time: 2018-08-14 11:21:00\n",
      "    Recommendation 2 :\n",
      "        Document ID: 66\n",
      "        Title: D_Trump2018_8_14_13_15\n",
      "        Attribute: {'author': 'D_Trump66', 'source': None, 'time': '1534277700.0'}\n",
      "        Author: D_Trump66\n",
      "        Source: None\n",
      "        Time: 2018-08-14 13:15:00\n",
      "---------------\n",
      "Document recommendations for topic 7 :\n",
      "    Keywords: strzok firing;judicial watch;watch strzok;tom fitton;mueller operation;fitton judicial;firing mueller;firing decisive\n",
      "    Recommendation 1 :\n",
      "        Document ID: 76\n",
      "        Title: D_Trump2018_8_14_10_59\n",
      "        Attribute: {'author': 'D_Trump76', 'source': None, 'time': '1534269540.0'}\n",
      "        Author: D_Trump76\n",
      "        Source: None\n",
      "        Time: 2018-08-14 10:59:00\n",
      "    Recommendation 2 :\n",
      "        Document ID: 74\n",
      "        Title: D_Trump2018_8_14_11_13\n",
      "        Attribute: {'author': 'D_Trump74', 'source': None, 'time': '1534270380.0'}\n",
      "        Author: D_Trump74\n",
      "        Source: None\n",
      "        Time: 2018-08-14 11:13:00\n",
      "    Recommendation 3 :\n",
      "        Document ID: 52\n",
      "        Title: D_Trump2018_8_15_14_15\n",
      "        Attribute: {'author': 'D_Trump52', 'source': None, 'time': '1534367700.0'}\n",
      "        Author: D_Trump52\n",
      "        Source: None\n",
      "        Time: 2018-08-15 14:15:00\n",
      "---------------\n",
      "Document recommendations for topic 8 :\n",
      "    Keywords: john brennan;cia director;serving cia;mistakes john;looked mistakes;brennan serving;tuckercarlson speaking;speaking john\n",
      "    Recommendation 1 :\n",
      "        Document ID: 4\n",
      "        Title: D_Trump2018_8_18_13_12\n",
      "        Attribute: {'author': 'D_Trump4', 'source': None, 'time': '1534623120.0'}\n",
      "        Author: D_Trump4\n",
      "        Source: None\n",
      "        Time: 2018-08-18 13:12:00\n",
      "    Recommendation 2 :\n",
      "        Document ID: 30\n",
      "        Title: D_Trump2018_8_17_0_45\n",
      "        Attribute: {'author': 'D_Trump30', 'source': None, 'time': '1534491900.0'}\n",
      "        Author: D_Trump30\n",
      "        Source: None\n",
      "        Time: 2018-08-17 00:45:00\n",
      "---------------\n",
      "-------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print('------------- Get document recommendations -----------------')\n",
    "\n",
    "dataset = dataset # str | Dataset name.\n",
    "#query = '(\"Trump\" OR \"president\")' # str | Fulltext query, using mysql MATCH boolean query format. Example, (\\\"word1\\\" OR \\\"word2\\\") AND (\\\"word3\\\" OR \\\"word4\\\") (optional)\n",
    "query = ''\n",
    "custom_stop_words = [\"real\",\"hillary\"] # ERRORUNKNOWN | List of stop words. (optional)\n",
    "num_topics = 8 # int | Number of topics to be extracted from the dataset. (optional) (default to 8)\n",
    "num_keywords = 8 # int | Number of keywords per topic that is extracted from the dataset. (optional) (default to 8)\n",
    "excluded_docs = '' # str | List of document IDs that should be excluded from the analysis. Example, \\\"docid1, docid2, ..., docidN\\\"  (optional)\n",
    "\n",
    "try:\n",
    "    api_response = api_instance_doc.get_doc_recommend_api(\n",
    "        dataset, \n",
    "        query=query, \n",
    "        custom_stop_words=custom_stop_words, \n",
    "        num_topics=num_topics, \n",
    "        num_keywords=num_keywords)\n",
    "except ApiException as e:\n",
    "    print(\"Exception when calling DocumentsApi->get_doc_recommend_api: %s\\n\" % e)\n",
    "    \n",
    "i = 1\n",
    "for res in api_response.results:\n",
    "    print('Document recommendations for topic', i, ':')\n",
    "    print('    Keywords:', res.topic)\n",
    "\n",
    "    j = 1\n",
    "    for doc in res.recommendations:\n",
    "        print('    Recommendation', j, ':')\n",
    "        print('        Document ID:', doc.sourceid)\n",
    "        print('        Title:', doc.title)\n",
    "        print('        Attribute:', doc.attribute)\n",
    "        print('        Author:', doc.attribute.author)\n",
    "        print('        Source:', doc.attribute.source)\n",
    "        print('        Time:', datetime.datetime.fromtimestamp(float(doc.attribute.time)))\n",
    "        j = j + 1\n",
    "    \n",
    "    print('---------------')\n",
    "    i = i + 1\n",
    "    \n",
    "#pprint(api_response)   # raw API response\n",
    "print('-------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get document summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------ Get document summary  --------------------\n",
      "Document Summary\n",
      "    ID: 50\n",
      "    Title: D_Trump2018_8_15_15_4\n",
      "    Summary: ['Our Country was built on Tariffs and Tariffs are now leading us to great new Trade Deals - as opposed to the horrible and unfair Trade Deals that I inherited as your President.', 'Other Countries should not be allowed to come in and steal the wealth of our great U.S.A. No longer!']\n",
      "-------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print('------------------ Get document summary  --------------------')\n",
    "\n",
    "dataset = dataset # str | Dataset name.\n",
    "doc_title = 'D_Trump2018_8_15_15_4' # str | The title of the document to be summarized.\n",
    "custom_stop_words = [\"real\",\"hillary\"] # ERRORUNKNOWN | List of stop words. (optional)\n",
    "summary_length = 6 # int | The maximum number of bullet points a user wants to see in the document summary. (optional) (default to 6)\n",
    "context_amount = 0 # int | The number of sentences surrounding key summary sentences in the documents that they come from. (optional) (default to 0)\n",
    "\n",
    "try:\n",
    "    api_response = api_instance_doc.get_doc_summary_api(\n",
    "        dataset, \n",
    "        doc_title, \n",
    "        custom_stop_words=custom_stop_words, \n",
    "        summary_length=summary_length, \n",
    "        context_amount=context_amount)\n",
    "    \n",
    "except ApiException as e:\n",
    "    print(\"Exception when calling DocumentsApi->get_doc_summary_api: %s\\n\" % e)\n",
    " \n",
    "print('Document Summary')\n",
    "print('    ID:', api_response.summary.sourceid)\n",
    "print('    Title:', api_response.doc_title)\n",
    "print('    Summary:', api_response.summary.sentences)\n",
    "\n",
    "#pprint(api_response)   # raw API response\n",
    "print('-------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
