{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import csv, json, datetime\n",
    "import time\n",
    "import nucleus_client\n",
    "from nucleus_client.rest import ApiException\n",
    "from pprint import pprint\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Determine if in Jupyter notebook or not\n",
    "\n",
    "try:\n",
    "    ip = get_ipython()\n",
    "    running_notebook = True\n",
    "except NameError:\n",
    "    running_notebook = False\n",
    "\n",
    "if running_notebook:\n",
    "    print('Running example in Jupyter Notebook')\n",
    "else:\n",
    "    print('Running example in script mode')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot topic historical analysis\n",
    "def topic_charts_historical(historical_metrics, selected_topics, show_sentiment_consensus):\n",
    "\n",
    "    # Charts of Strength, Sentiment, Consensus for each of the topics\n",
    "    current_max_strength = 0\n",
    "    current_max_sent = 0\n",
    "    current_max_cons = 0\n",
    "    for i in selected_topics:\n",
    "        if np.nanmax(np.abs(historical_metrics[i]['strength'])) > current_max_strength:\n",
    "            current_max_strength = np.nanmax(np.abs(historical_metrics[i]['strength']))\n",
    "        if np.nanmax(np.abs(historical_metrics[i]['sentiment'])) > current_max_sent:\n",
    "            current_max_sent = np.nanmax(np.abs(historical_metrics[i]['sentiment']))\n",
    "        if np.nanmax(np.abs(historical_metrics[i]['consensus'])) > current_max_cons:\n",
    "            current_max_cons = np.nanmax(np.abs(historical_metrics[i]['consensus']))\n",
    "\n",
    "    if show_sentiment_consensus == True:\n",
    "        plt.rcParams['figure.figsize'] = [12, 18*len(selected_topics)]\n",
    "        fig = plt.figure() \n",
    "        for i in selected_topics:\n",
    "            current_ax = 'ax' + str(selected_topics.index(i) + 1)\n",
    "            if current_ax == 'ax1':\n",
    "                current_ax = plt.subplot(3*len(selected_topics), 1, 1 + 3*selected_topics.index(i))\n",
    "                ax1 = plt.subplot(3*len(selected_topics), 1, 1 + 3*selected_topics.index(i))\n",
    "            else:\n",
    "                current_ax = plt.subplot(3*len(selected_topics), 1, 1 + 3*selected_topics.index(i), sharex=ax1)\n",
    "\n",
    "            # 3 subplots, one per strength, sentiment, consensus\n",
    "            current_ax.set_ylim([0, 1.05*current_max_strength])\n",
    "            current_ax.plot('time_stamps', 'strength', data=historical_metrics[i], color='blue')\n",
    "            plt.ylabel('Prevalence', fontsize=14, fontweight=\"bold\")\n",
    "            plt.title(historical_metrics[i]['topic'], fontsize=14, fontweight=\"bold\")\n",
    "\n",
    "            current_axSent = plt.subplot(3*len(selected_topics), 1, 1 + 3*selected_topics.index(i) + 1, sharex=ax1)\n",
    "            current_axSent.set_ylim([-1.05*current_max_sent, 1.05*current_max_sent])\n",
    "            current_axSent.plot('time_stamps', 'sentiment', data=historical_metrics[i], color='green')\n",
    "            plt.ylabel('Sentiment', fontsize=14, fontweight=\"bold\")\n",
    "\n",
    "            current_axCons = plt.subplot(3*len(selected_topics), 1, 1 + 3*selected_topics.index(i) + 2, sharex=ax1)\n",
    "            current_axCons.set_ylim([0, 1.05*current_max_cons])\n",
    "            current_axCons.plot('time_stamps', 'consensus', data=historical_metrics[i], color='red')\n",
    "            plt.ylabel('Consensus', fontsize=14, fontweight=\"bold\")\n",
    "\n",
    "        fig.autofmt_xdate(rotation=90)\n",
    "    else:\n",
    "        plt.rcParams['figure.figsize'] = [12, 6*len(selected_topics)]\n",
    "        fig = plt.figure()\n",
    "        for i in selected_topics:\n",
    "            current_ax = 'ax' + str(selected_topics.index(i) + 1)\n",
    "            if current_ax == 'ax1':\n",
    "                current_ax = plt.subplot(len(selected_topics), 1, selected_topics.index(i) + 1)\n",
    "                ax1 = plt.subplot(len(selected_topics), 1, selected_topics.index(i) + 1)\n",
    "            else:\n",
    "                current_ax = plt.subplot(len(selected_topics), 1, selected_topics.index(i) + 1, sharex=ax1)\n",
    "            current_ax.set_ylim([0, 1.05*current_max_strength])\n",
    "            current_ax.plot('time_stamps', 'strength', data=historical_metrics[i], color='blue')\n",
    "            plt.ylabel('Prevalence', fontsize=14, fontweight=\"bold\")        \n",
    "            plt.title(historical_metrics[i]['topic'], fontsize=14, fontweight=\"bold\")\n",
    "        fig.autofmt_xdate(rotation=90)\n",
    "        \n",
    "    return 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configure API host and key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configuration = nucleus_client.Configuration()\n",
    "configuration.host = 'http://localhost:5000'\n",
    "configuration.api_key['x-api-key'] = 'dK-R_zk3G-pV2ggiKN_LnA'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create API instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('-------------------------------------------------------------')\n",
    "print('--                Dataset API Examples                     --')\n",
    "print('-------------------------------------------------------------')\n",
    "api_instance_dataset = nucleus_client.DatasetsApi(nucleus_client.ApiClient(configuration))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Append file from local drive to dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('--------- Append file from local drive to dataset -----------')\n",
    "print('')\n",
    "file = 'quarles20181109a.pdf' # file | \n",
    "dataset = 'dataset_test' # str | Destination dataset where the file will be inserted.\n",
    "\n",
    "try:\n",
    "    api_instance_dataset.post_upload_file(file, dataset)\n",
    "except ApiException as e:\n",
    "    print(\"Exception when calling DatasetsApi->post_upload_file: %s\\n\" % e)\n",
    "\n",
    "print(file, 'has been added to dataset', dataset)\n",
    "print('-------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Append file from URL to dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('------------ Append file from URL to dataset ---------------')\n",
    "\n",
    "dataset = dataset\n",
    "file_url = 'https://www.federalreserve.gov/newsevents/speech/files/quarles20181109a.pdf'\n",
    "payload = nucleus_client.UploadURLModel(\n",
    "                dataset=dataset,\n",
    "                file_url=file_url\n",
    "            ) # UploadURLModel | \n",
    "\n",
    "try:\n",
    "    api_response = api_instance_dataset.post_upload_url(payload)\n",
    "except ApiException as e:\n",
    "    print(\"Exception when calling DatasetsApi->post_upload_url: %s\\n\" % e)\n",
    "    \n",
    "#pprint(api_response)   # raw API response\n",
    "print(file_url, 'has been added to dataset', dataset)\n",
    "print('-------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Append json from csv to dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('----------- Append json from CSV to dataset -----------------')\n",
    "# add documents to dataset\n",
    "csv_file = 'trump-tweets-100.csv'\n",
    "dataset = dataset   \n",
    "\n",
    "doc_cnt = 0\n",
    "with open(csv_file, encoding='utf-8-sig') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for row in reader:\n",
    "        if doc_cnt < 10:\n",
    "            payload = nucleus_client.Appendjsonparams(dataset=dataset, \n",
    "                                                  language='english', \n",
    "                                                  document={'time'   : row['time'],\n",
    "                                                            'title'  : row['title'],\n",
    "                                                            'content': row['content'],\n",
    "                                                            'author' : row['author']}\n",
    "                                                 )\n",
    "\n",
    "            try:\n",
    "                api_response = api_instance_dataset.post_append_json_to_dataset(payload)\n",
    "            except ApiException as e:\n",
    "                print(\"Exception when calling DatasetsApi->post_append_json_to_dataset: %s\\n\" % e)\n",
    "        \n",
    "        doc_cnt = doc_cnt + 1\n",
    "        \n",
    "print('Dataset', dataset, 'now has', api_response.num_documents, 'documents.')\n",
    "print('-------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List available datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('---------------- List available datasets ---------------------')\n",
    "try:\n",
    "    api_response = api_instance_dataset.get_list_datasets()\n",
    "except ApiException as e:\n",
    "    print(\"Exception when calling DatasetsApi->get_list_datasets: %s\\n\" % e)\n",
    "\n",
    "list_datasets = api_response.to_dict()['list_datasets']\n",
    "\n",
    "print(len(list_datasets), 'datasets in the database:')\n",
    "for ds in list_datasets:\n",
    "    print('    ', ds)\n",
    "\n",
    "    \n",
    "print('-------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get dataset information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('--------------- Get dataset information -------------------')\n",
    "dataset = dataset # str | Dataset name.\n",
    "query = '' # str | Fulltext query, using mysql MATCH boolean query format. (optional)\n",
    "metadata_selection = '' # str | json object of {\\\"metadata_field\\\":[\\\"selected_values\\\"]} (optional)\n",
    "time_period = '' # str | Time period selection (optional)\n",
    "\n",
    "try:\n",
    "    api_response = api_instance_dataset.get_dataset_info(\n",
    "        dataset, \n",
    "        query=query, \n",
    "        metadata_selection=metadata_selection, \n",
    "        time_period=time_period)\n",
    "except ApiException as e:\n",
    "    print(\"Exception when calling DatasetsApi->get_dataset_info: %s\\n\" % e)\n",
    "\n",
    "print('Information about dataset', dataset)\n",
    "print('    Language:', api_response.detected_language)\n",
    "print('    Number of documents:', api_response.num_documents)\n",
    "print('    Time range:', datetime.datetime.fromtimestamp(float(api_response.time_range[0])),\n",
    "             'to', datetime.datetime.fromtimestamp(float(api_response.time_range[1])))\n",
    "\n",
    "#pprint(api_response) # raw API response\n",
    "print('-------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('--------------------- Delete document -----------------------')\n",
    "dataset = dataset\n",
    "docid = '1'\n",
    "payload = nucleus_client.Deletedocumentmodel(dataset=dataset,\n",
    "                                             docid=docid) # Deletedocumentmodel | \n",
    "\n",
    "try:\n",
    "    api_response = api_instance_dataset.post_delete_document(payload)\n",
    "except ApiException as e:\n",
    "    print(\"Exception when calling DatasetsApi->post_delete_document: %s\\n\" % e)\n",
    "\n",
    "\n",
    "print('Document', docid, 'from dataset', dataset, 'has been deleted.')\n",
    "# print(api_response)     # raw API response\n",
    "print('-------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('--------------------- Delete dataset ------------------------')\n",
    "\n",
    "dataset = dataset  \n",
    "payload = nucleus_client.Deletedatasetmodel(dataset=dataset) # Deletedatasetmodel | \n",
    "\n",
    "try:\n",
    "    api_response = api_instance_dataset.post_delete_dataset(payload)\n",
    "    pprint(api_response)\n",
    "except ApiException as e:\n",
    "    print(\"Exception when calling DatasetsApi->post_delete_dataset: %s\\n\" % e)\n",
    "    \n",
    "# List datasets again to check if the specified dataset has been deleted\n",
    "try:\n",
    "    api_response = api_instance_dataset.get_list_datasets()\n",
    "    pprint(api_response)\n",
    "except ApiException as e:\n",
    "    print(\"Exception when calling DatasetsApi->get_list_datasets: %s\\n\" % e)\n",
    "    \n",
    "print('-------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a full dataset for testing other APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('--------- Create a full dataset for testing other APIs ---------')\n",
    "# add documents to dataset\n",
    "csv_file = 'trump-tweets-100.csv'\n",
    "dataset = 'trump_tweets'   \n",
    "\n",
    "with open(csv_file, encoding='utf-8-sig') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for row in reader:\n",
    "        payload = nucleus_client.Appendjsonparams(dataset=dataset, \n",
    "                                                  language='english', \n",
    "                                                  document={'time'   : row['time'],\n",
    "                                                            'title'  : row['title'],\n",
    "                                                            'content': row['content'],\n",
    "                                                            'author' : row['author']}\n",
    "                                                 )\n",
    "\n",
    "        try:\n",
    "            response = api_instance_dataset.post_append_json_to_dataset(payload)\n",
    "        except ApiException as e:\n",
    "            print(\"Exception when calling DatasetsApi->post_append_json_to_dataset: %s\\n\" % e)\n",
    "            \n",
    "print('Dataset', dataset, 'now has', response.num_documents, 'documents.')\n",
    "print('-------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create API Instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('-------------------------------------------------------------')\n",
    "print('--                Topic API Examples                     --')\n",
    "print('-------------------------------------------------------------')\n",
    "api_instance_topic = nucleus_client.TopicsApi(nucleus_client.ApiClient(configuration))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get list of topics from dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('------------- Get list of topics from dataset --------------')\n",
    "dataset = dataset\n",
    "#query = '(\"Trump\" OR \"president\")' # str | Fulltext query, using mysql MATCH boolean query format. Example, (\\\"word1\\\" OR \\\"word2\\\") AND (\\\"word3\\\" OR \\\"word4\\\") (optional)\n",
    "query = ''\n",
    "custom_stop_words = [\"real\",\"hillary\"] # str | List of stop words. (optional)\n",
    "num_topics = 8 # int | Number of topics to be extracted from the dataset. (optional) (default to 8)\n",
    "metadata_selection =\"\" # str | json object of {\\\"metadata_field\\\":[\\\"selected_values\\\"]} (optional)\n",
    "time_period =\"\"# str | Time period selection (optional)\n",
    "\n",
    "try:\n",
    "    api_response = api_instance_topic.get_topic_api(\n",
    "        dataset,                                \n",
    "        query=query,                   \n",
    "        custom_stop_words=custom_stop_words,     \n",
    "        num_topics=num_topics,\n",
    "        metadata_selection=metadata_selection,\n",
    "        time_period=time_period)\n",
    "except ApiException as e:\n",
    "    print(\"Exception when calling TopicsApi->get_topic_api: %s\\n\" % e)\n",
    "    \n",
    "#print(api_response)\n",
    "i = 1\n",
    "for res in api_response.results:\n",
    "    print('Topic', i, 'keywords:')\n",
    "    print('    Keywords:', res.topic)\n",
    "    keywords_weight_str = \";\".join(str(x) for x in res.keywords_weight)\n",
    "    print('    Keyword weights:', keywords_weight_str)\n",
    "    print('    Strength:', res.strength)\n",
    "    doc_topic_exposure_sel = []  # list of non-zero doc_topic_exposure\n",
    "    doc_id_sel = []        # list of doc ids matching doc_topic_exposure_sel\n",
    "    for j in range(len(res.doc_topic_exposure)):\n",
    "        doc_topic_exp = float(res.doc_topic_exposure[j])\n",
    "        if doc_topic_exp != 0:\n",
    "            doc_topic_exposure_sel.append(doc_topic_exp)\n",
    "            doc_id_sel.append(res.doc_id[j])\n",
    "    \n",
    "    doc_id_sel_str = ' '.join(str(x) for x in doc_id_sel)\n",
    "    doc_topic_exposure_sel_str = ' '.join(str(x) for x in doc_topic_exposure_sel)\n",
    "    print('    Document IDs:', doc_id_sel_str)\n",
    "    print('    Document exposures:', doc_topic_exposure_sel_str)\n",
    "\n",
    "    print('---------------')\n",
    "    i = i + 1\n",
    "    \n",
    "print('-------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get topic summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('------------------- Get topic summary -----------------------')\n",
    "dataset = dataset # str | Dataset name.\n",
    "#query = '(\"Trump\" OR \"president\")' # str | Fulltext query, using mysql MATCH boolean query format. Example, (\\\"word1\\\" OR \\\"word2\\\") AND (\\\"word3\\\" OR \\\"word4\\\") (optional)\n",
    "query = ''\n",
    "custom_stop_words = [\"real\",\"hillary\"] # str | List of stop words. (optional)\n",
    "num_topics = 8 # int | Number of topics to be extracted from the dataset. (optional) (default to 8)\n",
    "num_keywords = 8 # int | Number of keywords per topic that is extracted from the dataset. (optional) (default to 8)\n",
    "metadata_selection =\"\" # str | json object of {\\\"metadata_field\\\":[\\\"selected_values\\\"]} (optional)\n",
    "summary_length = 6 # int | The maximum number of bullet points a user wants to see in each topic summary. (optional) (default to 6)\n",
    "context_amount = 0 # int | The number of sentences surrounding key summary sentences in the documents that they come from. (optional) (default to 0)\n",
    "num_docs = 20 # int | The maximum number of key documents to use for summarization. (optional) (default to 20)\n",
    "excluded_docs = '' # str | List of document IDs that should be excluded from the analysis. Example, \\\"docid1, docid2, ..., docidN\\\"  (optional)\n",
    "\n",
    "try:\n",
    "    api_response = api_instance_topic.get_topic_summary_api(\n",
    "        dataset, \n",
    "        query=query, \n",
    "        custom_stop_words=custom_stop_words, \n",
    "        num_topics=num_topics, \n",
    "        num_keywords=num_keywords,\n",
    "        metadata_selection=metadata_selection,\n",
    "        summary_length=summary_length, \n",
    "        context_amount=context_amount, \n",
    "        num_docs=num_docs)\n",
    "except ApiException as e:\n",
    "    print(\"Exception when calling TopicsApi->get_topic_summary_api: %s\\n\" % e)\n",
    "\n",
    "i = 1\n",
    "for res in api_response.results:\n",
    "    print('Topic', i, 'summary:')\n",
    "    print('    Keywords:', res.topic)\n",
    "    for j in range(len(res.summary)):\n",
    "        print('    Document ID:', res.summary[j].sourceid)\n",
    "        print('        Title:', res.summary[j].title)\n",
    "        print('        Sentences:', res.summary[j].sentences)\n",
    "        print('        Author:', res.summary[j].attribute.author)\n",
    "        print('        Source:', res.summary[j].attribute.source)\n",
    "        print('        Time:', datetime.datetime.fromtimestamp(float(res.summary[j].attribute.time)))\n",
    "\n",
    "        #print(type(res.summary[j].attribute))\n",
    "        \n",
    "    print('---------------')\n",
    "    i = i + 1\n",
    "    \n",
    "    \n",
    "#pprint(api_response)  # raw API response\n",
    "print('-------------------------------------------------------------')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get topic sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('---------------- Get topic sentiment ------------------------')\n",
    "dataset = dataset # str | Dataset name.\n",
    "#query = '(\"Trump\" OR \"president\")' # str | Fulltext query, using mysql MATCH boolean query format. Example, (\\\"word1\\\" OR \\\"word2\\\") AND (\\\"word3\\\" OR \\\"word4\\\") (optional)\n",
    "query = ''\n",
    "custom_stop_words = [\"real\",\"hillary\"] # str | List of stop words. (optional)\n",
    "num_topics = 8 # int | Number of topics to be extracted from the dataset. (optional) (default to 8)\n",
    "num_keywords = 8 # int | Number of keywords per topic that is extracted from the dataset. (optional) (default to 8)\n",
    "excluded_docs = '' # str | List of document IDs that should be excluded from the analysis. Example, \\\"docid1, docid2, ..., docidN\\\"  (optional)\n",
    "\n",
    "try:\n",
    "    api_response = api_instance_topic.get_topic_sentiment_api(\n",
    "        dataset, \n",
    "        query=query, \n",
    "        custom_stop_words=custom_stop_words, \n",
    "        num_topics=num_topics, \n",
    "        num_keywords=num_keywords)\n",
    "    \n",
    "except ApiException as e:\n",
    "    print(\"Exception when calling TopicsApi->get_topic_sentiment_api: %s\\n\" % e)\n",
    "\n",
    "i = 1\n",
    "for res in api_response.results:\n",
    "    print('Topic', i, 'sentiment:')\n",
    "    print('    Keywords:', res.topic)\n",
    "    print('    Sentiment:', res.sentiment)\n",
    "    print('    Strength:', res.strength)\n",
    "    \n",
    "    doc_id_str = ' '.join(str(x) for x in res.doc_id)\n",
    "    doc_sentiment_str = ' '.join(str(x) for x in res.doc_sentiment)\n",
    "    doc_score_str = ' '.join(str(x) for x in res.doc_score)\n",
    "    print('    Doucment IDs:', doc_id_str)\n",
    "    print('    Doucment Sentiments:', doc_sentiment_str)\n",
    "    print('    Doucment Scores:', doc_score_str)\n",
    "    \n",
    "    print('---------------')\n",
    "    i = i + 1\n",
    "    \n",
    "#pprint(api_response)\n",
    "print('-------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get topic consensus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('---------------- Get topic consensus ------------------------')\n",
    "dataset = dataset # str | Dataset name.\n",
    "query = '' # str | Fulltext query, using mysql MATCH boolean query format. Example, (\\\"word1\\\" OR \\\"word2\\\") AND (\\\"word3\\\" OR \\\"word4\\\") (optional)\n",
    "custom_stop_words = [\"real\",\"hillary\"] # str | List of stop words. (optional)\n",
    "num_topics = 8 # int | Number of topics to be extracted from the dataset. (optional) (default to 8)\n",
    "num_keywords = 8 # int | Number of keywords per topic that is extracted from the dataset. (optional) (default to 8)\n",
    "excluded_docs = [''] # str | List of document IDs that should be excluded from the analysis. Example, \\\"docid1, docid2, ..., docidN\\\"  (optional)\n",
    "\n",
    "try:\n",
    "    api_response = api_instance_topic.get_topic_consensus_api(\n",
    "        dataset, \n",
    "        query=query, \n",
    "        custom_stop_words=custom_stop_words, \n",
    "        num_topics=num_topics, \n",
    "        num_keywords=num_keywords)\n",
    "except ApiException as e:\n",
    "    print(\"Exception when calling TopicsApi->get_topic_consensus_api: %s\\n\" % e)\n",
    "    \n",
    "i = 1\n",
    "for res in api_response.results:\n",
    "    print('Topic', i, 'consensus:')\n",
    "    print('    Keywords:', res.topic)\n",
    "    print('    Consensus:', res.consensus)\n",
    "    print('    Strength:', res.strength)\n",
    "    \n",
    "    print('---------------')\n",
    "    i = i + 1\n",
    "    \n",
    "#pprint(api_response) # raw API response\n",
    "print('-------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get topic historical analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('------------ Get topic historical analysis ----------------')\n",
    "\n",
    "dataset = dataset   # str | Dataset name.\n",
    "time_period = '6M'  # str | Time period selection (default to 1M)\n",
    "update_period = 'd' # str | Frequency at which the historical anlaysis is performed (default to d)\n",
    "query = '' # str | Fulltext query, using mysql MATCH boolean query format. Example, (\\\"word1\\\" OR \\\"word2\\\") AND (\\\"word3\\\" OR \\\"word4\\\") (optional)\n",
    "custom_stop_words = [\"real\",\"hillary\"] # str | List of stop words (optional)\n",
    "num_topics = 8 # int | Number of topics to be extracted from the dataset. (optional) (default to 8)\n",
    "num_keywords = 8 # int | Number of keywords per topic that is extracted from the dataset. (optional) (default to 8)\n",
    "metadata_selection = '' # str | json object of {\\\"metadata_field\\\":[\\\"selected_values\\\"]} (optional)\n",
    "inc_step = 1 # int | Number of increments of the udpate period in between two historical computations. (optional) (default to 1)\n",
    "excluded_docs = [''] # str | List of document IDs that should be excluded from the analysis. Example, \\\"docid1, docid2, ..., docidN\\\"  (optional)\n",
    "\n",
    "try:\n",
    "    api_response = api_instance_topic.get_topic_historical_analysis_api(\n",
    "        dataset, \n",
    "        time_period, \n",
    "        update_period, \n",
    "        query=query, \n",
    "        custom_stop_words=custom_stop_words, \n",
    "        num_topics=num_topics, \n",
    "        num_keywords=num_keywords, \n",
    "        metadata_selection=metadata_selection, \n",
    "        inc_step=inc_step, \n",
    "        excluded_docs=excluded_docs)\n",
    "    \n",
    "except ApiException as e:\n",
    "    print(\"Exception when calling TopicsApi->get_topic_historical_analysis_api: %s\\n\" % e)\n",
    "\n",
    "results = api_response.results\n",
    "\n",
    "# chart the historical metrics when running in Jupyter Notebook\n",
    "if running_notebook:\n",
    "    print('Plotting historical metrics data...')\n",
    "    historical_metrics = []\n",
    "    for res in results:\n",
    "        # conctruct a list of historical metrics dictionaries for charting\n",
    "        historical_metrics.append({\n",
    "            'topic'    : res.topic,\n",
    "            'time_stamps' : np.array(res.time_stamps),\n",
    "            'strength' : np.array(res.strength, dtype=np.float32),\n",
    "            'consensus': np.array(res.consensus, dtype=np.float32), \n",
    "            'sentiment': np.array(res.sentiment, dtype=np.float32)})\n",
    "\n",
    "    selected_topics = range(len(historical_metrics)) \n",
    "    topic_charts_historical(historical_metrics, selected_topics, True)\n",
    "else:\n",
    "    print('Printing historical metrics data...')\n",
    "    print('NOTE: historical metrics data can be plotted when running the example in Jupyter Notebook')\n",
    "    i = 1\n",
    "    for res in results:\n",
    "        print('Topic', i, res.topic)\n",
    "        print('    Timestamps:', res.time_stamps)\n",
    "        print('    Strength:', res.strength)\n",
    "        print('    Consensus:', res.consensus)\n",
    "        print('    Sentiment:', res.sentiment)\n",
    "        print('----------------')\n",
    "        i = i + 1\n",
    "#pprint(api_response)\n",
    "print('-------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get author connectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('----------------- Get author connectivity -------------------')\n",
    "dataset = dataset # str | Dataset name.\n",
    "target_author = 'D_Trump16' # str | Name of the author to be analyzed.\n",
    "query = '' # str | Fulltext query, using mysql MATCH boolean query format. Subject covered by the author, on which to focus the analysis of connectivity. Example, (\\\"word1\\\" OR \\\"word2\\\") AND (\\\"word3\\\" OR \\\"word4\\\") (optional)\n",
    "custom_stop_words = [\"real\",\"hillary\"] # str | List of words possibly used by the target author that are considered not information-bearing. (optional)\n",
    "time_period = '12M' # str | Time period selection. Required. Valid values: \"1M\",\"3M\",\"6M\",\"12M\",\"3Y\",\"5Y\"\n",
    "metadata_selection = '' # str | json object of {\\\"metadata_field\\\":[\\\"selected_values\\\"]} (optional)\n",
    "excluded_docs = [''] # str | List of document IDs that should be excluded from the analysis. Example, \\\"docid1, docid2, ..., docidN\\\"  (optional)\n",
    "\n",
    "try:\n",
    "    api_response = api_instance_topic.get_author_connectivity_api(\n",
    "        dataset, \n",
    "        target_author, \n",
    "        query=query, \n",
    "        custom_stop_words=custom_stop_words, \n",
    "        time_period=time_period, \n",
    "        metadata_selection=metadata_selection, \n",
    "        excluded_docs=excluded_docs)\n",
    "    \n",
    "except ApiException as e:\n",
    "    print(\"Exception when calling TopicsApi->get_author_connectivity_api: %s\\n\" % e)\n",
    "\n",
    "res = api_response.results\n",
    "print('Mainstream connections:')\n",
    "for mc in res.mainstream_connection:\n",
    "    print('    Topic:', mc.topic)\n",
    "    print('    Authors:', \" \".join(str(x) for x in mc.authors))\n",
    "    \n",
    "print('Niche connections:')\n",
    "for nc in res.niche_connection:\n",
    "    print('    Topic:', nc.topic)\n",
    "    print('    Authors:', \" \".join(str(x) for x in nc.authors))  \n",
    "    \n",
    "#pprint(api_response)   # raw API response\n",
    "print('-------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get topic delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('------------------- Get topic deltas -----------------------')\n",
    "dataset = 'trump_tweets' \n",
    "#dataset = dataset # str | Dataset name.\n",
    "#query = '(\"Trump\" OR \"president\")' # str | Fulltext query, using mysql MATCH boolean query format. Example, (\\\"word1\\\" OR \\\"word2\\\") AND (\\\"word3\\\" OR \\\"word4\\\") (optional)\n",
    "query = ''\n",
    "custom_stop_words = [\"\"] # str | List of stop words. (optional)\n",
    "num_topics = 8 # int | Number of topics to be extracted from the dataset. (optional) (default to 8)\n",
    "num_keywords = 8 # int | Number of keywords per topic that is extracted from the dataset. (optional) (default to 8)\n",
    "metadata_selection =\"\" # str | json object of {\\\"metadata_field\\\":[\\\"selected_values\\\"]} (optional)\n",
    "time_start_t0 = '2018-08-12 00:00:00'\n",
    "time_end_t0 = '2018-08-15 13:00:00'\n",
    "time_start_t1 = '2018-08-16 00:00:00'\n",
    "time_end_t1 = '2018-08-19 00:00:00'\n",
    "excluded_docs = '' # str | List of document IDs that should be excluded from the analysis. Example, \\\"docid1, docid2, ..., docidN\\\"  (optional)\n",
    "\n",
    "try:\n",
    "    api_response = api_instance_topic.get_topic_delta_api(\n",
    "        dataset, \n",
    "        query=query, \n",
    "        custom_stop_words=custom_stop_words, \n",
    "        num_topics=num_topics, \n",
    "        num_keywords=num_keywords,\n",
    "        time_start_t0 = time_start_t0,\n",
    "        time_end_t0 = time_end_t0,\n",
    "        time_start_t1 = time_start_t1,\n",
    "        time_end_t1 = time_end_t1,\n",
    "        metadata_selection=metadata_selection)\n",
    "except ApiException as e:\n",
    "    print(\"Exception when calling TopicsApi->get_topic_delta_api: %s\\n\" % e)\n",
    "\n",
    "i = 1\n",
    "for res in api_response.results:\n",
    "    print('Topic', i, 'changes in exposure:')\n",
    "    print('    Keywords:', res.topic)\n",
    "    print('    Document ID:', res.doc_id_t0, res.doc_id_t1)\n",
    "    print('    Per Source Change in Exposure:', res.doc_topic_exposure_delta)\n",
    "    print('---------------')\n",
    "    i = i + 1\n",
    "    \n",
    "    \n",
    "#pprint(api_response)  # raw API response\n",
    "print('-------------------------------------------------------------')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create API instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('-------------------------------------------------------------')\n",
    "print('--                Document API examples                    --')\n",
    "print('-------------------------------------------------------------')\n",
    "\n",
    "api_instance_doc = nucleus_client.DocumentsApi(nucleus_client.ApiClient(configuration))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get document information without content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset # str | Dataset name.\n",
    "doc_titles = ['D_Trump2018_8_18_1_47']   # str | The title of the document to retrieve. Example: \\\" \\\"title 1\\\" \\\"  (optional)\n",
    "doc_ids = ['11', '12', '20']      # int | The docid of the document to retrieve. Example: \\\"docid1\\\"  (optional)\n",
    "\n",
    "try:\n",
    "    api_response = api_instance_doc.get_doc_info(dataset, doc_titles=doc_titles, doc_ids=doc_ids)\n",
    "    \n",
    "except ApiException as e:\n",
    "    print(\"Exception when calling DocumentsApi->get_doc_info: %s\\n\" % e)\n",
    "    \n",
    "for res in api_response.results:\n",
    "    print('Document ID:', res.sourceid)\n",
    "    print('    Title:', res.title)\n",
    "    print('    Author:', res.attribute.author)\n",
    "    print('    Source:', res.attribute.source)\n",
    "    print('    Time:', datetime.datetime.fromtimestamp(float(res.attribute.time)))\n",
    "\n",
    "    print('---------------')\n",
    "    \n",
    "    \n",
    "#pprint(api_response)  # raw response from API server\n",
    "print('-------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display document details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('-------------------------------------------------------------')\n",
    "\n",
    "dataset = dataset # str | Dataset name.\n",
    "doc_titles = ['D_Trump2018_8_18_1_47']   # str | The title of the document to retrieve. Example: \\\" \\\"title 1\\\" \\\"  (optional)\n",
    "doc_ids = ['11']      # int | The docid of the document to retrieve. Example: \\\"docid1\\\"  (optional)\n",
    "\n",
    "try:\n",
    "    api_response = api_instance_doc.get_doc_display(dataset, doc_titles=doc_titles, doc_ids=doc_ids)\n",
    "    \n",
    "except ApiException as e:\n",
    "    print(\"Exception when calling DocumentsApi->get_doc_display_api: %s\\n\" % e)\n",
    "\n",
    "for res in api_response.results:\n",
    "    print('Document ID:', res.sourceid)\n",
    "    print('    Title:', res.title)\n",
    "    print('    Author:', res.attribute.author)\n",
    "    print('    Source:', res.attribute.source)\n",
    "    print('    Time:', datetime.datetime.fromtimestamp(float(res.attribute.time)))\n",
    "    print('    Content', res.content)\n",
    "\n",
    "    print('---------------')\n",
    "\n",
    "\n",
    "#pprint(api_response) # raw response from API server\n",
    "print('-------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get document recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('------------- Get document recommendations -----------------')\n",
    "\n",
    "dataset = dataset # str | Dataset name.\n",
    "#query = '(\"Trump\" OR \"president\")' # str | Fulltext query, using mysql MATCH boolean query format. Example, (\\\"word1\\\" OR \\\"word2\\\") AND (\\\"word3\\\" OR \\\"word4\\\") (optional)\n",
    "query = ''\n",
    "custom_stop_words = [\"real\",\"hillary\"] # ERRORUNKNOWN | List of stop words. (optional)\n",
    "num_topics = 8 # int | Number of topics to be extracted from the dataset. (optional) (default to 8)\n",
    "num_keywords = 8 # int | Number of keywords per topic that is extracted from the dataset. (optional) (default to 8)\n",
    "excluded_docs = '' # str | List of document IDs that should be excluded from the analysis. Example, \\\"docid1, docid2, ..., docidN\\\"  (optional)\n",
    "\n",
    "try:\n",
    "    api_response = api_instance_doc.get_doc_recommend_api(\n",
    "        dataset, \n",
    "        query=query, \n",
    "        custom_stop_words=custom_stop_words, \n",
    "        num_topics=num_topics, \n",
    "        num_keywords=num_keywords)\n",
    "except ApiException as e:\n",
    "    print(\"Exception when calling DocumentsApi->get_doc_recommend_api: %s\\n\" % e)\n",
    "    \n",
    "i = 1\n",
    "for res in api_response.results:\n",
    "    print('Document recommendations for topic', i, ':')\n",
    "    print('    Keywords:', res.topic)\n",
    "\n",
    "    j = 1\n",
    "    for doc in res.recommendations:\n",
    "        print('    Recommendation', j, ':')\n",
    "        print('        Document ID:', doc.sourceid)\n",
    "        print('        Title:', doc.title)\n",
    "        print('        Attribute:', doc.attribute)\n",
    "        print('        Author:', doc.attribute.author)\n",
    "        print('        Source:', doc.attribute.source)\n",
    "        print('        Time:', datetime.datetime.fromtimestamp(float(doc.attribute.time)))\n",
    "        j = j + 1\n",
    "    \n",
    "    print('---------------')\n",
    "    i = i + 1\n",
    "    \n",
    "#pprint(api_response)   # raw API response\n",
    "print('-------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get document summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('------------------ Get document summary  --------------------')\n",
    "\n",
    "dataset = dataset # str | Dataset name.\n",
    "doc_title = 'D_Trump2018_8_15_15_4' # str | The title of the document to be summarized.\n",
    "custom_stop_words = [\"real\",\"hillary\"] # ERRORUNKNOWN | List of stop words. (optional)\n",
    "summary_length = 6 # int | The maximum number of bullet points a user wants to see in the document summary. (optional) (default to 6)\n",
    "context_amount = 0 # int | The number of sentences surrounding key summary sentences in the documents that they come from. (optional) (default to 0)\n",
    "\n",
    "try:\n",
    "    api_response = api_instance_doc.get_doc_summary_api(\n",
    "        dataset, \n",
    "        doc_title, \n",
    "        custom_stop_words=custom_stop_words, \n",
    "        summary_length=summary_length, \n",
    "        context_amount=context_amount)\n",
    "    \n",
    "except ApiException as e:\n",
    "    print(\"Exception when calling DocumentsApi->get_doc_summary_api: %s\\n\" % e)\n",
    " \n",
    "print('Document Summary')\n",
    "print('    ID:', api_response.summary.sourceid)\n",
    "print('    Title:', api_response.doc_title)\n",
    "print('    Summary:', api_response.summary.sentences)\n",
    "\n",
    "#pprint(api_response)   # raw API response\n",
    "print('-------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
