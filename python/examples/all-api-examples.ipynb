{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization, configure API host and key, and create new API instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running example in Jupyter Notebook\n"
     ]
    }
   ],
   "source": [
    "# from __future__ import print_function\n",
    "import csv, json, datetime\n",
    "import time\n",
    "import nucleus_api\n",
    "from nucleus_api.rest import ApiException\n",
    "from pprint import pprint\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Determine if in Jupyter notebook or not\n",
    "\n",
    "try:\n",
    "    ip = get_ipython()\n",
    "    running_notebook = True\n",
    "except NameError:\n",
    "    running_notebook = False\n",
    "\n",
    "if running_notebook:\n",
    "    print('Running example in Jupyter Notebook')\n",
    "else:\n",
    "    print('Running example in script mode')\n",
    "    \n",
    "configuration = nucleus_api.Configuration()\n",
    "configuration.host = 'UPDATE-WITH-API-HOST'\n",
    "configuration.api_key['x-api-key'] = 'UPDATE-WITH-API-KEY'\n",
    "\n",
    "# Create API instance\n",
    "api_instance = nucleus_api.NucleusApi(nucleus_api.ApiClient(configuration))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Append file from local drive to dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- Append file from local drive to dataset -----------\n",
      "api_response= {'job_id': None, 'result': 'quarles20181109a.pdf'}\n",
      "quarles20181109a.pdf has been added to dataset dataset_test\n",
      "-------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print('--------- Append file from local drive to dataset -----------')\n",
    "file = 'quarles20181109a.pdf'         # file | \n",
    "dataset = 'dataset_test'              # str | Destination dataset where the file will be inserted.\n",
    "metadata = {\"time\": \"1/2/2018\", \n",
    "            \"author\": \"Test Author\"}  # Optional json containing additional document metadata\n",
    "\n",
    "try:\n",
    "    api_response = api_instance.post_upload_file(file, dataset, metadata=metadata)\n",
    "    \n",
    "except ApiException as e:\n",
    "    print(\"Exception when calling DatasetsApi->post_upload_file: %s\\n\" % e)\n",
    "    exit\n",
    "\n",
    "print('api_response=', api_response)   # raw API response\n",
    "print(api_response.result, 'has been added to dataset', dataset)\n",
    "print('-------------------------------------------------------------')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Append file from URL to dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------ Append file from URL to dataset ---------------\n",
      "api_response= {'job_id': None,\n",
      " 'result': 'https://www.federalreserve.gov/newsevents/speech/files/quarles20181109a.pdf'}\n",
      "https://www.federalreserve.gov/newsevents/speech/files/quarles20181109a.pdf has been added to dataset dataset_test\n",
      "-------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print('------------ Append file from URL to dataset ---------------')\n",
    "\n",
    "dataset = 'dataset_test'\n",
    "file_url = 'https://www.federalreserve.gov/newsevents/speech/files/quarles20181109a.pdf'\n",
    "payload = nucleus_api.UploadURLModel(\n",
    "                dataset=dataset,\n",
    "                file_url=file_url\n",
    "            ) # UploadURLModel | \n",
    "\n",
    "try:\n",
    "    api_response = api_instance.post_upload_url(payload)\n",
    "except ApiException as e:\n",
    "    print(\"Exception when calling DatasetsApi->post_upload_url: %s\\n\" % e)\n",
    "    \n",
    "print('api_response=', api_response)   # raw API response\n",
    "print(api_response.result, 'has been added to dataset', dataset)\n",
    "print('-------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Append json from csv to dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- Append json from CSV to dataset -----------------\n",
      "api_response {'job_id': None, 'result': '3'}\n",
      "Dataset dataset_test now has 3 documents.\n",
      "-------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print('----------- Append json from CSV to dataset -----------------')\n",
    "# add documents to dataset\n",
    "csv_file = 'trump-tweets-100.csv'\n",
    "dataset = 'dataset_test'  \n",
    "\n",
    "doc_cnt = 0\n",
    "with open(csv_file, encoding='utf-8-sig') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for row in reader:\n",
    "        if doc_cnt < 1:\n",
    "            payload = nucleus_api.Appendjsonparams(dataset=dataset, \n",
    "                                                  language='english', \n",
    "                                                  document={'time'   : row['time'],\n",
    "                                                            'title'  : row['title'],\n",
    "                                                            'content': row['content'],\n",
    "                                                            'author' : row['author']}\n",
    "                                                 )\n",
    "\n",
    "            try:\n",
    "                api_response = api_instance.post_append_json_to_dataset(payload)\n",
    "                print('api_response', api_response)\n",
    "            except ApiException as e:\n",
    "                print(\"Exception when calling DatasetsApi->post_append_json_to_dataset: %s\\n\" % e)\n",
    "        \n",
    "        doc_cnt = doc_cnt + 1\n",
    "        \n",
    "print('Dataset', dataset, 'now has', api_response.result, 'documents.')\n",
    "print('-------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List available datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------- List available datasets ---------------------\n",
      "2 datasets in the database:\n",
      "     dataset_test\n",
      "     trump_tweets\n",
      "-------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print('---------------- List available datasets ---------------------')\n",
    "try:\n",
    "    api_response = api_instance.get_list_datasets()\n",
    "except ApiException as e:\n",
    "    print(\"Exception when calling DatasetsApi->get_list_datasets: %s\\n\" % e)\n",
    "\n",
    "list_datasets = api_response.result\n",
    "\n",
    "print(len(list_datasets), 'datasets in the database:')\n",
    "for ds in list_datasets:\n",
    "    print('    ', ds)\n",
    "\n",
    "    \n",
    "print('-------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get dataset information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------- Get dataset information -------------------\n",
      "api_response= {'job_id': None,\n",
      " 'result': {'dataset': 'dataset_test',\n",
      "            'detected_language': 'en',\n",
      "            'metadata': '{}',\n",
      "            'num_documents': '3',\n",
      "            'time_range': ['1514880000.0', '1544846196.0']}}\n",
      "Information about dataset dataset_test\n",
      "    Language: en\n",
      "    Number of documents: 3\n",
      "    Time range: 2018-01-02 00:00:00 to 2018-12-14 19:56:36\n",
      "-------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print('--------------- Get dataset information -------------------')\n",
    "dataset = 'dataset_test' # str | Dataset name.\n",
    "query = '' # str | Fulltext query, using mysql MATCH boolean query format. (optional)\n",
    "metadata_selection = '' # str | json object of {\\\"metadata_field\\\":[\\\"selected_values\\\"]} (optional)\n",
    "time_period = '' # str | Time period selection (optional)\n",
    "\n",
    "try:\n",
    "    api_response = api_instance.get_dataset_info(\n",
    "        dataset, \n",
    "        query=query, \n",
    "        metadata_selection=metadata_selection, \n",
    "        time_period=time_period)\n",
    "    print('api_response=', api_response) # raw API response\n",
    "except ApiException as e:\n",
    "    print(\"Exception when calling DatasetsApi->get_dataset_info: %s\\n\" % e)\n",
    "\n",
    "print('Information about dataset', dataset)\n",
    "print('    Language:', api_response.result.detected_language)\n",
    "print('    Number of documents:', api_response.result.num_documents)\n",
    "print('    Time range:', datetime.datetime.fromtimestamp(float(api_response.result.time_range[0])),\n",
    "             'to', datetime.datetime.fromtimestamp(float(api_response.result.time_range[1])))\n",
    "\n",
    "\n",
    "print('-------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------- Delete document -----------------------\n",
      "Document 1 from dataset dataset_test has been deleted.\n",
      "-------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print('--------------------- Delete document -----------------------')\n",
    "dataset = 'dataset_test'\n",
    "docid = '1'\n",
    "payload = nucleus_api.Deletedocumentmodel(dataset=dataset,\n",
    "                                             docid=docid) # Deletedocumentmodel | \n",
    "\n",
    "try:\n",
    "    api_response = api_instance.post_delete_document(payload)\n",
    "except ApiException as e:\n",
    "    print(\"Exception when calling DatasetsApi->post_delete_document: %s\\n\" % e)\n",
    "\n",
    "\n",
    "print('Document', docid, 'from dataset', dataset, 'has been deleted.')\n",
    "# print(api_response)     # raw API response\n",
    "print('-------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------- Delete dataset ------------------------\n",
      "{'job_id': None, 'result': 'Dataset deleted'}\n",
      "api_response= {'job_id': None, 'result': ['trump_tweets']}\n",
      "-------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print('--------------------- Delete dataset ------------------------')\n",
    "\n",
    "dataset = 'dataset_test'  \n",
    "payload = nucleus_api.Deletedatasetmodel(dataset=dataset) # Deletedatasetmodel | \n",
    "\n",
    "try:\n",
    "    api_response = api_instance.post_delete_dataset(payload)\n",
    "    pprint(api_response)\n",
    "except ApiException as e:\n",
    "    print(\"Exception when calling DatasetsApi->post_delete_dataset: %s\\n\" % e)\n",
    "    \n",
    "# List datasets again to check if the specified dataset has been deleted\n",
    "try:\n",
    "    api_response = api_instance.get_list_datasets()\n",
    "    print('api_response=', api_response)\n",
    "except ApiException as e:\n",
    "    print(\"Exception when calling DatasetsApi->get_list_datasets: %s\\n\" % e)\n",
    "    \n",
    "print('-------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a full dataset for testing other APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- Create a full dataset for testing other APIs ---------\n",
      "api_response= {'job_id': None, 'result': '3100'}\n",
      "Dataset trump_tweets now has 3100 documents.\n",
      "-------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print('--------- Create a full dataset for testing other APIs ---------')\n",
    "# add documents to dataset\n",
    "csv_file = 'trump-tweets-100.csv'\n",
    "dataset = 'trump_tweets'   \n",
    "\n",
    "with open(csv_file, encoding='utf-8-sig') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for row in reader:\n",
    "        payload = nucleus_api.Appendjsonparams(dataset=dataset, \n",
    "                                                  language='english', \n",
    "                                                  document={'time'   : row['time'],\n",
    "                                                            'title'  : row['title'],\n",
    "                                                            'content': row['content'],\n",
    "                                                            'author' : row['author']}\n",
    "                                                 )\n",
    "\n",
    "        try:\n",
    "            api_response = api_instance.post_append_json_to_dataset(payload)\n",
    "        except ApiException as e:\n",
    "            print(\"Exception when calling DatasetsApi->post_append_json_to_dataset: %s\\n\" % e)\n",
    "            \n",
    "print('api_response=', api_response)\n",
    "print('Dataset', dataset, 'now has', api_response.result, 'documents.')\n",
    "print('-------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get list of topics from dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------- Get list of topics from dataset --------------\n",
      "Topic 1 keywords:\n",
      "    Keywords: america great;great great;andrew cuomo;taxed andrew;highest taxed;great believe;governor highest;believe governor\n",
      "    Keyword weights: 0.14795797393756516;0.16695785166789107;0.14904268327507322;0.10720829822389409;0.10720829822389409;0.10720829822389409;0.10720829822389409;0.10720829822389409\n",
      "    Strength: 0.17477316796369655\n",
      "    Document IDs: 16 17 18 24 44 45\n",
      "    Document exposures: 0.13842219371178488 0.0804584723966716 0.07557757475759962 0.09427887714883187 0.312912509996506 0.2983503719886059\n",
      "---------------\n",
      "Topic 2 keywords:\n",
      "    Keywords: illegal aliens;united permission;permission illegal;legally chuck;enter united;chuck schumer;aliens treated;aliens illegal\n",
      "    Keyword weights: 0.1111111111111111;0.1111111111111111;0.1111111111111111;0.1111111111111111;0.2222222222222222;0.1111111111111111;0.1111111111111111;0.1111111111111111\n",
      "    Strength: 0.12529311552435418\n",
      "    Document IDs: 57 63\n",
      "    Document exposures: 0.5 0.5\n",
      "---------------\n",
      "Topic 3 keywords:\n",
      "    Keywords: bruce ohr;peter strzok;fired fbi;agent peter;wife nelly;ohr justice;justice department;fusion gps\n",
      "    Keyword weights: 0.17704642562739586;0.11485991166063131;0.11485991166063131;0.11485991166063131;0.11485991166063131;0.12117130924335964;0.12117130924335964;0.12117130924335964\n",
      "    Strength: 0.16651861972651685\n",
      "    Document IDs: 11 12 13 31 33 68 70 91\n",
      "    Document exposures: 0.05799296722944725 0.08858572070134517 0.048520397500979105 0.10228997938655726 0.30640242163327774 0.10221071807041876 0.17896342760444403 0.11503436787353073\n",
      "---------------\n",
      "Topic 4 keywords:\n",
      "    Keywords: witch hunt;donald trump;frame donald;hillary clinton;rigged witch;trump gregg;gregg jarrett;clinton frame\n",
      "    Keyword weights: 0.08644792328377869;0.10668662079612214;0.08930771280814101;0.21697599423263134;0.14665673578828484;0.18102916652348455;0.08644792328377869;0.08644792328377869\n",
      "    Strength: 0.14660981702170603\n",
      "    Document IDs: 16 46 53 67 69 90 91 96 99\n",
      "    Document exposures: 0.03843228001612903 0.2677971675231634 0.0628738072730804 0.11033418347614264 0.3140968410154378 0.03406657517653554 0.06693427915859546 0.055845149187054514 0.04961971717386124\n",
      "---------------\n",
      "Topic 5 keywords:\n",
      "    Keywords: fake news;news media;wacky omarosa;presidential lowlife;omarosa modern;modern form;lowlife omarosa;form communication\n",
      "    Keyword weights: 0.18180528180141983;0.14262476425556084;0.13720328827832373;0.10767333313293909;0.10767333313293909;0.10767333313293909;0.10767333313293909;0.10767333313293909\n",
      "    Strength: 0.100587731732617\n",
      "    Document IDs: 8 38 40 89 93 95\n",
      "    Document exposures: 0.07823291401602575 0.0958153602151467 0.24180468243319012 0.1022605580282864 0.3983910893134939 0.08349539599385718\n",
      "---------------\n",
      "Topic 6 keywords:\n",
      "    Keywords: trump campaign;lou dobbs;forward special;evidence collusion;dobbs forward;democrats evidence;conflicts angry;angry democrats\n",
      "    Keyword weights: 0.14007373683036595;0.12284660902423344;0.12284660902423344;0.12284660902423344;0.12284660902423344;0.12284660902423344;0.12284660902423344;0.12284660902423344\n",
      "    Strength: 0.11687186488497478\n",
      "    Document IDs: 27 43 66 73\n",
      "    Document exposures: 0.05089655322295002 0.052153447503261105 0.4162759433607732 0.4806740559130157\n",
      "---------------\n",
      "Topic 7 keywords:\n",
      "    Keywords: strzok firing;judicial watch;watch strzok;tom fitton;mueller operation;fitton judicial;firing mueller;firing decisive\n",
      "    Keyword weights: 0.169967170507853;0.17431650476619812;0.025828238980726895;0.1259776171490444;0.1259776171490444;0.1259776171490444;0.1259776171490444;0.1259776171490444\n",
      "    Strength: 0.08654967168663112\n",
      "    Document IDs: 28 52 74 76\n",
      "    Document exposures: 0.1128563149776027 0.11508948362847467 0.22860053963526916 0.5434536617586534\n",
      "---------------\n",
      "Topic 8 keywords:\n",
      "    Keywords: john brennan;cia director;serving cia;mistakes john;looked mistakes;brennan serving;tuckercarlson speaking;speaking john\n",
      "    Keyword weights: 0.11354335520734882;0.11354335520734882;0.21461784150703836;0.11354335520734882;0.11354335520734882;0.1860984826927547;0.07255512748540587;0.07255512748540587\n",
      "    Strength: 0.08279601145950366\n",
      "    Document IDs: 4 30 47\n",
      "    Document exposures: 0.5387633645951668 0.33371686842898046 0.12751976697585257\n",
      "---------------\n",
      "-------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print('------------- Get list of topics from dataset --------------')\n",
    "dataset = 'trump_tweets'\n",
    "#query = '(\"Trump\" OR \"president\")' # str | Fulltext query, using mysql MATCH boolean query format. Example, (\\\"word1\\\" OR \\\"word2\\\") AND (\\\"word3\\\" OR \\\"word4\\\") (optional)\n",
    "query = ''\n",
    "custom_stop_words = [\"real\",\"hillary\"] # str | List of stop words. (optional)\n",
    "num_topics = 8 # int | Number of topics to be extracted from the dataset. (optional) (default to 8)\n",
    "metadata_selection =\"\" # str | json object of {\\\"metadata_field\\\":[\\\"selected_values\\\"]} (optional)\n",
    "time_period =\"\"# str | Time period selection (optional)\n",
    "\n",
    "try:\n",
    "    api_response = api_instance.get_topic_api(\n",
    "        dataset,                                \n",
    "        query=query,                   \n",
    "        custom_stop_words=custom_stop_words,     \n",
    "        num_topics=num_topics,\n",
    "        metadata_selection=metadata_selection,\n",
    "        time_period=time_period)\n",
    "except ApiException as e:\n",
    "    print(\"Exception when calling TopicsApi->get_topic_api: %s\\n\" % e)\n",
    "    \n",
    "#print(api_response)\n",
    "i = 1\n",
    "for res in api_response.result:\n",
    "    print('Topic', i, 'keywords:')\n",
    "    print('    Keywords:', res.topic)\n",
    "    keywords_weight_str = \";\".join(str(x) for x in res.keywords_weight)\n",
    "    print('    Keyword weights:', keywords_weight_str)\n",
    "    print('    Strength:', res.strength)\n",
    "    doc_topic_exposure_sel = []  # list of non-zero doc_topic_exposure\n",
    "    doc_id_sel = []        # list of doc ids matching doc_topic_exposure_sel\n",
    "    for j in range(len(res.doc_topic_exposure)):\n",
    "        doc_topic_exp = float(res.doc_topic_exposure[j])\n",
    "        if doc_topic_exp != 0:\n",
    "            doc_topic_exposure_sel.append(doc_topic_exp)\n",
    "            doc_id_sel.append(res.doc_id[j])\n",
    "    \n",
    "    doc_id_sel_str = ' '.join(str(x) for x in doc_id_sel)\n",
    "    doc_topic_exposure_sel_str = ' '.join(str(x) for x in doc_topic_exposure_sel)\n",
    "    print('    Document IDs:', doc_id_sel_str)\n",
    "    print('    Document exposures:', doc_topic_exposure_sel_str)\n",
    "\n",
    "    print('---------------')\n",
    "    i = i + 1\n",
    "    \n",
    "print('-------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get topic summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------- Get topic summary -----------------------\n",
      "Topic 1 summary:\n",
      "    Keywords: america great;great great;andrew cuomo;taxed andrew;highest taxed;great believe;governor highest;believe governor\n",
      "    Document ID: 16\n",
      "        Title: D_Trump2018_8_17_19_25\n",
      "        Sentences: [\"Which is worse Hightax Andrew Cuomo's statement “WE’RE NOT GOING TO MAKE AMERICA GREAT AGAIN IT WAS NEVER THAT GREAT” or Hillary Clinton’s “DEPLORABLES” statement...\"]\n",
      "        Author: D_Trump16\n",
      "        Source: None\n",
      "        Time: 2018-08-17 19:25:00\n",
      "    Document ID: 24\n",
      "        Title: D_Trump2018_8_17_11_44\n",
      "        Sentences: ['How does a politician Cuomo known for pushing people and businesses out of his state not to mention having the highest taxes in the U.S. survive making the statement WE’RE NOT GOING TO MAKE AMERICA GREAT AGAIN IT WAS NEVER THAT GREAT?']\n",
      "        Author: D_Trump24\n",
      "        Source: None\n",
      "        Time: 2018-08-17 11:44:00\n",
      "    Document ID: 44\n",
      "        Title: D_Trump2018_8_16_2_2\n",
      "        Sentences: ['“WE’RE NOT GOING TO MAKE AMERICA GREAT AGAIN IT WAS NEVER THAT GREAT.” Can you believe this is the Governor of the Highest Taxed State in the U.S. Andrew Cuomo having a total meltdown!']\n",
      "        Author: D_Trump44\n",
      "        Source: None\n",
      "        Time: 2018-08-16 02:02:00\n",
      "    Document ID: 45\n",
      "        Title: D_Trump2018_8_16_1_53\n",
      "        Sentences: ['“WE’RE NOT GONG TO MAKE AMERICA GREAT AGAIN IT WAS NEVER THAT GREAT.”  Can you believe this is the Governor of the Highest Taxed State in the U.S. Andrew Cuomo having a total meltdown!']\n",
      "        Author: D_Trump45\n",
      "        Source: None\n",
      "        Time: 2018-08-16 01:53:00\n",
      "---------------\n",
      "Topic 2 summary:\n",
      "    Keywords: illegal aliens;united permission;permission illegal;legally chuck;enter united;chuck schumer;aliens treated;aliens illegal\n",
      "    Document ID: 63\n",
      "        Title: D_Trump2018_8_15_12_44\n",
      "        Sentences: ['“People who enter the United States without our permission are illegal aliens and illegal aliens should not be treated the same as people who enters the U.S. legally.”  Chuck Schumer in 2009 before he went left and haywire!']\n",
      "        Author: D_Trump63\n",
      "        Source: None\n",
      "        Time: 2018-08-15 12:44:00\n",
      "---------------\n",
      "Topic 3 summary:\n",
      "    Keywords: bruce ohr;peter strzok;fired fbi;agent peter;wife nelly;ohr justice;justice department;fusion gps\n",
      "    Document ID: 33\n",
      "        Title: D_Trump2018_8_16_23_37\n",
      "        Sentences: ['“The FBI received documents from Bruce Ohr (of the Justice Department  whose wife Nelly worked for Fusion GPS).” Disgraced and fired FBI Agent Peter Strzok.']\n",
      "        Author: D_Trump33\n",
      "        Source: None\n",
      "        Time: 2018-08-16 23:37:00\n",
      "    Document ID: 70\n",
      "        Title: D_Trump2018_8_14_11_55\n",
      "        Sentences: ['Bruce Ohr of the “Justice” Department (can you believe he is still there) is accused of helping disgraced Christopher Steele “find dirt on Trump.” Ohr’s wife Nelly was in on the act big time - worked for Fusion GPS on Fake Dossier.']\n",
      "        Author: D_Trump70\n",
      "        Source: None\n",
      "        Time: 2018-08-14 11:55:00\n",
      "    Document ID: 91\n",
      "        Title: D_Trump2018_8_13_16_4\n",
      "        Sentences: ['Agent Peter Strzok was just fired from the FBI - finally.']\n",
      "        Author: D_Trump91\n",
      "        Source: None\n",
      "        Time: 2018-08-13 16:04:00\n",
      "---------------\n",
      "Topic 4 summary:\n",
      "    Keywords: witch hunt;donald trump;frame donald;hillary clinton;rigged witch;trump gregg;gregg jarrett;clinton frame\n",
      "    Document ID: 46\n",
      "        Title: D_Trump2018_8_16_1_14\n",
      "        Sentences: ['We have the unfortunate situation where they then decided they were going to frame Donald Trump” concerning the Rigged Witch Hunt.']\n",
      "        Author: D_Trump46\n",
      "        Source: None\n",
      "        Time: 2018-08-16 01:14:00\n",
      "    Document ID: 67\n",
      "        Title: D_Trump2018_8_14_13_10\n",
      "        Sentences: ['Strzok started the illegal Rigged Witch Hunt - why isn’t this so-called “probe” ended immediately?']\n",
      "        Author: D_Trump67\n",
      "        Source: None\n",
      "        Time: 2018-08-14 13:10:00\n",
      "    Document ID: 69\n",
      "        Title: D_Trump2018_8_14_12_6\n",
      "        Sentences: ['“They were all in on it clear Hillary Clinton and FRAME Donald Trump for things he didn’t do.” Gregg Jarrett on @foxandfriends  If we had a real Attorney General this Witch Hunt would never have been started!']\n",
      "        Author: D_Trump69\n",
      "        Source: None\n",
      "        Time: 2018-08-14 12:06:00\n",
      "---------------\n",
      "Topic 5 summary:\n",
      "    Keywords: fake news;news media;wacky omarosa;presidential lowlife;omarosa modern;modern form;lowlife omarosa;form communication\n",
      "    Document ID: 93\n",
      "        Title: D_Trump2018_8_13_14_21\n",
      "        Sentences: ['While I know it’s “not presidential” to take on a lowlife like Omarosa and while I would rather not be doing so this is a modern day form of communication and I know the Fake News Media will be working overtime to make even Wacky Omarosa look legitimate as possible.']\n",
      "        Author: D_Trump93\n",
      "        Source: None\n",
      "        Time: 2018-08-13 14:21:00\n",
      "---------------\n",
      "Topic 6 summary:\n",
      "    Keywords: trump campaign;lou dobbs;forward special;evidence collusion;dobbs forward;democrats evidence;conflicts angry;angry democrats\n",
      "    Document ID: 73\n",
      "        Title: D_Trump2018_8_14_11_21\n",
      "        Sentences: ['Lou Dobbs: “This cannot go forward...this Special Councel with all of his conflicts with his 17 Angry Democrats without any evidence of collusion by the Trump Campaign and Russia.']\n",
      "        Author: D_Trump73\n",
      "        Source: None\n",
      "        Time: 2018-08-14 11:21:00\n",
      "---------------\n",
      "Topic 7 summary:\n",
      "    Keywords: strzok firing;judicial watch;watch strzok;tom fitton;mueller operation;fitton judicial;firing mueller;firing decisive\n",
      "    Document ID: 52\n",
      "        Title: D_Trump2018_8_15_14_15\n",
      "        Sentences: ['“The action (the Strzok firing) was a decisive step in the right direction in correcting the wrongs committed by what has been described as Comey’s skinny inner circle.”  Chris Swecker former FBI Assistant Director.']\n",
      "        Author: D_Trump52\n",
      "        Source: None\n",
      "        Time: 2018-08-15 14:15:00\n",
      "    Document ID: 76\n",
      "        Title: D_Trump2018_8_14_10_59\n",
      "        Sentences: ['Tom Fitton of Judicial Watch: “The Strzok firing is as much about the Mueller operation as anything else.']\n",
      "        Author: D_Trump76\n",
      "        Source: None\n",
      "        Time: 2018-08-14 10:59:00\n",
      "---------------\n",
      "Topic 8 summary:\n",
      "    Keywords: john brennan;cia director;serving cia;mistakes john;looked mistakes;brennan serving;tuckercarlson speaking;speaking john\n",
      "    Document ID: 4\n",
      "        Title: D_Trump2018_8_18_13_12\n",
      "        Sentences: ['Has anyone looked at the mistakes that John Brennan made while serving as CIA Director?']\n",
      "        Author: D_Trump4\n",
      "        Source: None\n",
      "        Time: 2018-08-18 13:12:00\n",
      "    Document ID: 30\n",
      "        Title: D_Trump2018_8_17_0_45\n",
      "        Sentences: ['@TuckerCarlson speaking of John Brennan: “How did somebody so obviously limited intellectually get to be CIA Director in the first place?” Now that is a really good question!']\n",
      "        Author: D_Trump30\n",
      "        Source: None\n",
      "        Time: 2018-08-17 00:45:00\n",
      "---------------\n",
      "-------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print('------------------- Get topic summary -----------------------')\n",
    "dataset = 'trump_tweets' # str | Dataset name.\n",
    "#query = '(\"Trump\" OR \"president\")' # str | Fulltext query, using mysql MATCH boolean query format. Example, (\\\"word1\\\" OR \\\"word2\\\") AND (\\\"word3\\\" OR \\\"word4\\\") (optional)\n",
    "query = ''\n",
    "custom_stop_words = [\"real\",\"hillary\"] # str | List of stop words. (optional)\n",
    "num_topics = 8 # int | Number of topics to be extracted from the dataset. (optional) (default to 8)\n",
    "num_keywords = 8 # int | Number of keywords per topic that is extracted from the dataset. (optional) (default to 8)\n",
    "metadata_selection =\"\" # str | json object of {\\\"metadata_field\\\":[\\\"selected_values\\\"]} (optional)\n",
    "summary_length = 6 # int | The maximum number of bullet points a user wants to see in each topic summary. (optional) (default to 6)\n",
    "context_amount = 0 # int | The number of sentences surrounding key summary sentences in the documents that they come from. (optional) (default to 0)\n",
    "num_docs = 20 # int | The maximum number of key documents to use for summarization. (optional) (default to 20)\n",
    "excluded_docs = '' # str | List of document IDs that should be excluded from the analysis. Example, \\\"docid1, docid2, ..., docidN\\\"  (optional)\n",
    "\n",
    "try:\n",
    "    api_response = api_instance.get_topic_summary_api(\n",
    "        dataset, \n",
    "        query=query, \n",
    "        custom_stop_words=custom_stop_words, \n",
    "        num_topics=num_topics, \n",
    "        num_keywords=num_keywords,\n",
    "        metadata_selection=metadata_selection,\n",
    "        summary_length=summary_length, \n",
    "        context_amount=context_amount, \n",
    "        num_docs=num_docs)\n",
    "except ApiException as e:\n",
    "    print(\"Exception when calling TopicsApi->get_topic_summary_api: %s\\n\" % e)\n",
    "\n",
    "i = 1\n",
    "for res in api_response.result:\n",
    "    print('Topic', i, 'summary:')\n",
    "    print('    Keywords:', res.topic)\n",
    "    for j in range(len(res.summary)):\n",
    "        print('    Document ID:', res.summary[j].sourceid)\n",
    "        print('        Title:', res.summary[j].title)\n",
    "        print('        Sentences:', res.summary[j].sentences)\n",
    "        print('        Author:', res.summary[j].attribute.author)\n",
    "        print('        Source:', res.summary[j].attribute.source)\n",
    "        print('        Time:', datetime.datetime.fromtimestamp(float(res.summary[j].attribute.time)))\n",
    "\n",
    "        #print(type(res.summary[j].attribute))\n",
    "        \n",
    "    print('---------------')\n",
    "    i = i + 1\n",
    "    \n",
    "    \n",
    "#pprint(api_response)  # raw API response\n",
    "print('-------------------------------------------------------------')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get topic sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------- Get topic sentiment ------------------------\n",
      "Topic 1 sentiment:\n",
      "    Keywords: america great;great great;andrew cuomo;taxed andrew;highest taxed;great believe;governor highest;believe governor\n",
      "    Sentiment: 0.41852550172486996\n",
      "    Strength: 0.17477316796369655\n",
      "    Doucment IDs: 16 17 18 24 44 45\n",
      "    Doucment Sentiments: 0.2727272727272727 -0.2666666666666667 0.11764705882352941 0.45454545454545453 0.6 0.5454545454545454\n",
      "    Doucment Scores: [0.1384221937117849, 0.08045847239667162, 0.07557757475759962, 0.09427887714883189, 0.31291250999650605, 0.2983503719886059]\n",
      "---------------\n",
      "Topic 2 sentiment:\n",
      "    Keywords: illegal aliens;united permission;permission illegal;legally chuck;enter united;chuck schumer;aliens treated;aliens illegal\n",
      "    Sentiment: -0.46153846153846156\n",
      "    Strength: 0.12529311552435418\n",
      "    Doucment IDs: 57 63\n",
      "    Doucment Sentiments: -0.46153846153846156 -0.46153846153846156\n",
      "    Doucment Scores: [0.5, 0.5]\n",
      "---------------\n",
      "Topic 3 sentiment:\n",
      "    Keywords: bruce ohr;peter strzok;fired fbi;agent peter;wife nelly;ohr justice;justice department;fusion gps\n",
      "    Sentiment: -0.347947217991058\n",
      "    Strength: 0.16651861972651685\n",
      "    Doucment IDs: 11 12 13 31 33 68 70 91\n",
      "    Doucment Sentiments: -0.3333333333333333 -0.3333333333333333 -0.1 -0.3125 -0.25 -0.625 -0.3157894736842105 -0.4\n",
      "    Doucment Scores: [0.07570715190115822, 0.07570715190115822, 0.041466514860734466, 0.08741908905942716, 0.2618577180816574, 0.13461747240857874, 0.15294577152089295, 0.17027913026639271]\n",
      "---------------\n",
      "Topic 4 sentiment:\n",
      "    Keywords: witch hunt;donald trump;frame donald;hillary clinton;rigged witch;trump gregg;gregg jarrett;clinton frame\n",
      "    Sentiment: -0.40051525545005806\n",
      "    Strength: 0.14660981702170603\n",
      "    Doucment IDs: 16 46 53 67 69 90 91 96 99\n",
      "    Doucment Sentiments: 0.2727272727272727 -0.375 -0.8333333333333334 -0.8571428571428571 -0.3333333333333333 -0.2222222222222222 -0.16666666666666666 -0.2857142857142857 -0.4\n",
      "    Doucment Scores: [0.034333319262838405, 0.2392354980344115, 0.06685335995670713, 0.12355975826466116, 0.2805971208969716, 0.03795691260077215, 0.09454492834098563, 0.07303006833307407, 0.04988903430957834]\n",
      "---------------\n",
      "Topic 5 sentiment:\n",
      "    Keywords: fake news;news media;wacky omarosa;presidential lowlife;omarosa modern;modern form;lowlife omarosa;form communication\n",
      "    Sentiment: -0.19429249489176126\n",
      "    Strength: 0.100587731732617\n",
      "    Doucment IDs: 8 38 40 89 93 95\n",
      "    Doucment Sentiments: -0.5 -0.375 -0.5 -0.5 0.2857142857142857 -0.6666666666666666\n",
      "    Doucment Scores: [0.09415980058720225, 0.09415980058720225, 0.23762662508220356, 0.10049363411313801, 0.3915074309718749, 0.08205270865837908]\n",
      "---------------\n",
      "Topic 6 sentiment:\n",
      "    Keywords: trump campaign;lou dobbs;forward special;evidence collusion;dobbs forward;democrats evidence;conflicts angry;angry democrats\n",
      "    Sentiment: -0.2711896972210555\n",
      "    Strength: 0.11687186488497478\n",
      "    Doucment IDs: 27 43 66 73\n",
      "    Doucment Sentiments: -0.5 -0.3 -0.3333333333333333 -0.16666666666666666\n",
      "    Doucment Scores: [0.07524141859813199, 0.047586851430450085, 0.438585864985709, 0.438585864985709]\n",
      "---------------\n",
      "Topic 7 sentiment:\n",
      "    Keywords: strzok firing;judicial watch;watch strzok;tom fitton;mueller operation;fitton judicial;firing mueller;firing decisive\n",
      "    Sentiment: -0.5628179498964005\n",
      "    Strength: 0.08654967168663112\n",
      "    Doucment IDs: 28 52 74 76\n",
      "    Doucment Sentiments: -0.4 -0.375 -0.75 -0.5714285714285714\n",
      "    Doucment Scores: [0.1280441131104907, 0.08428761623610359, 0.16741924586562787, 0.620249024787778]\n",
      "---------------\n",
      "Topic 8 sentiment:\n",
      "    Keywords: john brennan;cia director;serving cia;mistakes john;looked mistakes;brennan serving;tuckercarlson speaking;speaking john\n",
      "    Sentiment: -0.1773119152040573\n",
      "    Strength: 0.08279601145950366\n",
      "    Doucment IDs: 4 30 47\n",
      "    Doucment Sentiments: -0.3333333333333333 0.1111111111111111 -0.14285714285714285\n",
      "    Doucment Scores: [0.5932385414019006, 0.3092632399354499, 0.09749821866264961]\n",
      "---------------\n",
      "-------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print('---------------- Get topic sentiment ------------------------')\n",
    "dataset = 'trump_tweets' # str | Dataset name.\n",
    "#query = '(\"Trump\" OR \"president\")' # str | Fulltext query, using mysql MATCH boolean query format. Example, (\\\"word1\\\" OR \\\"word2\\\") AND (\\\"word3\\\" OR \\\"word4\\\") (optional)\n",
    "query = ''\n",
    "custom_stop_words = [\"real\",\"hillary\"] # str | List of stop words. (optional)\n",
    "num_topics = 8 # int | Number of topics to be extracted from the dataset. (optional) (default to 8)\n",
    "num_keywords = 8 # int | Number of keywords per topic that is extracted from the dataset. (optional) (default to 8)\n",
    "excluded_docs = '' # str | List of document IDs that should be excluded from the analysis. Example, \\\"docid1, docid2, ..., docidN\\\"  (optional)\n",
    "\n",
    "try:\n",
    "    api_response = api_instance.get_topic_sentiment_api(\n",
    "        dataset, \n",
    "        query=query, \n",
    "        custom_stop_words=custom_stop_words, \n",
    "        num_topics=num_topics, \n",
    "        num_keywords=num_keywords)\n",
    "    \n",
    "except ApiException as e:\n",
    "    print(\"Exception when calling TopicsApi->get_topic_sentiment_api: %s\\n\" % e)\n",
    "\n",
    "i = 1\n",
    "for res in api_response.result:\n",
    "    print('Topic', i, 'sentiment:')\n",
    "    print('    Keywords:', res.topic)\n",
    "    print('    Sentiment:', res.sentiment)\n",
    "    print('    Strength:', res.strength)\n",
    "    \n",
    "    doc_id_str = ' '.join(str(x) for x in res.doc_id)\n",
    "    doc_sentiment_str = ' '.join(str(x) for x in res.doc_sentiment)\n",
    "    doc_score_str = ' '.join(str(x) for x in res.doc_score)\n",
    "    print('    Doucment IDs:', doc_id_str)\n",
    "    print('    Doucment Sentiments:', doc_sentiment_str)\n",
    "    print('    Doucment Scores:', doc_score_str)\n",
    "    \n",
    "    print('---------------')\n",
    "    i = i + 1\n",
    "    \n",
    "#pprint(api_response)\n",
    "print('-------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get topic consensus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------- Get topic consensus ------------------------\n",
      "Topic 1 consensus:\n",
      "    Keywords: america great;great great;andrew cuomo;taxed andrew;highest taxed;great believe;governor highest;believe governor\n",
      "    Consensus: 0.9195415276033284\n",
      "    Strength: 0.17477316796369655\n",
      "---------------\n",
      "Topic 2 consensus:\n",
      "    Keywords: illegal aliens;united permission;permission illegal;legally chuck;enter united;chuck schumer;aliens treated;aliens illegal\n",
      "    Consensus: 1.0\n",
      "    Strength: 0.12529311552435418\n",
      "---------------\n",
      "Topic 3 consensus:\n",
      "    Keywords: bruce ohr;peter strzok;fired fbi;agent peter;wife nelly;ohr justice;justice department;fusion gps\n",
      "    Consensus: 0.9999999999999999\n",
      "    Strength: 0.16651861972651685\n",
      "---------------\n",
      "Topic 4 consensus:\n",
      "    Keywords: witch hunt;donald trump;frame donald;hillary clinton;rigged witch;trump gregg;gregg jarrett;clinton frame\n",
      "    Consensus: 0.9656666807371614\n",
      "    Strength: 0.14660981702170603\n",
      "---------------\n",
      "Topic 5 consensus:\n",
      "    Keywords: fake news;news media;wacky omarosa;presidential lowlife;omarosa modern;modern form;lowlife omarosa;form communication\n",
      "    Consensus: 0.6084925690281252\n",
      "    Strength: 0.100587731732617\n",
      "---------------\n",
      "Topic 6 consensus:\n",
      "    Keywords: trump campaign;lou dobbs;forward special;evidence collusion;dobbs forward;democrats evidence;conflicts angry;angry democrats\n",
      "    Consensus: 1.0\n",
      "    Strength: 0.11687186488497478\n",
      "---------------\n",
      "Topic 7 consensus:\n",
      "    Keywords: strzok firing;judicial watch;watch strzok;tom fitton;mueller operation;fitton judicial;firing mueller;firing decisive\n",
      "    Consensus: 1.0\n",
      "    Strength: 0.08654967168663112\n",
      "---------------\n",
      "Topic 8 consensus:\n",
      "    Keywords: john brennan;cia director;serving cia;mistakes john;looked mistakes;brennan serving;tuckercarlson speaking;speaking john\n",
      "    Consensus: 0.6907367600645502\n",
      "    Strength: 0.08279601145950366\n",
      "---------------\n",
      "-------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print('---------------- Get topic consensus ------------------------')\n",
    "dataset = 'trump_tweets' # str | Dataset name.\n",
    "query = '' # str | Fulltext query, using mysql MATCH boolean query format. Example, (\\\"word1\\\" OR \\\"word2\\\") AND (\\\"word3\\\" OR \\\"word4\\\") (optional)\n",
    "custom_stop_words = [\"real\",\"hillary\"] # str | List of stop words. (optional)\n",
    "num_topics = 8 # int | Number of topics to be extracted from the dataset. (optional) (default to 8)\n",
    "num_keywords = 8 # int | Number of keywords per topic that is extracted from the dataset. (optional) (default to 8)\n",
    "excluded_docs = [''] # str | List of document IDs that should be excluded from the analysis. Example, \\\"docid1, docid2, ..., docidN\\\"  (optional)\n",
    "\n",
    "try:\n",
    "    api_response = api_instance.get_topic_consensus_api(\n",
    "        dataset, \n",
    "        query=query, \n",
    "        custom_stop_words=custom_stop_words, \n",
    "        num_topics=num_topics, \n",
    "        num_keywords=num_keywords)\n",
    "except ApiException as e:\n",
    "    print(\"Exception when calling TopicsApi->get_topic_consensus_api: %s\\n\" % e)\n",
    "    \n",
    "i = 1\n",
    "for res in api_response.result:\n",
    "    print('Topic', i, 'consensus:')\n",
    "    print('    Keywords:', res.topic)\n",
    "    print('    Consensus:', res.consensus)\n",
    "    print('    Strength:', res.strength)\n",
    "    \n",
    "    print('---------------')\n",
    "    i = i + 1\n",
    "    \n",
    "#pprint(api_response) # raw API response\n",
    "print('-------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get topic historical analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------ Get topic historical analysis ----------------\n",
      "Error: {\"stack\": \"Traceback (most recent call last):\\n  File \\\"/Users/jimw/github/sumup/nucleus-dev/nucleus/rest_api/job_manager.py\\\", line 73, in subprocess_entrypoint\\n    res = function(**expected_args)\\n  File \\\"/Users/jimw/github/sumup/nucleus-dev/nucleus/rest_api/apis/topics.py\\\", line 462, in get_historical_topics\\n    results_hist = topic_hist.fit(dataset, model, topics)\\n  File \\\"/Users/jimw/github/sumup/nucleus-dev/nucleus/analytics/topic_historical_metrics.py\\\", line 252, in fit\\n    self.custom_dict)\\n  File \\\"/Users/jimw/github/sumup/nucleus-dev/nucleus/analytics/topic_historical_metrics.py\\\", line 103, in topic_metrics_historical\\n    for subtr in iterate_time(tr, inc_code, overlap=overlap, inc_step=inc_step):\\nTypeError: iterate_time() got an unexpected keyword argument 'inc_step'\\n\"}\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'to_str'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-7fdeb85f9e50>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mmetadata_selection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetadata_selection\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0minc_step\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minc_step\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         excluded_docs=excluded_docs)\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mApiException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/github/sumup/nucleus-dev/nucleus/rest_api/swagger/python/python-api/nucleus_api/api/nucleus_api.py\u001b[0m in \u001b[0;36mget_topic_historical_analysis_api\u001b[0;34m(self, dataset, time_period, update_period, **kwargs)\u001b[0m\n\u001b[1;32m   1913\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_topic_historical_analysis_api_with_http_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime_period\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate_period\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# noqa: E501\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1914\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1915\u001b[0;31m             \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_topic_historical_analysis_api_with_http_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime_period\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate_period\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# noqa: E501\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1916\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1917\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/github/sumup/nucleus-dev/nucleus/rest_api/swagger/python/python-api/nucleus_api/api/nucleus_api.py\u001b[0m in \u001b[0;36mget_topic_historical_analysis_api_with_http_info\u001b[0;34m(self, dataset, time_period, update_period, **kwargs)\u001b[0m\n\u001b[1;32m   2062\u001b[0m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2063\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2064\u001b[0;31m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2065\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2066\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'to_str'"
     ]
    }
   ],
   "source": [
    "print('------------ Get topic historical analysis ----------------')\n",
    "\n",
    "dataset = 'trump_tweets'   # str | Dataset name.\n",
    "time_period = '6M'  # str | Time period selection (default to 1M)\n",
    "update_period = 'd' # str | Frequency at which the historical anlaysis is performed (default to d)\n",
    "query = '' # str | Fulltext query, using mysql MATCH boolean query format. Example, (\\\"word1\\\" OR \\\"word2\\\") AND (\\\"word3\\\" OR \\\"word4\\\") (optional)\n",
    "custom_stop_words = [\"real\",\"hillary\"] # str | List of stop words (optional)\n",
    "num_topics = 8 # int | Number of topics to be extracted from the dataset. (optional) (default to 8)\n",
    "num_keywords = 8 # int | Number of keywords per topic that is extracted from the dataset. (optional) (default to 8)\n",
    "metadata_selection = '' # str | json object of {\\\"metadata_field\\\":[\\\"selected_values\\\"]} (optional)\n",
    "inc_step = 1 # int | Number of increments of the udpate period in between two historical computations. (optional) (default to 1)\n",
    "excluded_docs = [''] # str | List of document IDs that should be excluded from the analysis. Example, \\\"docid1, docid2, ..., docidN\\\"  (optional)\n",
    "\n",
    "try:\n",
    "    api_response = api_instance.get_topic_historical_analysis_api(\n",
    "        dataset, \n",
    "        time_period, \n",
    "        update_period, \n",
    "        query=query, \n",
    "        custom_stop_words=custom_stop_words, \n",
    "        num_topics=num_topics, \n",
    "        num_keywords=num_keywords, \n",
    "        metadata_selection=metadata_selection, \n",
    "        inc_step=inc_step, \n",
    "        excluded_docs=excluded_docs)\n",
    "    \n",
    "except ApiException as e:\n",
    "    print(\"Exception when calling TopicsApi->get_topic_historical_analysis_api: %s\\n\" % e)\n",
    "\n",
    "print('api_response=', api_response)\n",
    "results = api_response.result\n",
    "\n",
    "# chart the historical metrics when running in Jupyter Notebook\n",
    "if running_notebook:\n",
    "    print('Plotting historical metrics data...')\n",
    "    historical_metrics = []\n",
    "    for res in results:\n",
    "        # conctruct a list of historical metrics dictionaries for charting\n",
    "        historical_metrics.append({\n",
    "            'topic'    : res.topic,\n",
    "            'time_stamps' : np.array(res.time_stamps),\n",
    "            'strength' : np.array(res.strength, dtype=np.float32),\n",
    "            'consensus': np.array(res.consensus, dtype=np.float32), \n",
    "            'sentiment': np.array(res.sentiment, dtype=np.float32)})\n",
    "\n",
    "    selected_topics = range(len(historical_metrics)) \n",
    "    nucleus_api.api.nucleus_api.topic_charts_historical(historical_metrics, selected_topics, True)\n",
    "else:\n",
    "    print('Printing historical metrics data...')\n",
    "    print('NOTE: historical metrics data can be plotted when running the example in Jupyter Notebook')\n",
    "    i = 1\n",
    "    for res in results:\n",
    "        print('Topic', i, res.topic)\n",
    "        print('    Timestamps:', res.time_stamps)\n",
    "        print('    Strength:', res.strength)\n",
    "        print('    Consensus:', res.consensus)\n",
    "        print('    Sentiment:', res.sentiment)\n",
    "        print('----------------')\n",
    "        i = i + 1\n",
    "#pprint(api_response)\n",
    "print('-------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get author connectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------- Get author connectivity -------------------\n",
      "Error: {\"stack\": \"Traceback (most recent call last):\\n  File \\\"/Users/jimw/github/sumup/nucleus-dev/nucleus/rest_api/job_manager.py\\\", line 73, in subprocess_entrypoint\\n    res = function(**expected_args)\\n  File \\\"/Users/jimw/github/sumup/nucleus-dev/nucleus/rest_api/apis/topics.py\\\", line 603, in get_author_conn\\n    results_author = author_connect.fit(dataset,query,values, fulltext_query=fulltext_query)\\nUnboundLocalError: local variable 'fulltext_query' referenced before assignment\\n\"}\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'to_str'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-5658f169955b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mtime_period\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtime_period\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mmetadata_selection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetadata_selection\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         excluded_docs=excluded_docs)\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mApiException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/github/sumup/nucleus-dev/nucleus/rest_api/swagger/python/python-api/nucleus_api/api/nucleus_api.py\u001b[0m in \u001b[0;36mget_author_connectivity_api\u001b[0;34m(self, dataset, target_author, time_period, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_author_connectivity_api_with_http_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_author\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime_period\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# noqa: E501\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m             \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_author_connectivity_api_with_http_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_author\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime_period\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# noqa: E501\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/github/sumup/nucleus-dev/nucleus/rest_api/swagger/python/python-api/nucleus_api/api/nucleus_api.py\u001b[0m in \u001b[0;36mget_author_connectivity_api_with_http_info\u001b[0;34m(self, dataset, target_author, time_period, **kwargs)\u001b[0m\n\u001b[1;32m    261\u001b[0m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'to_str'"
     ]
    }
   ],
   "source": [
    "print('----------------- Get author connectivity -------------------')\n",
    "dataset = dataset # str | Dataset name.\n",
    "target_author = 'D_Trump16' # str | Name of the author to be analyzed.\n",
    "query = '' # str | Fulltext query, using mysql MATCH boolean query format. Subject covered by the author, on which to focus the analysis of connectivity. Example, (\\\"word1\\\" OR \\\"word2\\\") AND (\\\"word3\\\" OR \\\"word4\\\") (optional)\n",
    "custom_stop_words = [\"real\",\"hillary\"] # str | List of words possibly used by the target author that are considered not information-bearing. (optional)\n",
    "time_period = '12M' # str | Time period selection. Required. Valid values: \"1M\",\"3M\",\"6M\",\"12M\",\"3Y\",\"5Y\"\n",
    "metadata_selection = '' # str | json object of {\\\"metadata_field\\\":[\\\"selected_values\\\"]} (optional)\n",
    "excluded_docs = [''] # str | List of document IDs that should be excluded from the analysis. Example, \\\"docid1, docid2, ..., docidN\\\"  (optional)\n",
    "\n",
    "try:\n",
    "    api_response = api_instance.get_author_connectivity_api(\n",
    "        dataset, \n",
    "        target_author, \n",
    "        query=query, \n",
    "        custom_stop_words=custom_stop_words, \n",
    "        time_period=time_period, \n",
    "        metadata_selection=metadata_selection, \n",
    "        excluded_docs=excluded_docs)\n",
    "    \n",
    "except ApiException as e:\n",
    "    print(\"Exception when calling TopicsApi->get_author_connectivity_api: %s\\n\" % e)\n",
    "\n",
    "res = api_response.results\n",
    "print('Mainstream connections:')\n",
    "for mc in res.mainstream_connection:\n",
    "    print('    Topic:', mc.topic)\n",
    "    print('    Authors:', \" \".join(str(x) for x in mc.authors))\n",
    "    \n",
    "print('Niche connections:')\n",
    "for nc in res.niche_connection:\n",
    "    print('    Topic:', nc.topic)\n",
    "    print('    Authors:', \" \".join(str(x) for x in nc.authors))  \n",
    "    \n",
    "#pprint(api_response)   # raw API response\n",
    "print('-------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get topic delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------- Get topic deltas -----------------------\n",
      "Topic 1 changes in exposure:\n",
      "    Keywords: witch hunt;donald trump;hillary clinton;trump gregg;jarrett foxandfriends;gregg jarrett;frame donald;clinton frame\n",
      "    Document ID: ['62', '63', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '80', '81', '82', '83', '84', '86', '87', '88', '89', '90', '91', '92', '93', '94', '95', '96', '97', '98', '99'] ['2', '3', '4', '6', '7', '8', '9', '10', '11', '12', '13', '15', '16', '17', '18', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47']\n",
      "    Per Source Change in Exposure: ['-0.0', '-0.0', '-0.0', '-0.0', '-0.09359794860000106', '-0.0', '-0.602637378696451', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.07224232323152462', '-0.08015249457686387', '-0.0', '-0.0', '-0.0', '-0.0', '-0.08015249457686387', '-0.0', '-0.0', '-0.07121736031829556']\n",
      "---------------\n",
      "Topic 2 changes in exposure:\n",
      "    Keywords: lou dobbs;forward special;evidence collusion;dobbs forward;democrats evidence;conflicts angry;collusion trump;angry democrats\n",
      "    Document ID: ['62', '63', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '80', '81', '82', '83', '84', '86', '87', '88', '89', '90', '91', '92', '93', '94', '95', '96', '97', '98', '99'] ['2', '3', '4', '6', '7', '8', '9', '10', '11', '12', '13', '15', '16', '17', '18', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47']\n",
      "    Per Source Change in Exposure: ['-0.0', '-0.0', '-0.0', '-0.46410161513775455', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.5358983848622454', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0']\n",
      "---------------\n",
      "Topic 3 changes in exposure:\n",
      "    Keywords: peter strzok;fired fbi;agent peter;strzok fraud;rigged investigation;investigation started;fraud rigged;fbi agent\n",
      "    Document ID: ['62', '63', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '80', '81', '82', '83', '84', '86', '87', '88', '89', '90', '91', '92', '93', '94', '95', '96', '97', '98', '99'] ['2', '3', '4', '6', '7', '8', '9', '10', '11', '12', '13', '15', '16', '17', '18', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47']\n",
      "    Per Source Change in Exposure: ['-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.6548022111098636', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.34519778889013636', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0']\n",
      "---------------\n",
      "Topic 4 changes in exposure:\n",
      "    Keywords: crooked hillary;strzok fbi;sham investigation;fired agent;fbi charge;clinton sham;charge crooked;agent strzok\n",
      "    Document ID: ['62', '63', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '80', '81', '82', '83', '84', '86', '87', '88', '89', '90', '91', '92', '93', '94', '95', '96', '97', '98', '99'] ['2', '3', '4', '6', '7', '8', '9', '10', '11', '12', '13', '15', '16', '17', '18', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47']\n",
      "    Per Source Change in Exposure: ['-0.0', '-0.0', '-0.0', '-0.0', '-0.11425402807090787', '-0.08693428014876822', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.14657108505601155', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.6522406067243123', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0']\n",
      "---------------\n",
      "Topic 5 changes in exposure:\n",
      "    Keywords: illegal aliens;united permission;treated enters;permission illegal;enters legally;enter united;aliens treated;aliens illegal\n",
      "    Document ID: ['62', '63', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '80', '81', '82', '83', '84', '86', '87', '88', '89', '90', '91', '92', '93', '94', '95', '96', '97', '98', '99'] ['2', '3', '4', '6', '7', '8', '9', '10', '11', '12', '13', '15', '16', '17', '18', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47']\n",
      "    Per Source Change in Exposure: ['-0.0', '-1.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0']\n",
      "---------------\n",
      "Topic 6 changes in exposure:\n",
      "    Keywords: heard heard;heard trump;claims heard;trump campaign;campaign russia;special councel;councel conflicts;counsel conflicts\n",
      "    Document ID: ['62', '63', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '80', '81', '82', '83', '84', '86', '87', '88', '89', '90', '91', '92', '93', '94', '95', '96', '97', '98', '99'] ['2', '3', '4', '6', '7', '8', '9', '10', '11', '12', '13', '15', '16', '17', '18', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47']\n",
      "    Per Source Change in Exposure: ['-0.0', '-0.0', '-0.0', '-0.008782286305066394', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.013671939226989588', '-0.0', '-0.0', '-0.002445861550650327', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.9750999129172937', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0']\n",
      "---------------\n",
      "Topic 7 changes in exposure:\n",
      "    Keywords: judicial watch;strzok firing;watch strzok;tom fitton;mueller operation;fitton judicial;firing mueller;honest prosecutors\n",
      "    Document ID: ['62', '63', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '80', '81', '82', '83', '84', '86', '87', '88', '89', '90', '91', '92', '93', '94', '95', '96', '97', '98', '99'] ['2', '3', '4', '6', '7', '8', '9', '10', '11', '12', '13', '15', '16', '17', '18', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47']\n",
      "    Per Source Change in Exposure: ['-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.29086924238544243', '-0.0', '-0.7091307576145576', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0']\n",
      "---------------\n",
      "Topic 8 changes in exposure:\n",
      "    Keywords: white house;omarosa credibility;media interviews;interviews white;credibility media;job white;house guess;break job\n",
      "    Document ID: ['62', '63', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '80', '81', '82', '83', '84', '86', '87', '88', '89', '90', '91', '92', '93', '94', '95', '96', '97', '98', '99'] ['2', '3', '4', '6', '7', '8', '9', '10', '11', '12', '13', '15', '16', '17', '18', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47']\n",
      "    Per Source Change in Exposure: ['-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.4358504119580356', '-0.0', '-0.0', '-0.0', '-0.0', '-0.5641495880419644', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0']\n",
      "---------------\n",
      "-------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print('------------------- Get topic deltas -----------------------')\n",
    "dataset = 'trump_tweets' \n",
    "#dataset = dataset # str | Dataset name.\n",
    "#query = '(\"Trump\" OR \"president\")' # str | Fulltext query, using mysql MATCH boolean query format. Example, (\\\"word1\\\" OR \\\"word2\\\") AND (\\\"word3\\\" OR \\\"word4\\\") (optional)\n",
    "query = ''\n",
    "custom_stop_words = [\"\"] # str | List of stop words. (optional)\n",
    "num_topics = 8 # int | Number of topics to be extracted from the dataset. (optional) (default to 8)\n",
    "num_keywords = 8 # int | Number of keywords per topic that is extracted from the dataset. (optional) (default to 8)\n",
    "metadata_selection =\"\" # str | json object of {\\\"metadata_field\\\":[\\\"selected_values\\\"]} (optional)\n",
    "time_start_t0 = '2018-08-12 00:00:00'\n",
    "time_end_t0 = '2018-08-15 13:00:00'\n",
    "time_start_t1 = '2018-08-16 00:00:00'\n",
    "time_end_t1 = '2018-08-19 00:00:00'\n",
    "excluded_docs = '' # str | List of document IDs that should be excluded from the analysis. Example, \\\"docid1, docid2, ..., docidN\\\"  (optional)\n",
    "\n",
    "try:\n",
    "    api_response = api_instance.get_topic_delta_api(\n",
    "        dataset, \n",
    "        query=query, \n",
    "        custom_stop_words=custom_stop_words, \n",
    "        num_topics=num_topics, \n",
    "        num_keywords=num_keywords,\n",
    "        time_start_t0 = time_start_t0,\n",
    "        time_end_t0 = time_end_t0,\n",
    "        time_start_t1 = time_start_t1,\n",
    "        time_end_t1 = time_end_t1,\n",
    "        metadata_selection=metadata_selection)\n",
    "except ApiException as e:\n",
    "    print(\"Exception when calling TopicsApi->get_topic_delta_api: %s\\n\" % e)\n",
    "\n",
    "i = 1\n",
    "for res in api_response.result:\n",
    "    print('Topic', i, 'changes in exposure:')\n",
    "    print('    Keywords:', res.topic)\n",
    "    print('    Document ID:', res.doc_id_t0, res.doc_id_t1)\n",
    "    print('    Per Source Change in Exposure:', res.doc_topic_exposure_delta)\n",
    "    print('---------------')\n",
    "    i = i + 1\n",
    "    \n",
    "    \n",
    "#pprint(api_response)  # raw API response\n",
    "print('-------------------------------------------------------------')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get document information without content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document ID: 10\n",
      "    Title: D_Trump2018_8_18_1_47\n",
      "    Author: D_Trump10\n",
      "    Source: None\n",
      "    Time: 2018-08-18 01:47:00\n",
      "---------------\n",
      "Document ID: 11\n",
      "    Title: D_Trump2018_8_18_1_46\n",
      "    Author: D_Trump11\n",
      "    Source: None\n",
      "    Time: 2018-08-18 01:46:00\n",
      "---------------\n",
      "Document ID: 12\n",
      "    Title: D_Trump2018_8_18_1_37\n",
      "    Author: D_Trump12\n",
      "    Source: None\n",
      "    Time: 2018-08-18 01:37:00\n",
      "---------------\n",
      "Document ID: 20\n",
      "    Title: D_Trump2018_8_17_12_38\n",
      "    Author: D_Trump20\n",
      "    Source: None\n",
      "    Time: 2018-08-17 12:38:00\n",
      "---------------\n",
      "-------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "dataset = 'trump_tweets' # str | Dataset name.\n",
    "doc_titles = ['D_Trump2018_8_18_1_47']   # str | The title of the document to retrieve. Example: \\\" \\\"title 1\\\" \\\"  (optional)\n",
    "doc_ids = ['11', '12', '20']      # int | The docid of the document to retrieve. Example: \\\"docid1\\\"  (optional)\n",
    "\n",
    "try:\n",
    "    api_response = api_instance.get_doc_info(dataset, doc_titles=doc_titles, doc_ids=doc_ids)\n",
    "    \n",
    "except ApiException as e:\n",
    "    print(\"Exception when calling DocumentsApi->get_doc_info: %s\\n\" % e)\n",
    "    \n",
    "for res in api_response.result:\n",
    "    print('Document ID:', res.sourceid)\n",
    "    print('    Title:', res.title)\n",
    "    print('    Author:', res.attribute.author)\n",
    "    print('    Source:', res.attribute.source)\n",
    "    print('    Time:', datetime.datetime.fromtimestamp(float(res.attribute.time)))\n",
    "\n",
    "    print('---------------')\n",
    "    \n",
    "    \n",
    "#pprint(api_response)  # raw response from API server\n",
    "print('-------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display document details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------\n",
      "Document ID: 1\n",
      "    Title: D_Trump2018_8_18_19_39\n",
      "    Author: D_Trump1\n",
      "    Source: None\n",
      "    Time: 2018-08-18 19:39:00\n",
      "    Content \n",
      "---------------\n",
      "-------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print('-------------------------------------------------------------')\n",
    "\n",
    "dataset = 'trump_tweets' # str | Dataset name.\n",
    "#doc_titles = ['D_Trump2018_8_18_1_47']   # str | The title of the document to retrieve. Example: \\\" \\\"title 1\\\" \\\"  (optional)\n",
    "doc_ids = ['1']      # int | The docid of the document to retrieve. Example: \\\"docid1\\\"  (optional)\n",
    "\n",
    "try:\n",
    "    api_response = api_instance.get_doc_display(dataset, \n",
    "                                                    #doc_titles=doc_titles, \n",
    "                                                    doc_ids=doc_ids)\n",
    "    \n",
    "except ApiException as e:\n",
    "    print(\"Exception when calling DocumentsApi->get_doc_display_api: %s\\n\" % e)\n",
    "\n",
    "for res in api_response.result:\n",
    "    print('Document ID:', res.sourceid)\n",
    "    print('    Title:', res.title)\n",
    "    print('    Author:', res.attribute.author)\n",
    "    print('    Source:', res.attribute.source)\n",
    "    print('    Time:', datetime.datetime.fromtimestamp(float(res.attribute.time)))\n",
    "    print('    Content', res.content)\n",
    "\n",
    "    print('---------------')\n",
    "\n",
    "\n",
    "#pprint(api_response) # raw response from API server\n",
    "print('-------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get document recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------- Get document recommendations -----------------\n",
      "Document recommendations for topic 1 :\n",
      "    Keywords: america great;great great;andrew cuomo;taxed andrew;highest taxed;great believe;governor highest;believe governor\n",
      "    Recommendation 1 :\n",
      "        Document ID: 44\n",
      "        Title: D_Trump2018_8_16_2_2\n",
      "        Attribute: {'author': 'D_Trump44', 'source': None, 'time': '1534410120.0'}\n",
      "        Author: D_Trump44\n",
      "        Source: None\n",
      "        Time: 2018-08-16 02:02:00\n",
      "    Recommendation 2 :\n",
      "        Document ID: 45\n",
      "        Title: D_Trump2018_8_16_1_53\n",
      "        Attribute: {'author': 'D_Trump45', 'source': None, 'time': '1534409580.0'}\n",
      "        Author: D_Trump45\n",
      "        Source: None\n",
      "        Time: 2018-08-16 01:53:00\n",
      "    Recommendation 3 :\n",
      "        Document ID: 16\n",
      "        Title: D_Trump2018_8_17_19_25\n",
      "        Attribute: {'author': 'D_Trump16', 'source': None, 'time': '1534559100.0'}\n",
      "        Author: D_Trump16\n",
      "        Source: None\n",
      "        Time: 2018-08-17 19:25:00\n",
      "    Recommendation 4 :\n",
      "        Document ID: 24\n",
      "        Title: D_Trump2018_8_17_11_44\n",
      "        Attribute: {'author': 'D_Trump24', 'source': None, 'time': '1534531440.0'}\n",
      "        Author: D_Trump24\n",
      "        Source: None\n",
      "        Time: 2018-08-17 11:44:00\n",
      "    Recommendation 5 :\n",
      "        Document ID: 17\n",
      "        Title: D_Trump2018_8_17_14_17\n",
      "        Attribute: {'author': 'D_Trump17', 'source': None, 'time': '1534540620.0'}\n",
      "        Author: D_Trump17\n",
      "        Source: None\n",
      "        Time: 2018-08-17 14:17:00\n",
      "    Recommendation 6 :\n",
      "        Document ID: 18\n",
      "        Title: D_Trump2018_8_17_14_10\n",
      "        Attribute: {'author': 'D_Trump18', 'source': None, 'time': '1534540200.0'}\n",
      "        Author: D_Trump18\n",
      "        Source: None\n",
      "        Time: 2018-08-17 14:10:00\n",
      "---------------\n",
      "Document recommendations for topic 2 :\n",
      "    Keywords: illegal aliens;united permission;permission illegal;legally chuck;enter united;chuck schumer;aliens treated;aliens illegal\n",
      "    Recommendation 1 :\n",
      "        Document ID: 63\n",
      "        Title: D_Trump2018_8_15_12_44\n",
      "        Attribute: {'author': 'D_Trump63', 'source': None, 'time': '1534362240.0'}\n",
      "        Author: D_Trump63\n",
      "        Source: None\n",
      "        Time: 2018-08-15 12:44:00\n",
      "    Recommendation 2 :\n",
      "        Document ID: 57\n",
      "        Title: D_Trump2018_8_15_13_18\n",
      "        Attribute: {'author': 'D_Trump57', 'source': None, 'time': '1534364280.0'}\n",
      "        Author: D_Trump57\n",
      "        Source: None\n",
      "        Time: 2018-08-15 13:18:00\n",
      "---------------\n",
      "Document recommendations for topic 3 :\n",
      "    Keywords: bruce ohr;peter strzok;fired fbi;agent peter;wife nelly;ohr justice;justice department;fusion gps\n",
      "    Recommendation 1 :\n",
      "        Document ID: 33\n",
      "        Title: D_Trump2018_8_16_23_37\n",
      "        Attribute: {'author': 'D_Trump33', 'source': None, 'time': '1534487820.0'}\n",
      "        Author: D_Trump33\n",
      "        Source: None\n",
      "        Time: 2018-08-16 23:37:00\n",
      "    Recommendation 2 :\n",
      "        Document ID: 70\n",
      "        Title: D_Trump2018_8_14_11_55\n",
      "        Attribute: {'author': 'D_Trump70', 'source': None, 'time': '1534272900.0'}\n",
      "        Author: D_Trump70\n",
      "        Source: None\n",
      "        Time: 2018-08-14 11:55:00\n",
      "    Recommendation 3 :\n",
      "        Document ID: 91\n",
      "        Title: D_Trump2018_8_13_16_4\n",
      "        Attribute: {'author': 'D_Trump91', 'source': None, 'time': '1534201440.0'}\n",
      "        Author: D_Trump91\n",
      "        Source: None\n",
      "        Time: 2018-08-13 16:04:00\n",
      "    Recommendation 4 :\n",
      "        Document ID: 68\n",
      "        Title: D_Trump2018_8_14_13_1\n",
      "        Attribute: {'author': 'D_Trump68', 'source': None, 'time': '1534276860.0'}\n",
      "        Author: D_Trump68\n",
      "        Source: None\n",
      "        Time: 2018-08-14 13:01:00\n",
      "---------------\n",
      "Document recommendations for topic 4 :\n",
      "    Keywords: witch hunt;donald trump;frame donald;hillary clinton;rigged witch;trump gregg;gregg jarrett;clinton frame\n",
      "    Recommendation 1 :\n",
      "        Document ID: 69\n",
      "        Title: D_Trump2018_8_14_12_6\n",
      "        Attribute: {'author': 'D_Trump69', 'source': None, 'time': '1534273560.0'}\n",
      "        Author: D_Trump69\n",
      "        Source: None\n",
      "        Time: 2018-08-14 12:06:00\n",
      "    Recommendation 2 :\n",
      "        Document ID: 46\n",
      "        Title: D_Trump2018_8_16_1_14\n",
      "        Attribute: {'author': 'D_Trump46', 'source': None, 'time': '1534407240.0'}\n",
      "        Author: D_Trump46\n",
      "        Source: None\n",
      "        Time: 2018-08-16 01:14:00\n",
      "    Recommendation 3 :\n",
      "        Document ID: 67\n",
      "        Title: D_Trump2018_8_14_13_10\n",
      "        Attribute: {'author': 'D_Trump67', 'source': None, 'time': '1534277400.0'}\n",
      "        Author: D_Trump67\n",
      "        Source: None\n",
      "        Time: 2018-08-14 13:10:00\n",
      "---------------\n",
      "Document recommendations for topic 5 :\n",
      "    Keywords: fake news;news media;wacky omarosa;presidential lowlife;omarosa modern;modern form;lowlife omarosa;form communication\n",
      "    Recommendation 1 :\n",
      "        Document ID: 93\n",
      "        Title: D_Trump2018_8_13_14_21\n",
      "        Attribute: {'author': 'D_Trump93', 'source': None, 'time': '1534195260.0'}\n",
      "        Author: D_Trump93\n",
      "        Source: None\n",
      "        Time: 2018-08-13 14:21:00\n",
      "    Recommendation 2 :\n",
      "        Document ID: 40\n",
      "        Title: D_Trump2018_8_16_12_50\n",
      "        Attribute: {'author': 'D_Trump40', 'source': None, 'time': '1534449000.0'}\n",
      "        Author: D_Trump40\n",
      "        Source: None\n",
      "        Time: 2018-08-16 12:50:00\n",
      "---------------\n",
      "Document recommendations for topic 6 :\n",
      "    Keywords: trump campaign;lou dobbs;forward special;evidence collusion;dobbs forward;democrats evidence;conflicts angry;angry democrats\n",
      "    Recommendation 1 :\n",
      "        Document ID: 73\n",
      "        Title: D_Trump2018_8_14_11_21\n",
      "        Attribute: {'author': 'D_Trump73', 'source': None, 'time': '1534270860.0'}\n",
      "        Author: D_Trump73\n",
      "        Source: None\n",
      "        Time: 2018-08-14 11:21:00\n",
      "    Recommendation 2 :\n",
      "        Document ID: 66\n",
      "        Title: D_Trump2018_8_14_13_15\n",
      "        Attribute: {'author': 'D_Trump66', 'source': None, 'time': '1534277700.0'}\n",
      "        Author: D_Trump66\n",
      "        Source: None\n",
      "        Time: 2018-08-14 13:15:00\n",
      "---------------\n",
      "Document recommendations for topic 7 :\n",
      "    Keywords: strzok firing;judicial watch;watch strzok;tom fitton;mueller operation;fitton judicial;firing mueller;firing decisive\n",
      "    Recommendation 1 :\n",
      "        Document ID: 76\n",
      "        Title: D_Trump2018_8_14_10_59\n",
      "        Attribute: {'author': 'D_Trump76', 'source': None, 'time': '1534269540.0'}\n",
      "        Author: D_Trump76\n",
      "        Source: None\n",
      "        Time: 2018-08-14 10:59:00\n",
      "    Recommendation 2 :\n",
      "        Document ID: 74\n",
      "        Title: D_Trump2018_8_14_11_13\n",
      "        Attribute: {'author': 'D_Trump74', 'source': None, 'time': '1534270380.0'}\n",
      "        Author: D_Trump74\n",
      "        Source: None\n",
      "        Time: 2018-08-14 11:13:00\n",
      "    Recommendation 3 :\n",
      "        Document ID: 52\n",
      "        Title: D_Trump2018_8_15_14_15\n",
      "        Attribute: {'author': 'D_Trump52', 'source': None, 'time': '1534367700.0'}\n",
      "        Author: D_Trump52\n",
      "        Source: None\n",
      "        Time: 2018-08-15 14:15:00\n",
      "---------------\n",
      "Document recommendations for topic 8 :\n",
      "    Keywords: john brennan;cia director;serving cia;mistakes john;looked mistakes;brennan serving;tuckercarlson speaking;speaking john\n",
      "    Recommendation 1 :\n",
      "        Document ID: 4\n",
      "        Title: D_Trump2018_8_18_13_12\n",
      "        Attribute: {'author': 'D_Trump4', 'source': None, 'time': '1534623120.0'}\n",
      "        Author: D_Trump4\n",
      "        Source: None\n",
      "        Time: 2018-08-18 13:12:00\n",
      "    Recommendation 2 :\n",
      "        Document ID: 30\n",
      "        Title: D_Trump2018_8_17_0_45\n",
      "        Attribute: {'author': 'D_Trump30', 'source': None, 'time': '1534491900.0'}\n",
      "        Author: D_Trump30\n",
      "        Source: None\n",
      "        Time: 2018-08-17 00:45:00\n",
      "---------------\n",
      "-------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print('------------- Get document recommendations -----------------')\n",
    "\n",
    "dataset = 'trump_tweets' # str | Dataset name.\n",
    "#query = '(\"Trump\" OR \"president\")' # str | Fulltext query, using mysql MATCH boolean query format. Example, (\\\"word1\\\" OR \\\"word2\\\") AND (\\\"word3\\\" OR \\\"word4\\\") (optional)\n",
    "query = ''\n",
    "custom_stop_words = [\"real\",\"hillary\"] # ERRORUNKNOWN | List of stop words. (optional)\n",
    "num_topics = 8 # int | Number of topics to be extracted from the dataset. (optional) (default to 8)\n",
    "num_keywords = 8 # int | Number of keywords per topic that is extracted from the dataset. (optional) (default to 8)\n",
    "excluded_docs = '' # str | List of document IDs that should be excluded from the analysis. Example, \\\"docid1, docid2, ..., docidN\\\"  (optional)\n",
    "\n",
    "try:\n",
    "    api_response = api_instance.get_doc_recommend_api(\n",
    "        dataset, \n",
    "        query=query, \n",
    "        custom_stop_words=custom_stop_words, \n",
    "        num_topics=num_topics, \n",
    "        num_keywords=num_keywords)\n",
    "except ApiException as e:\n",
    "    print(\"Exception when calling DocumentsApi->get_doc_recommend_api: %s\\n\" % e)\n",
    "    \n",
    "i = 1\n",
    "for res in api_response.result:\n",
    "    print('Document recommendations for topic', i, ':')\n",
    "    print('    Keywords:', res.topic)\n",
    "\n",
    "    j = 1\n",
    "    for doc in res.recommendations:\n",
    "        print('    Recommendation', j, ':')\n",
    "        print('        Document ID:', doc.sourceid)\n",
    "        print('        Title:', doc.title)\n",
    "        print('        Attribute:', doc.attribute)\n",
    "        print('        Author:', doc.attribute.author)\n",
    "        print('        Source:', doc.attribute.source)\n",
    "        print('        Time:', datetime.datetime.fromtimestamp(float(doc.attribute.time)))\n",
    "        j = j + 1\n",
    "    \n",
    "    print('---------------')\n",
    "    i = i + 1\n",
    "    \n",
    "#pprint(api_response)   # raw API response\n",
    "print('-------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get document summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------ Get document summary  --------------------\n",
      "Document Summary\n",
      "    ID: 50\n",
      "    Title: D_Trump2018_8_15_15_4\n",
      "    Summary: ['Our Country was built on Tariffs and Tariffs are now leading us to great new Trade Deals - as opposed to the horrible and unfair Trade Deals that I inherited as your President.', 'Other Countries should not be allowed to come in and steal the wealth of our great U.S.A. No longer!']\n",
      "-------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print('------------------ Get document summary  --------------------')\n",
    "\n",
    "dataset = 'trump_tweets' # str | Dataset name.\n",
    "doc_title = 'D_Trump2018_8_15_15_4' # str | The title of the document to be summarized.\n",
    "custom_stop_words = [\"real\",\"hillary\"] # ERRORUNKNOWN | List of stop words. (optional)\n",
    "summary_length = 6 # int | The maximum number of bullet points a user wants to see in the document summary. (optional) (default to 6)\n",
    "context_amount = 0 # int | The number of sentences surrounding key summary sentences in the documents that they come from. (optional) (default to 0)\n",
    "\n",
    "try:\n",
    "    api_response = api_instance.get_doc_summary_api(\n",
    "        dataset, \n",
    "        doc_title, \n",
    "        custom_stop_words=custom_stop_words, \n",
    "        summary_length=summary_length, \n",
    "        context_amount=context_amount)\n",
    "    \n",
    "except ApiException as e:\n",
    "    print(\"Exception when calling DocumentsApi->get_doc_summary_api: %s\\n\" % e)\n",
    " \n",
    "print('Document Summary')\n",
    "print('    ID:', api_response.result.summary.sourceid)\n",
    "print('    Title:', api_response.result.doc_title)\n",
    "print('    Summary:', api_response.result.summary.sentences)\n",
    "\n",
    "#pprint(api_response)   # raw API response\n",
    "print('-------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
