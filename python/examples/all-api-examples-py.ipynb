{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial Initialization\n",
    "\n",
    "NOTICE: All information contained herein is, and remains the property of SumUp Analytics Inc. and its suppliers, if any. The intellectual and technical concepts contained herein are proprietary to SumUp Analytics Inc. and its suppliers and may be covered by U.S. and Foreign Patents, patents in process, and are protected by trade secret or copyright law.\n",
    "\n",
    "Dissemination of this information or reproduction of this material is strictly forbidden unless prior written permission is obtained from SumUp Analytics Inc.\n",
    "\n",
    "Copyright (c) 2018-2020 SumUp Analytics, Inc. All Rights Reserved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure API host and key, and create a new API instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import json\n",
    "import datetime\n",
    "import time\n",
    "import nucleus_api\n",
    "from nucleus_api.rest import ApiException\n",
    "import nucleus_api.api.nucleus_api as nucleus_helper\n",
    "from pprint import pprint\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# Determine if in Jupyter notebook or not\n",
    "try:\n",
    "    ip = get_ipython()\n",
    "    running_notebook = True\n",
    "except NameError:\n",
    "    running_notebook = False\n",
    "\n",
    "if running_notebook:\n",
    "    print('Running example in Jupyter Notebook')\n",
    "else:\n",
    "    print('Running example in script mode')\n",
    "    \n",
    "configuration = nucleus_api.Configuration()\n",
    "configuration.host = 'UPDATE-WITH-API-SERVER-HOSTNAME'\n",
    "configuration.api_key['x-api-key'] = 'UPDATE-WITH-API-KEY'\n",
    "\n",
    "# Create API instance\n",
    "api_instance = nucleus_api.NucleusApi(nucleus_api.ApiClient(configuration))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Management\n",
    "\n",
    "In this section, we walk you through our dataset ingestion and management APIs. You will learn how to:\n",
    "- Create a new dataset from different origin locations\n",
    "- Customize the metadata you want to include in a dataset\n",
    "- Append documents to an existing dataset\n",
    "- Delete specific documents or entire datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Append specific file from local drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset = \"dataset_test\"\n",
    "file = 'quarles20181109a.pdf'         \n",
    "metadata = {\"time\": \"1/2/2018\", \n",
    "            \"author\": \"Test Author\"}  # Optional json containing additional document metadata\n",
    "try:\n",
    "    api_response = api_instance.post_upload_file(file, dataset, metadata=metadata)\n",
    "    fp = api_response.result\n",
    "    print(fp.filename, '(', fp.size, 'bytes) has been added to dataset', dataset,)    \n",
    "except ApiException as e:\n",
    "    api_error = json.loads(e.body)\n",
    "    print('ERROR:', api_error['message'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Append all PDFs from a folder to a dataset using parallel injection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = 'fomc-minutes'         \n",
    "dataset = 'dataset_test'# str | Destination dataset where the file will be inserted.\n",
    "\n",
    "# build file iterable. Each item in the iterable is in the format below:\n",
    "# {'filename': filename,   # filename to be uploaded. REQUIRED\n",
    "#  'metadata': {           # metadata for the file. Optional\n",
    "#      'key1': val1,       # keys can have arbiturary names as long as the names only\n",
    "#      'key2': val2        # contain alphanumeric (0-9|a-z|A-Z) and underscore (_)\n",
    "#   } \n",
    "# }\n",
    "\n",
    "file_iter = []\n",
    "for root, dirs, files in os.walk(folder):\n",
    "    for file in files:\n",
    "        if Path(file).suffix == '.pdf':\n",
    "            file_dict = {'filename': os.path.join(root, file),\n",
    "                         'metadata': {'field1': 'financial'}}\n",
    "            file_iter.append(file_dict)\n",
    "\n",
    "file_props = nucleus_helper.upload_files(api_instance, dataset, file_iter, processes=1)\n",
    "for fp in file_props:\n",
    "    print(fp.filename, '(', fp.size, 'bytes) has been added to dataset', dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Append a file from a URL to a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset = 'dataset_test'\n",
    "file_url = 'https://s3-us-west-2.amazonaws.com/sumup-public/nucleus-sdk/quarles20181109a.docx'\n",
    "# Optional filename saved on the server for the URL. If not specified, Nucleus will make an intelligent guess from the file URL\n",
    "filename = 'quarles20181109a-newname.pdf'  \n",
    "payload = nucleus_api.UploadURLModel(dataset=dataset,\n",
    "                                     file_url=file_url,\n",
    "                                     filename=filename)\n",
    "try:\n",
    "    api_response = api_instance.post_upload_url(payload)\n",
    "    url_prop = api_response.result\n",
    "    print(url_prop.file_url, '(', url_prop.size, ' bytes) has been added to dataset', dataset)\n",
    "\n",
    "except ApiException as e:\n",
    "    api_error = json.loads(e.body)\n",
    "    print('ERROR:', api_error['message'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Append multiple URLs to a dataset using parallel injection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset = 'dataset_test'\n",
    "file_urls = ['https://s3-us-west-2.amazonaws.com/sumup-public/nucleus-sdk/quarles20181109a.docx',\n",
    "             'https://s3-us-west-2.amazonaws.com/sumup-public/nucleus-sdk/quarles20181109b.docx',\n",
    "             'https://s3-us-west-2.amazonaws.com/sumup-public/nucleus-sdk/quarles20181109c.docx',\n",
    "             'https://s3-us-west-2.amazonaws.com/sumup-public/nucleus-sdk/quarles20181109d.docx']\n",
    "\n",
    "url_props = nucleus_helper.upload_urls(api_instance, dataset, file_urls, processes=1)\n",
    "\n",
    "for up in url_props:\n",
    "    print(up.file_url, '(', up.size, ' bytes) has been added to dataset', dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Append a JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'dataset_test'\n",
    "\n",
    "# The fields \"title\", \"time\", and \"content\" are mandatory in the JSON record.\n",
    "# Users can add any custom fields to the JSON record and all the information will be saved as metadata for the document.\n",
    "document = {\"title\": \"This a test json title field\",\n",
    "            \"time\": \"2019-01-01\",\n",
    "            \"content\": \"This is a test json content field\"}\n",
    "\n",
    "payload = nucleus_api.Appendjsonparams(dataset=dataset,\n",
    "                                       document=document)\n",
    "try:\n",
    "    api_response = api_instance.post_append_json_to_dataset(payload)\n",
    "    print(api_response.result)\n",
    "except ApiException as e:\n",
    "    api_error = json.loads(e.body)\n",
    "    print('ERROR:', api_error['message'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The JSON record must have \"title\", \"time\", and \"content\" fields.\n",
    "\n",
    "Users can add custom fields to the JSON record and all the information will be saved as metadata for the dataset.\n",
    "\n",
    "This metadata can subsequently be used in the analytics APIs to apply custom selections of documents in your dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Append JSONs from CSV file using parallel injection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "csv_file = 'trump-tweets-100.csv'\n",
    "dataset = 'trump_tweets'\n",
    "\n",
    "with open(csv_file, encoding='utf-8-sig') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    json_props = nucleus_helper.upload_jsons(api_instance, dataset, reader, processes=1)\n",
    "    \n",
    "    total_size = 0\n",
    "    total_jsons = 0\n",
    "    for jp in json_props:\n",
    "        total_size += jp.size\n",
    "        total_jsons += 1\n",
    "        \n",
    "    print(total_jsons, 'JSON records (', total_size, 'bytes) appended to', dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CSV file must have \"title\", \"time\", and \"content\" columns.\n",
    "\n",
    "Users can add any column to their CSV file and all the information will be saved as metadata for the dataset.\n",
    "\n",
    "This metadata can subsequently be used in the analytics APIs to apply custom selections of documents in your dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a dataset using embedded datafeeds:\n",
    "\n",
    "The Nucleus platform makes available a collection of datafeeds in read-only mode to users.\n",
    "- Central Banks: content in native language and English official translation where needed, grouped by content category, covering 14 Federal banks and all US Regional banks\n",
    "- SEC filings: 10Ks, 10Qs, 8Ks, 6Ks, 20Fs and S1s, including revised /A files for all companies filing with the SEC\n",
    "- News Media RSS: 200 English RSS feeds covering the fields of AI, Finance, Economics, News, Crypto, Culture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Central Banks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_central_bank = 'sumup/central_banks_chinese'\n",
    "metadata_selection_central_bank = {'bank': 'people_bank_of_china', \n",
    "                                   'document_category': ('speech', 'press release', 'publication')}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connect to these feeds by language using the following naming structure for the dataset name: 'sumup/central_banks_LANGUAGE'\n",
    "\n",
    "with LANGUAGE in {english, chinese, japanese, german, portuguese, spanish, russian, french, italian}\n",
    "\n",
    "You can then define a custom metadata selection off this feed by specifiying a set of banks and a set of document categories\n",
    "- document_category in {speech, press release, publication, formal research}\n",
    "- bank in {federal_reserve, bank_of_canada, banco_de_mexico, bank_of_brazil, ecb, bank_of_england, bundesbank, bank_of_france, bank_of_italy, bank_of_spain, russian_fed, people_bank_of_china, bank_of_japan, bank_of_australia, atlanta_fed, boston_fed, chicago_fed, cleveland_fed, dallas_fed, kansas_city_fed, minneapolis_fed, new_york_fed, philadelphia_fed, richmond_fed, san_francisco_fed, st_louis_fed}\n",
    "\n",
    "When passing these parameters to any of the analytics APIs, you can also specify a time period selection using either of:\n",
    "- The time_period input argument\n",
    "- The period_start and period_end input arguments\n",
    "\n",
    "Examples of such calls are detailed in this tutorial, within the sections discussing analytics APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### News RSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'sumup/rss_feed_ai'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connect to these feeds by field using the following naming structure for the dataset name: 'sumup/rss_feed_FIELD'\n",
    "\n",
    "with FIELD in {ai, finance, economics, news, crypto, culture}\n",
    "\n",
    "When passing these parameters to any of the analytics APIs, you can also specify a time period selection using either of:\n",
    "- The time_period input argument\n",
    "- The period_start and period_end input arguments\n",
    "\n",
    "Examples of such calls are detailed in this tutorial, within the sections discussing analytics APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SEC Filings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GET THE LIST OF ALL THE COMPANIES AVAILABLE IN THE FEED\n",
    "\n",
    "payload = nucleus_api.EdgarFields(tickers=[], \n",
    "                                  filing_types=[], \n",
    "                                  sections=[])\n",
    "try:\n",
    "    api_response = api_instance.post_available_sec_filings(payload)\n",
    "    api_ok = True\n",
    "except ApiException as e:\n",
    "    api_error = json.loads(e.body)\n",
    "    print('ERROR:', api_error['message'])\n",
    "    api_ok = False\n",
    "    \n",
    "if api_ok:\n",
    "    print('SEC filings selected:')\n",
    "    print('    Company count:', len(api_response.result.tickers))\n",
    "    print('    Date range:', api_response.result.date_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GET THE LIST OF AVAILABLE FILING TYPES FOR A COMPANY\n",
    "\n",
    "payload = nucleus_api.EdgarFields(tickers=[\"IBM\"], # Select IBM company\n",
    "                                  filing_types=[], \n",
    "                                  sections=[])\n",
    "try:\n",
    "    api_response = api_instance.post_available_sec_filings(payload)\n",
    "    api_ok = True\n",
    "except ApiException as e:\n",
    "    api_error = json.loads(e.body)\n",
    "    print('ERROR:', api_error['message'])\n",
    "    api_ok = False\n",
    "\n",
    "if api_ok:\n",
    "    print('SEC filings for:', api_response.result.tickers)\n",
    "    print('    Types:', api_response.result.filing_types)\n",
    "    print('    Count:', api_response.result.count)\n",
    "    print('    Date ranges:', api_response.result.date_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GET THE LIST OF AVAILABLE SECTIONS IN A GIVEN FILING TYPE FOR A GIVEN COMPANY\n",
    "\n",
    "payload = nucleus_api.EdgarFields(tickers=[\"IBM\"], # Select IBM company\n",
    "                                  filing_types=[\"10-K\"], # Get list of sections available in 10-Ks\n",
    "                                  sections=[])\n",
    "try:\n",
    "    api_response = api_response = api_instance.post_available_sec_filings(payload)\n",
    "    api_ok = True\n",
    "except ApiException as e:\n",
    "    api_error = json.loads(e.body)\n",
    "    print('ERROR:', api_error['message'])\n",
    "    api_ok = False\n",
    "    \n",
    "if api_ok:\n",
    "    print('Sections in {} filings for {}'.format(api_response.result.filing_types, api_response.result.tickers))\n",
    "    for section in api_response.result.sections:\n",
    "        print('    {}'.format(section))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BUILD A DATASET FROM A CUSTOM SELECTION OF SEC FILINGS\n",
    "\n",
    "dataset = \"dataset_sec1\" \n",
    "\n",
    "# Dataset from a particular section for a ticker\n",
    "payload = nucleus_api.EdgarQuery(destination_dataset=dataset,\n",
    "                                 tickers=[\"BABA\"], \n",
    "                                 filing_types=[\"20-F\"], \n",
    "                                 sections=[\"Quantitative and Qualitative Disclosures about Market Risk\"])\n",
    "try:\n",
    "    api_response = api_instance.post_create_dataset_from_sec_filings(payload)\n",
    "    api_ok = True\n",
    "except ApiException as e:\n",
    "    api_error = json.loads(e.body)\n",
    "    print('ERROR:', api_error['message'])\n",
    "    api_ok = False\n",
    "    \n",
    "if api_ok:\n",
    "    print('Dataset {} created successfully from SEC filings'.format(api_response.result['destination_dataset']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BUILD A DATASET FROM A CUSTOM SELECTION OF SEC FILINGS\n",
    "\n",
    "dataset = \"dataset_sec2\" \n",
    "period_start = \"2018-01-01\" \n",
    "period_end= \"2019-06-01\"\n",
    "\n",
    "# Dataset is all 8Ks for  the last 18 months\n",
    "payload = nucleus_api.EdgarQuery(destination_dataset=dataset,\n",
    "                                 tickers=[\"NFLX\"], \n",
    "                                 filing_types=[\"8-K\"], \n",
    "                                 sections=[],\n",
    "                                 period_start=period_start,\n",
    "                                 period_end=period_end)\n",
    "try:\n",
    "    api_response = api_instance.post_create_dataset_from_sec_filings(payload)\n",
    "    api_ok = True\n",
    "except ApiException as e:\n",
    "    api_error = json.loads(e.body)\n",
    "    print('ERROR:', api_error['message'])\n",
    "    api_ok = False\n",
    "    \n",
    "if api_ok:\n",
    "    print('Dataset {} created successfully from SEC filings'.format(api_response.result['destination_dataset']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SEC filings have a more complex structure, therefore Nucleus provides a specific set of APIs to interact with this data and to allow you to create tailored datasets\n",
    "\n",
    "There are two payloads available to you:\n",
    "- nucleus_api.EdgarFields, which allows you to navigate the SEC filings' content alongside specific requirements using the post_available_sec_filings API\n",
    "- nucleus_api.EdgarQuery, which allows you to create SEC filings' datasets with specific requirements using the post_create_dataset_from_sec_filings API\n",
    "\n",
    "These payloads expose 5 optional arguments:\n",
    "- tickers, which are as-recorded in the EDGAR database\n",
    "- filing_types, which are to be chosen among {10-K, 10-Q, 8-K, 6-K, 20-F, S-1, 10-K/A, 10-Q/A, 8-K/A, 6-K/A, 20-F/A, S-1/A}\n",
    "- sections, which are the standardized section names as-recorded in the EDGAR database for each form type\n",
    "- period_start, the starting date of the period you are interested in\n",
    "- period_end, the end date of the period you are interested in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List all the datasets available to a user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    api_response = api_instance.get_list_datasets()\n",
    "except ApiException as e:\n",
    "    print(\"Exception when calling DatasetsApi->get_list_datasets: %s\\n\" % e)\n",
    "\n",
    "list_datasets = api_response.result\n",
    "\n",
    "print(len(list_datasets), 'datasets in the database:')\n",
    "for ds in list_datasets:\n",
    "    print('    ', ds.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve summary information for a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'dataset_sec2' # str | Dataset name.\n",
    "query = '' # str | Fulltext query, using mysql MATCH boolean query format. (optional)\n",
    "metadata_selection = '' # str | json object of {\\\"metadata_field\\\":[\\\"selected_values\\\"]} (optional)\n",
    "time_period = '' # str | Time period selection (optional)\n",
    "period_start = \"\" # str | Start date for the period to analyze within the dataset. Format: \"YYYY-MM-DD HH:MM:SS\"\n",
    "period_end = \"\" # str | End date for the period to analyze within the dataset. Format: \"YYYY-MM-DD HH:MM:SS\"\n",
    "\n",
    "try:\n",
    "    payload = nucleus_api.DatasetInfo(dataset=dataset, \n",
    "                                    query=query, \n",
    "                                    metadata_selection=metadata_selection, \n",
    "                                    time_period=time_period)\n",
    "    api_response = api_instance.post_dataset_info(payload)\n",
    "    print('Information about dataset', dataset)\n",
    "    print('    Language:', api_response.result.detected_language)\n",
    "    print('    Number of documents:', api_response.result.num_documents)\n",
    "    print('    Time range:', datetime.datetime.fromtimestamp(float(api_response.result.time_range[0])),\n",
    "             'to', datetime.datetime.fromtimestamp(float(api_response.result.time_range[1])))\n",
    "except ApiException as e:\n",
    "    api_error = json.loads(e.body)\n",
    "    print('ERROR:', api_error['message'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete documents from a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset = 'dataset_test'\n",
    "\n",
    "doc_ids = ['1']\n",
    "payload = nucleus_api.DeleteDocumentModel(dataset=dataset,\n",
    "                                          doc_ids=doc_ids)\n",
    "try:\n",
    "    api_response = api_instance.post_delete_document(payload)\n",
    "except ApiException as e:\n",
    "    print(\"Exception when calling DatasetsApi->post_delete_document: %s\\n\" % e)\n",
    "\n",
    "\n",
    "print('Document', doc_ids, 'from dataset', dataset, 'deleted.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset = 'dataset_test'\n",
    "payload = nucleus_api.DeleteDatasetModel(dataset=dataset) # Deletedatasetmodel | \n",
    "\n",
    "try:\n",
    "    api_response = api_instance.post_delete_dataset(payload)\n",
    "    print(api_response.result['dataset_deleted'])\n",
    "except ApiException as e:\n",
    "    api_error = json.loads(e.body)\n",
    "    print('ERROR:', api_error['message'])\n",
    "    \n",
    "# List datasets again to check if the specified dataset has been deleted\n",
    "try:\n",
    "    api_response = api_instance.get_list_datasets()\n",
    "except ApiException as e:\n",
    "    api_error = json.loads(e.body)\n",
    "    print('ERROR:', api_error['message'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic-Level Analytics\n",
    "\n",
    "This section goes over all APIs that enable users to identify, extract and analyze topics found in a dataset.\n",
    "- Topic modeling\n",
    "- Topic transfer learning for propagation analysis of topics' strength, sentiment and consensus\n",
    "- Sentiment analysis\n",
    "- Consensus analysis\n",
    "- Cross-documents topic summarization\n",
    "- Historical analysis of topics' strength, sentiment, and consensus\n",
    "- Authors similarity analysis\n",
    "- Contrasted topic modeling: topic best separating two sub-categories of documents in a corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset = 'trump_tweets'\n",
    "#query = '(\"Trump\" OR \"president\")' # str | Fulltext query, using mysql MATCH boolean query format. Example, (\\\"word1\\\" OR \\\"word2\\\") AND (\\\"word3\\\" OR \\\"word4\\\") (optional)\n",
    "query = ''\n",
    "custom_stop_words = [\"real\",\"hillary\"] # str | List of stop words. (optional)\n",
    "num_topics = 8 # int | Number of topics to be extracted from the dataset. (optional) (default to 8)\n",
    "metadata_selection = \"\" # dict | JSON object specifying metadata-based queries on the dataset, of type {\"metadata_field\": \"selected_values\"} (optional)\n",
    "time_period = \"\"     # str | Time period selection. Choices: [\"1M\",\"3M\",\"6M\",\"12M\",\"3Y\",\"5Y\",\"\"] (optional)\n",
    "\n",
    "try:\n",
    "    payload = nucleus_api.Topics(dataset=dataset,                                \n",
    "                                query=query,                   \n",
    "                                custom_stop_words=custom_stop_words,     \n",
    "                                num_topics=num_topics,\n",
    "                                metadata_selection=metadata_selection,\n",
    "                                time_period=time_period)\n",
    "    api_response = api_instance.post_topic_api(payload)        \n",
    "except ApiException as e:\n",
    "    api_error = json.loads(e.body)\n",
    "    print('ERROR:', api_error['message'])\n",
    "\n",
    "doc_ids = api_response.result.doc_ids\n",
    "topics = api_response.result.topics\n",
    "for i, res in enumerate(topics):\n",
    "    print('Topic', i, 'keywords:')\n",
    "    print('    Keywords:', res.keywords)\n",
    "    keywords_weight_str = \";\".join(str(x) for x in res.keywords_weight)\n",
    "    print('    Keyword weights:', keywords_weight_str)\n",
    "    print('    Strength:', res.strength)\n",
    "    doc_topic_exposure_sel = []  # list of non-zero doc_topic_exposures\n",
    "    doc_id_sel = []        # list of doc ids matching doc_topic_exposure_sel\n",
    "    for j in range(len(res.doc_topic_exposures)):\n",
    "        doc_topic_exp = float(res.doc_topic_exposures[j])\n",
    "        if doc_topic_exp != 0:\n",
    "            doc_topic_exposure_sel.append(doc_topic_exp)\n",
    "            doc_id_sel.append(doc_ids[j])\n",
    "    \n",
    "    doc_id_sel_str = ' '.join(str(x) for x in doc_id_sel)\n",
    "    doc_topic_exposure_sel_str = ' '.join(str(x) for x in doc_topic_exposure_sel)\n",
    "    print('    Document IDs:', doc_id_sel_str)\n",
    "    print('    Document exposures:', doc_topic_exposure_sel_str)\n",
    "    print('---------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topics within time range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset = 'trump_tweets'\n",
    "#query = '(\"Trump\" OR \"president\")' # str | Fulltext query, using mysql MATCH boolean query format. Example, (\\\"word1\\\" OR \\\"word2\\\") AND (\\\"word3\\\" OR \\\"word4\\\") (optional)\n",
    "query = ''\n",
    "custom_stop_words = [\"real\",\"hillary\"] # str | List of stop words. (optional)\n",
    "num_topics = 8 # int | Number of topics to be extracted from the dataset. (optional) (default to 8)\n",
    "metadata_selection = \"\" # dict | JSON object specifying metadata-based queries on the dataset, of type {\"metadata_field\": \"selected_values\"} (optional)\n",
    "period_start = \"2016-10-15\" # str | Start date for the period to analyze within the dataset. Format: \"YYYY-MM-DD\"\n",
    "period_end = \"2019-01-01\" # str | End date for the period to analyze within the dataset. Format: \"YYYY-MM-DD\"\n",
    "\n",
    "try:\n",
    "    payload = nucleus_api.Topics(dataset=dataset,                                \n",
    "                                 query=query,                   \n",
    "                                 custom_stop_words=custom_stop_words,     \n",
    "                                 num_topics=num_topics,\n",
    "                                 metadata_selection=metadata_selection,\n",
    "                                 period_start=period_start,\n",
    "                                 period_end=period_end)\n",
    "    api_response = api_instance.post_topic_api(payload)        \n",
    "except ApiException as e:\n",
    "    api_error = json.loads(e.body)\n",
    "    print('ERROR:', api_error['message'])\n",
    "    \n",
    "doc_ids = api_response.result.doc_ids\n",
    "topics = api_response.result.topics\n",
    "for i, res in enumerate(topics):\n",
    "    print('Topic', i, 'keywords:')\n",
    "    print('    Keywords:', res.keywords)\n",
    "    keywords_weight_str = \";\".join(str(x) for x in res.keywords_weight)\n",
    "    print('    Keyword weights:', keywords_weight_str)\n",
    "    print('    Strength:', res.strength)\n",
    "    doc_topic_exposure_sel = []  # list of non-zero doc_topic_exposure\n",
    "    doc_id_sel = []        # list of doc ids matching doc_topic_exposure_sel\n",
    "    for j in range(len(res.doc_topic_exposures)):\n",
    "        doc_topic_exp = float(res.doc_topic_exposures[j])\n",
    "        if doc_topic_exp != 0:\n",
    "            doc_topic_exposure_sel.append(doc_topic_exp)\n",
    "            doc_id_sel.append(doc_ids[j])\n",
    "    \n",
    "    doc_id_sel_str = ' '.join(str(x) for x in doc_id_sel)\n",
    "    doc_topic_exposure_sel_str = ' '.join(str(x) for x in doc_topic_exposure_sel)\n",
    "    print('    Document IDs:', doc_id_sel_str)\n",
    "    print('    Document exposures:', doc_topic_exposure_sel_str)\n",
    "    print('---------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topics with metadata selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'trump_tweets'\n",
    "#query = '(\"Trump\" OR \"president\")' # str | Fulltext query, using mysql MATCH boolean query format. Example, (\\\"word1\\\" OR \\\"word2\\\") AND (\\\"word3\\\" OR \\\"word4\\\") (optional)\n",
    "query = ''\n",
    "custom_stop_words = [\"real\",\"hillary\"] # str | List of stop words. (optional)\n",
    "num_topics = 8 # int | Number of topics to be extracted from the dataset. (optional) (default to 8)\n",
    "metadata_selection = {\"author\": \"D_Trump16\"} # dict | JSON object specifying metadata-based queries on the dataset, of type {\"metadata_field\": \"selected_values\"} (optional)\n",
    "\n",
    "try:\n",
    "    payload = nucleus_api.Topics(dataset=dataset,                                \n",
    "                                 query=query,                   \n",
    "                                 custom_stop_words=custom_stop_words,     \n",
    "                                 num_topics=num_topics,\n",
    "                                 metadata_selection=metadata_selection)\n",
    "    api_response = api_instance.post_topic_api(payload)        \n",
    "except ApiException as e:\n",
    "    api_error = json.loads(e.body)\n",
    "    print('ERROR:', api_error['message'])\n",
    "    \n",
    "doc_ids = api_response.result.doc_ids\n",
    "topics = api_response.result.topics\n",
    "for i, res in enumerate(topics):\n",
    "    print('Topic', i, 'keywords:')\n",
    "    print('    Keywords:', res.keywords)\n",
    "    keywords_weight_str = \";\".join(str(x) for x in res.keywords_weight)\n",
    "    print('    Keyword weights:', keywords_weight_str)\n",
    "    print('    Strength:', res.strength)\n",
    "    doc_topic_exposure_sel = []  # list of non-zero doc_topic_exposure\n",
    "    doc_id_sel = []        # list of doc ids matching doc_topic_exposure_sel\n",
    "    for j in range(len(res.doc_topic_exposures)):\n",
    "        doc_topic_exp = float(res.doc_topic_exposures[j])\n",
    "        if doc_topic_exp != 0:\n",
    "            doc_topic_exposure_sel.append(doc_topic_exp)\n",
    "            doc_id_sel.append(doc_ids[j])\n",
    "    \n",
    "    doc_id_sel_str = ' '.join(str(x) for x in doc_id_sel)\n",
    "    doc_topic_exposure_sel_str = ' '.join(str(x) for x in doc_topic_exposure_sel)\n",
    "    print('    Document IDs:', doc_id_sel_str)\n",
    "    print('    Document exposures:', doc_topic_exposure_sel_str)\n",
    "    print('---------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topics without removing redundant content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'trump_tweets'\n",
    "#query = '(\"Trump\" OR \"president\")' # str | Fulltext query, using mysql MATCH boolean query format. Example, (\\\"word1\\\" OR \\\"word2\\\") AND (\\\"word3\\\" OR \\\"word4\\\") (optional)\n",
    "query = ''\n",
    "custom_stop_words = [\"real\",\"hillary\"] # str | List of stop words. (optional)\n",
    "num_topics = 8 # int | Number of topics to be extracted from the dataset. (optional) (default to 8)\n",
    "metadata_selection = \"\" # dict | JSON object specifying metadata-based queries on the dataset, of type {\"metadata_field\": \"selected_values\"} (optional)\n",
    "remove_redundancies = False # bool | If True, this option removes quasi-duplicates from the analysis. A quasi-duplicate would have the same NLP representation, but not necessarily the exact same text. (optional) (default True)\n",
    "\n",
    "try:\n",
    "    payload = nucleus_api.Topics(dataset=dataset,                                \n",
    "                                 query=query,                   \n",
    "                                 custom_stop_words=custom_stop_words,     \n",
    "                                 num_topics=num_topics,\n",
    "                                 metadata_selection=metadata_selection,\n",
    "                                 remove_redundancies=remove_redundancies)\n",
    "    api_response = api_instance.post_topic_api(payload)        \n",
    "except ApiException as e:\n",
    "    api_error = json.loads(e.body)\n",
    "    print('ERROR:', api_error['message'])\n",
    "    \n",
    "doc_ids = api_response.result.doc_ids\n",
    "topics = api_response.result.topics\n",
    "for i, res in enumerate(topics):\n",
    "    print('Topic', i, 'keywords:')\n",
    "    print('    Keywords:', res.keywords)\n",
    "    keywords_weight_str = \";\".join(str(x) for x in res.keywords_weight)\n",
    "    print('    Keyword weights:', keywords_weight_str)\n",
    "    print('    Strength:', res.strength)\n",
    "    doc_topic_exposure_sel = []  # list of non-zero doc_topic_exposure\n",
    "    doc_id_sel = []        # list of doc ids matching doc_topic_exposure_sel\n",
    "    for j in range(len(res.doc_topic_exposures)):\n",
    "        doc_topic_exp = float(res.doc_topic_exposures[j])\n",
    "        if doc_topic_exp != 0:\n",
    "            doc_topic_exposure_sel.append(doc_topic_exp)\n",
    "            doc_id_sel.append(doc_ids[j])\n",
    "    \n",
    "    doc_id_sel_str = ' '.join(str(x) for x in doc_id_sel)\n",
    "    doc_topic_exposure_sel_str = ' '.join(str(x) for x in doc_topic_exposure_sel)\n",
    "    print('    Document IDs:', doc_id_sel_str)\n",
    "    print('    Document exposures:', doc_topic_exposure_sel_str)\n",
    "    print('---------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate summary for each topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'trump_tweets'\n",
    "#query = '(\"Trump\" OR \"president\")' # str | Fulltext query, using mysql MATCH boolean query format. Example, (\\\"word1\\\" OR \\\"word2\\\") AND (\\\"word3\\\" OR \\\"word4\\\") (optional)\n",
    "query = ''\n",
    "custom_stop_words = [\"real\",\"hillary\"] # str | List of stop words. (optional)\n",
    "num_topics = 8 # int | Number of topics to be extracted from the dataset. (optional) (default to 8)\n",
    "num_keywords = 8 # int | Number of keywords per topic that is extracted from the dataset. (optional) (default to 8)\n",
    "summary_length = 6 # int | The maximum number of bullet points a user wants to see in each topic summary. (optional) (default to 6)\n",
    "context_amount = 0 # int | The number of sentences surrounding key summary sentences in the documents that they come from. (optional) (default to 0)\n",
    "num_docs = 20 # int | The maximum number of key documents to use for summarization. (optional) (default to 20)\n",
    "excluded_docs = '' # str | List of document IDs that should be excluded from the analysis. Example, [\"docid1\", \"docid2\", ..., \"docidN\"]  (optional)\n",
    "remove_redundancies = False # bool | If True, this option removes quasi-duplicates from the analysis. A quasi-duplicate would have the same NLP representation, but not necessarily the exact same text. (optional) (default True)\n",
    "\n",
    "metadata_selection =\"\" # dict | JSON object specifying metadata-based queries on the dataset, of type {\"metadata_field\": \"selected_values\"} (optional)\n",
    "time_period = \"\"     # str | Time period selection. Choices: [\"1M\",\"3M\",\"6M\",\"12M\",\"3Y\",\"5Y\",\"\"]  (optional)\n",
    "period_start = \"\" # str | Start date for the period to analyze within the dataset. Format: \"YYYY-MM-DD HH:MM:SS\"\n",
    "period_end = \"\" # str | End date for the period to analyze within the dataset. Format: \"YYYY-MM-DD HH:MM:SS\"\n",
    "api_response = None\n",
    "\n",
    "try:\n",
    "    payload = nucleus_api.TopicSummaryModel\t(dataset=dataset, \n",
    "                                             query=query,\n",
    "                                             custom_stop_words=custom_stop_words, \n",
    "                                             num_topics=num_topics, \n",
    "                                             num_keywords=num_keywords,\n",
    "                                             metadata_selection=metadata_selection,\n",
    "                                             summary_length=summary_length, \n",
    "                                             context_amount=context_amount, \n",
    "                                             num_docs=num_docs)\n",
    "    api_response = api_instance.post_topic_summary_api(payload)\n",
    "    api_ok = True\n",
    "except ApiException as e:\n",
    "    api_error = json.loads(e.body)\n",
    "    print('ERROR:', api_error['message'])\n",
    "    api_ok = False\n",
    "\n",
    "if api_ok:\n",
    "    for i,res in enumerate(api_response.result):\n",
    "        print('Topic', i, 'summary:')\n",
    "        print('    Keywords:', res.keywords)\n",
    "        for j in range(len(res.summary)):\n",
    "            print(res.summary[j])\n",
    "            print('    Document ID:', res.summary[j].doc_id)\n",
    "            print('        Title:', res.summary[j].title)\n",
    "            print('        Sentences:', res.summary[j].sentences)\n",
    "            print('        Author:', res.summary[j].attribute['author'])\n",
    "            print('        Time:', datetime.datetime.fromtimestamp(float(res.summary[j].attribute['time'])))\n",
    "        print('---------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measure sentiment on each topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset = 'trump_tweets' # str | Dataset name\n",
    "#query = '(\"Trump\" OR \"president\")' # str | Fulltext query, using mysql MATCH boolean query format. Example, (\\\"word1\\\" OR \\\"word2\\\") AND (\\\"word3\\\" OR \\\"word4\\\") (optional)\n",
    "query = ''\n",
    "custom_stop_words = [\"real\",\"hillary\"] # str | List of stop words. (optional)\n",
    "num_topics = 8 # int | Number of topics to be extracted from the dataset. (optional) (default to 8)\n",
    "num_keywords = 8 # int | Number of keywords per topic that is extracted from the dataset. (optional) (default to 8)\n",
    "excluded_docs = '' # str | List of document IDs that should be excluded from the analysis. Example, [\"docid1\", \"docid2\", ..., \"docidN\"]  (optional)\n",
    "custom_dict_file = {\"great\": 1.0, \"awful\": -1.0, \"clinton\":-1.0, \"trump\":1.0} # file | Custom sentiment dictionary JSON file. Example, {\"field1\": value1, ..., \"fieldN\": valueN} (optional)\n",
    "remove_redundancies = False # bool | If True, this option removes quasi-duplicates from the analysis. A quasi-duplicate would have the same NLP representation, but not necessarily the exact same text. (optional) (default True)\n",
    "\n",
    "metadata_selection =\"\" # dict | JSON object specifying metadata-based queries on the dataset, of type {\"metadata_field\": \"selected_values\"} (optional)\n",
    "time_period = \"\"     # str | Time period selection. Choices: [\"1M\",\"3M\",\"6M\",\"12M\",\"3Y\",\"5Y\",\"\"] (optional)\n",
    "period_start = \"\" # str | Start date for the period to analyze within the dataset. Format: \"YYYY-MM-DD HH:MM:SS\"\n",
    "period_end = \"\" # str | End date for the period to analyze within the dataset. Format: \"YYYY-MM-DD HH:MM:SS\"\n",
    "\n",
    "try:\n",
    "    payload = nucleus_api.TopicSentimentModel(dataset=dataset, \n",
    "                                              query=query, \n",
    "                                              custom_stop_words=custom_stop_words, \n",
    "                                              num_topics=num_topics, \n",
    "                                              num_keywords=num_keywords,\n",
    "                                              custom_dict_file=custom_dict_file)\n",
    "    api_response = api_instance.post_topic_sentiment_api(payload)\n",
    "    \n",
    "except ApiException as e:\n",
    "    api_error = json.loads(e.body)\n",
    "    print('ERROR:', api_error['message'])\n",
    "\n",
    "for i,res in enumerate(api_response.result):\n",
    "    print('Topic', i, 'sentiment:')\n",
    "    print('    Keywords:', res.keywords)\n",
    "    print('    Sentiment:', res.sentiment)\n",
    "    print('    Strength:', res.strength)\n",
    "    \n",
    "    doc_id_str = ' '.join(str(x) for x in res.doc_ids)\n",
    "    doc_sentiment_str = ' '.join(str(x) for x in res.doc_sentiments)\n",
    "    doc_score_str = ' '.join(str(x) for x in res.doc_topic_exposures)\n",
    "    print('    Document IDs:', doc_id_str)\n",
    "    print('    Document Sentiments:', doc_sentiment_str)\n",
    "    print('    Document Exposures:', doc_score_str)\n",
    "    print('---------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measure consensus on each topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset = 'trump_tweets' # str | Dataset name.\n",
    "query = '' # str | Fulltext query, using mysql MATCH boolean query format. Example, (\\\"word1\\\" OR \\\"word2\\\") AND (\\\"word3\\\" OR \\\"word4\\\") (optional)\n",
    "custom_stop_words = [\"real\",\"hillary\"] # str | List of stop words. (optional)\n",
    "num_topics = 8 # int | Number of topics to be extracted from the dataset. (optional) (default to 8)\n",
    "num_keywords = 8 # int | Number of keywords per topic that is extracted from the dataset. (optional) (default to 8)\n",
    "excluded_docs = [''] # str | List of document IDs that should be excluded from the analysis. Example, [\"docid1\", \"docid2\", ..., \"docidN\"]  (optional)\n",
    "custom_dict_file = {\"great\": 1.0, \"awful\": -1.0, \"clinton\":-1.0, \"trump\":1.0} # file | Custom sentiment dictionary JSON file. Example, {\"field1\": value1, ..., \"fieldN\": valueN} (optional)\n",
    "remove_redundancies = False # bool | If True, this option removes quasi-duplicates from the analysis. A quasi-duplicate would have the same NLP representation, but not necessarily the exact same text. (optional) (default True)\n",
    "\n",
    "metadata_selection =\"\" # dict | JSON object specifying metadata-based queries on the dataset, of type {\"metadata_field\": \"selected_values\"} (optional)\n",
    "time_period = \"\"     # str | Time period selection. Choices: [\"1M\",\"3M\",\"6M\",\"12M\",\"3Y\",\"5Y\",\"\"] (optional)\n",
    "period_start = \"\" # str | Start date for the period to analyze within the dataset. Format: \"YYYY-MM-DD HH:MM:SS\"\n",
    "period_end = \"\" # str | End date for the period to analyze within the dataset. Format: \"YYYY-MM-DD HH:MM:SS\"\n",
    "\n",
    "try:\n",
    "    payload = nucleus_api.TopicConsensusModel(dataset=dataset, \n",
    "                                              query=query, \n",
    "                                              custom_stop_words=custom_stop_words, \n",
    "                                              num_topics=num_topics, \n",
    "                                              num_keywords=num_keywords,\n",
    "                                              custom_dict_file=custom_dict_file)\n",
    "    api_response = api_instance.post_topic_consensus_api(payload)\n",
    "except ApiException as e:\n",
    "    api_error = json.loads(e.body)\n",
    "    print('ERROR:', api_error['message'])\n",
    "    \n",
    "for i, res in enumerate(api_response.result):\n",
    "    print('Topic', i, 'consensus:')\n",
    "    print('    Keywords:', res.keywords)\n",
    "    print('    Consensus:', res.consensus)\n",
    "    print('    Strength:', res.strength)\n",
    "    print('---------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform historical analysis of topics' strength, sentiment, and consensus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'trump_tweets'   # str | Dataset name.\n",
    "update_period = 'm' # str | Frequency at which the historical anlaysis is performed. choices=[\"d\",\"m\",\"H\",\"M\"] (default to d)\n",
    "query = '' # str | Fulltext query, using mysql MATCH boolean query format. Example, (\\\"word1\\\" OR \\\"word2\\\") AND (\\\"word3\\\" OR \\\"word4\\\") (optional)\n",
    "custom_stop_words = [\"real\",\"hillary\"] # str | List of stop words (optional)\n",
    "num_topics = 8 # int | Number of topics to be extracted from the dataset. (optional) (default to 8)\n",
    "num_keywords = 8 # int | Number of keywords per topic that is extracted from the dataset. (optional) (default to 8)\n",
    "inc_step = 1 # int | Number of increments of the udpate period in between two historical computations. (optional) (default to 1)\n",
    "excluded_docs = [''] # str | List of document IDs that should be excluded from the analysis. Example, [\"docid1\", \"docid2\", ..., \"docidN\"]  (optional)\n",
    "custom_dict_file = {} # file | Custom sentiment dictionary JSON file. Example, {\"field1\": value1, ..., \"fieldN\": valueN} (optional)\n",
    "remove_redundancies = False # bool | If True, this option removes quasi-duplicates from the analysis. A quasi-duplicate would have the same NLP representation, but not necessarily the exact same text. (optional) (default True)\n",
    "\n",
    "metadata_selection =\"\" # dict | JSON object specifying metadata-based queries on the dataset, of type {\"metadata_field\": \"selected_values\"} (optional)\n",
    "time_period = \"12M\"     # str | Time period selection. Choices: [\"1M\",\"3M\",\"6M\",\"12M\",\"3Y\",\"5Y\",\"\"] (optional)\n",
    "period_start = \"\" # str | Start date for the period to analyze within the dataset. Format: \"YYYY-MM-DD HH:MM:SS\"\n",
    "period_end = \"\" # str | End date for the period to analyze within the dataset. Format: \"YYYY-MM-DD HH:MM:SS\"\n",
    "api_response = None\n",
    "try:\n",
    "    payload = nucleus_api.TopicHistoryModel(dataset=dataset, \n",
    "                                            time_period=time_period, \n",
    "                                            query=query, \n",
    "                                            custom_stop_words=custom_stop_words, \n",
    "                                            num_topics=num_topics, \n",
    "                                            num_keywords=num_keywords, \n",
    "                                            metadata_selection=metadata_selection, \n",
    "                                            excluded_docs=excluded_docs,\n",
    "                                            custom_dict_file=custom_dict_file)\n",
    "    api_response = api_instance.post_topic_historical_analysis_api(payload)\n",
    "except ApiException as e:\n",
    "    api_error = json.loads(e.body)\n",
    "    print('ERROR:', api_error['message'])\n",
    "    print(e)\n",
    "\n",
    "print('Printing historical metrics data...')\n",
    "print('NOTE: historical metrics data can be plotted when running the example in Jupyter Notebook')\n",
    "\n",
    "for i,res in enumerate(api_response.result):\n",
    "    print('Topic', i, res.keywords)\n",
    "    print('    Timestamps:', res.time_stamps)\n",
    "    print('    Strengths:', res.strengths)\n",
    "    print('    Consensuses:', res.consensuses)\n",
    "    print('    Sentiments:', res.sentiments)\n",
    "    print('----------------')\n",
    "            \n",
    "\n",
    "# chart the historical metrics when running in Jupyter Notebook\n",
    "if running_notebook:\n",
    "    print('Plotting historical metrics data...')\n",
    "    historical_metrics = []\n",
    "    for res in api_response.result:\n",
    "        # construct a list of historical metrics dictionaries for charting\n",
    "        historical_metrics.append({\n",
    "            'topic'    : res.keywords,\n",
    "            'time_stamps' : np.array(res.time_stamps),\n",
    "            'strength' : np.array(res.strengths, dtype=np.float32),\n",
    "            'consensus': np.array(res.consensuses, dtype=np.float32), \n",
    "            'sentiment': np.array(res.sentiments, dtype=np.float32)})\n",
    "\n",
    "    selected_topics = range(len(historical_metrics)) \n",
    "    #nucleus_helper.topic_charts_historical(historical_metrics, selected_topics, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determine network of authors similar to a chosen contributor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'trump_tweets' # str | Dataset name.\n",
    "target_author = 'D_Trump16' # str | Name of the author to be analyzed.\n",
    "query = '' # str | Fulltext query, using mysql MATCH boolean query format. Subject covered by the author, on which to focus the analysis of connectivity. Example, (\\\"word1\\\" OR \\\"word2\\\") AND (\\\"word3\\\" OR \\\"word4\\\") (optional)\n",
    "custom_stop_words = [\"real\",\"hillary\"] # str | List of words possibly used by the target author that are considered not information-bearing. (optional)\n",
    "excluded_docs = [''] # str | List of document IDs that should be excluded from the analysis. Example, [\"docid1\", \"docid2\", ..., \"docidN\"]  (optional)\n",
    "\n",
    "metadata_selection =\"\" # dict | JSON object specifying metadata-based queries on the dataset, of type {\"metadata_field\": \"selected_values\"} (optional)\n",
    "time_period = \"12M\"     # str | Time period selection. Choices: [\"1M\",\"3M\",\"6M\",\"12M\",\"3Y\",\"5Y\",\"\"] (optional)\n",
    "period_start = \"\" # str | Start date for the period to analyze within the dataset. Format: \"YYYY-MM-DD HH:MM:SS\"\n",
    "period_end = \"\" # str | End date for the period to analyze within the dataset. Format: \"YYYY-MM-DD HH:MM:SS\"\n",
    "\n",
    "try:\n",
    "    payload = nucleus_api.AuthorConnection(dataset=dataset, \n",
    "                                           target_author=target_author, \n",
    "                                           query=query, \n",
    "                                           custom_stop_words=custom_stop_words, \n",
    "                                           time_period=time_period, \n",
    "                                           metadata_selection=metadata_selection, \n",
    "                                           excluded_docs=excluded_docs)\n",
    "    api_response = api_instance.post_author_connectivity_api(payload)    \n",
    "except ApiException as e:\n",
    "    api_error = json.loads(e.body)\n",
    "    print('ERROR:', api_error['message'])\n",
    "\n",
    "res = api_response.result\n",
    "print('Mainstream connections:')\n",
    "for mc in res.mainstream_connections:\n",
    "    print('    Keywords:', mc.keywords)\n",
    "    print('    Authors:', \" \".join(str(x) for x in mc.authors))\n",
    "    \n",
    "print('Niche connections:')\n",
    "for nc in res.niche_connections:\n",
    "    print('    Keywords:', nc.keywords)\n",
    "    print('    Authors:', \" \".join(str(x) for x in nc.authors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply transfer learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply transfer learning to topics in one dataset onto another"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset0 = 'trump_tweets'\n",
    "dataset1 = None # str | Validation dataset (optional if period_0 and period_1 dates provided)\n",
    "#query = '(\"Trump\" OR \"president\")' # str | Fulltext query, using mysql MATCH boolean query format. Example, (\\\"word1\\\" OR \\\"word2\\\") AND (\\\"word3\\\" OR \\\"word4\\\") (optional)\n",
    "query = ''\n",
    "custom_stop_words = [\"\"] # str | List of stop words. (optional)\n",
    "num_topics = 8 # int | Number of topics to be extracted from the dataset. (optional) (default to 8)\n",
    "num_keywords = 8 # int | Number of keywords per topic that is extracted from the dataset. (optional) (default to 8)\n",
    "metadata_selection = \"\" # dict | JSON object specifying metadata-based queries on the dataset, of type {\"metadata_field\": \"selected_values\"} (optional)\n",
    "period_0_start = '2018-08-12' # Not needed if you provide a validation dataset in the \"dataset1\" variable \n",
    "period_0_end = '2018-08-16' # Not needed if you provide a validation dataset in the \"dataset1\" variable\n",
    "period_1_start = '2018-08-14' # Not needed if you provide a validation dataset in the \"dataset1\" variable\n",
    "period_1_end = '2018-08-18' # Not needed if you provide a validation dataset in the \"dataset1\" variable\n",
    "excluded_docs = '' # str | List of document IDs that should be excluded from the analysis. Example, [\"docid1\", \"docid2\", ..., \"docidN\"]  (optional)\n",
    "remove_redundancies = False # bool | If True, this option removes quasi-duplicates from the analysis. A quasi-duplicate would have the same NLP representation, but not necessarily the exact same text. (optional) (default True)\n",
    "\n",
    "try:\n",
    "    payload = nucleus_api.TopicTransferModel(dataset0=dataset0,\n",
    "                                             dataset1=dataset1,\n",
    "                                             query=query, \n",
    "                                             custom_stop_words=custom_stop_words, \n",
    "                                             num_topics=num_topics, \n",
    "                                             num_keywords=num_keywords,\n",
    "                                             period_0_start=period_0_start,\n",
    "                                             period_0_end=period_0_end,\n",
    "                                             period_1_start=period_1_start,\n",
    "                                             period_1_end=period_1_end,\n",
    "                                             metadata_selection=metadata_selection)\n",
    "    api_response = api_instance.post_topic_transfer_api(payload)\n",
    "    api_ok = True\n",
    "except ApiException as e:\n",
    "    print(e)\n",
    "    api_error = json.loads(e.body)\n",
    "    print('ERROR:', api_error['message'])\n",
    "    api_ok = False\n",
    "\n",
    "print(api_response)\n",
    "\n",
    "if api_ok:\n",
    "    doc_ids_t1 = api_response.result.doc_ids_t1\n",
    "    topics = api_response.result.topics\n",
    "    for i,res in enumerate(topics):\n",
    "        print('Topic', i, 'exposure within validation dataset:')\n",
    "        print('    Keywords:', res.keywords)\n",
    "        print('    Strength:', res.strength)\n",
    "        print('    Document IDs:', doc_ids_t1)\n",
    "        print('    Exposure per Doc in Validation Dataset:', res.doc_topic_exposures_t1)\n",
    "        print('---------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply transfer learning to topics exogenously chosen onto a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset0 = 'trump_tweets'\n",
    "dataset1 = None # str | Validation dataset (optional if period_0 and period_1 dates provided)\n",
    "fixed_topics = [{\"keywords\": [\"north korea\", \"nuclear weapons\", \"real estate\"], \"weights\": [0.5, 0.3, 0.2]},\n",
    "               {\"keywords\": [\"America\", \"jobs\", \"stock market\"], \"weights\": [0.3, 0.3, 0.3]}] # The weights are optional\n",
    "query = ''\n",
    "custom_stop_words = [\"\"] # str | List of stop words. (optional)\n",
    "num_topics = 8 # int | Number of topics to be extracted from the dataset. (optional) (default to 8)\n",
    "num_keywords = 8 # int | Number of keywords per topic that is extracted from the dataset. (optional) (default to 8)\n",
    "metadata_selection = \"\" # dict | JSON object specifying metadata-based queries on the dataset, of type {\"metadata_field\": \"selected_values\"} (optional)\n",
    "period_0_start = '2017-01-01' # Not needed if you provide a validation dataset in the \"dataset1\" variable \n",
    "period_0_end = '2017-12-31' # Not needed if you provide a validation dataset in the \"dataset1\" variable\n",
    "period_1_start = '2018-01-01' # Not needed if you provide a validation dataset in the \"dataset1\" variable\n",
    "period_1_end = '2018-08-18' # Not needed if you provide a validation dataset in the \"dataset1\" variable\n",
    "period_0_start = '2018-08-12' # Not needed if you provide a validation dataset in the \"dataset1\" variable \n",
    "period_0_end = '2018-08-16' # Not needed if you provide a validation dataset in the \"dataset1\" variable\n",
    "period_1_start = '2018-08-14' # Not needed if you provide a validation dataset in the \"dataset1\" variable\n",
    "period_1_end = '2018-08-18' # Not needed if you provide a validation dataset in the \"dataset1\" variable\n",
    "\n",
    "excluded_docs = '' # str | List of document IDs that should be excluded from the analysis. Example, [\"docid1\", \"docid2\", ..., \"docidN\"]  (optional)\n",
    "remove_redundancies = False # bool | If True, this option removes quasi-duplicates from the analysis. A quasi-duplicate would have the same NLP representation, but not necessarily the exact same text. (optional) (default True)\n",
    "\n",
    "try:\n",
    "    payload = nucleus_api.TopicTransferModel(dataset0=dataset0,\n",
    "                                             dataset1=dataset1,\n",
    "                                             fixed_topics=fixed_topics,\n",
    "                                             query=query, \n",
    "                                             custom_stop_words=custom_stop_words, \n",
    "                                             num_topics=num_topics, \n",
    "                                             num_keywords=num_keywords,\n",
    "                                             period_0_start=period_0_start,\n",
    "                                             period_0_end=period_0_end,\n",
    "                                             period_1_start=period_1_start,\n",
    "                                             period_1_end=period_1_end,\n",
    "                                             metadata_selection=metadata_selection)\n",
    "    api_response = api_instance.post_topic_transfer_api(payload)\n",
    "    api_ok = True\n",
    "except ApiException as e:\n",
    "    api_error = json.loads(e.body)\n",
    "    print('ERROR:', api_error['message'])\n",
    "    api_ok = False\n",
    "\n",
    "if api_ok:\n",
    "    doc_ids_t1 = api_response.result.doc_ids_t1\n",
    "    topics = api_response.result.topics\n",
    "    for i,res in enumerate(topics):\n",
    "        print('Topic', i, 'exposure within validation dataset:')\n",
    "        print('    Keywords:', res.keywords)\n",
    "        print('    Strength:', res.strength)\n",
    "        print('    Document IDs:', doc_ids_t1)\n",
    "        print('    Exposure per Doc in Validation Dataset:', res.doc_topic_exposures_t1)\n",
    "        print('---------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply transfer learning to topics in one dataset onto another for sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset0 = 'trump_tweets'\n",
    "dataset1 = None\n",
    "#dataset1 = dataset # str | Validation dataset (optional if period_0 and period_1 dates provided)\n",
    "#query = '(\"Trump\" OR \"president\")' # str | Fulltext query, using mysql MATCH boolean query format. Example, (\\\"word1\\\" OR \\\"word2\\\") AND (\\\"word3\\\" OR \\\"word4\\\") (optional)\n",
    "#fixed_topic is also an available input argument\n",
    "query = ''\n",
    "custom_stop_words = [\"\"] # str | List of stop words. (optional)\n",
    "num_topics = 8 # int | Number of topics to be extracted from the dataset. (optional) (default to 8)\n",
    "num_keywords = 8 # int | Number of keywords per topic that is extracted from the dataset. (optional) (default to 8)\n",
    "metadata_selection = \"\" # dict | JSON object specifying metadata-based queries on the dataset, of type {\"metadata_field\": \"selected_values\"} (optional)\n",
    "period_0_start = '2018-08-12' # Not needed if you provide a validation dataset in the \"dataset1\" variable \n",
    "period_0_end = '2018-08-16' # Not needed if you provide a validation dataset in the \"dataset1\" variable\n",
    "period_1_start = '2018-08-14' # Not needed if you provide a validation dataset in the \"dataset1\" variable\n",
    "period_1_end = '2018-08-18' # Not needed if you provide a validation dataset in the \"dataset1\" variable\n",
    "excluded_docs = '' # str | List of document IDs that should be excluded from the analysis. Example, [\"docid1\", \"docid2\", ..., \"docidN\"]  (optional)\n",
    "custom_dict_file = {\"great\": 1.0, \"awful\": -1.0, \"clinton\":-1.0, \"trump\":1.0} # file | Custom sentiment dictionary JSON file. Example, {\"field1\": value1, ..., \"fieldN\": valueN} (optional)\n",
    "remove_redundancies = False # bool | If True, this option removes quasi-duplicates from the analysis. A quasi-duplicate would have the same NLP representation, but not necessarily the exact same text. (optional) (default True)\n",
    "\n",
    "try:\n",
    "    payload = nucleus_api.TopicSentimentTransferModel(dataset0=dataset0,\n",
    "                                                      dataset1=dataset1,\n",
    "                                                      query=query, \n",
    "                                                      custom_stop_words=custom_stop_words, \n",
    "                                                      num_topics=num_topics, \n",
    "                                                      num_keywords=num_keywords,\n",
    "                                                      period_0_start=period_0_start,\n",
    "                                                      period_0_end=period_0_end,\n",
    "                                                      period_1_start=period_1_start,\n",
    "                                                      period_1_end=period_1_end,\n",
    "                                                      metadata_selection=metadata_selection,\n",
    "                                                      custom_dict_file=custom_dict_file)\n",
    "    \n",
    "    api_response = api_instance.post_topic_sentiment_transfer_api(payload)\n",
    "    api_ok = True\n",
    "except ApiException as e:\n",
    "    api_error = json.loads(e.body)\n",
    "    print('ERROR:', api_error['message'])\n",
    "    api_ok = False\n",
    "\n",
    "if api_ok:\n",
    "    topics = api_response.result\n",
    "    for i,res in enumerate(topics):\n",
    "        print('Topic', i, 'exposure within validation dataset:')\n",
    "        print('    Keywords:', res.keywords)\n",
    "        print('    Strength:', res.strength)\n",
    "        print('    Sentiment:', res.sentiment)\n",
    "        print('    Document IDs:', res.doc_ids_t1)\n",
    "        print('    Sentiment per Doc in Validation Dataset:', res.doc_sentiments_t1)\n",
    "        print('---------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply transfer learning to topics in one dataset onto another for consensus analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset0 = 'trump_tweets'\n",
    "dataset1 = None # str | Validation dataset (optional if period_0 and period_1 dates provided)\n",
    "#query = '(\"Trump\" OR \"president\")' # str | Fulltext query, using mysql MATCH boolean query format. Example, (\\\"word1\\\" OR \\\"word2\\\") AND (\\\"word3\\\" OR \\\"word4\\\") (optional)\n",
    "#fixed_topic is also an available input argument\n",
    "query = ''\n",
    "custom_stop_words = [\"\"] # str | List of stop words. (optional)\n",
    "num_topics = 8 # int | Number of topics to be extracted from the dataset. (optional) (default to 8)\n",
    "num_keywords = 8 # int | Number of keywords per topic that is extracted from the dataset. (optional) (default to 8)\n",
    "metadata_selection = \"\" # dict | JSON object specifying metadata-based queries on the dataset, of type {\"metadata_field\": \"selected_values\"} (optional)\n",
    "period_0_start = '2018-08-12' # Not needed if you provide a validation dataset in the \"dataset1\" variable \n",
    "period_0_end = '2018-08-16' # Not needed if you provide a validation dataset in the \"dataset1\" variable\n",
    "period_1_start = '2018-08-14' # Not needed if you provide a validation dataset in the \"dataset1\" variable\n",
    "period_1_end = '2019-08-18' # Not needed if you provide a validation dataset in the \"dataset1\" variable\n",
    "excluded_docs = '' # str | List of document IDs that should be excluded from the analysis. Example, [\"docid1\", \"docid2\", ..., \"docidN\"]  (optional)\n",
    "custom_dict_file = {\"great\": 1.0, \"awful\": -1.0, \"clinton\":-1.0, \"trump\":1.0} # file | Custom sentiment dictionary JSON file. Example, {\"field1\": value1, ..., \"fieldN\": valueN} (optional)\n",
    "remove_redundancies = False # bool | If True, this option removes quasi-duplicates from the analysis. A quasi-duplicate would have the same NLP representation, but not necessarily the exact same text. (optional) (default True)\n",
    "\n",
    "try:\n",
    "    payload = nucleus_api.TopicConsensusTransferModel(dataset0=dataset0,\n",
    "                                                      dataset1=dataset1,\n",
    "                                                      query=query,\n",
    "                                                      custom_stop_words=custom_stop_words, \n",
    "                                                      num_topics=num_topics, \n",
    "                                                      num_keywords=num_keywords,\n",
    "                                                      period_0_start=period_0_start,\n",
    "                                                      period_0_end=period_0_end,\n",
    "                                                      period_1_start=period_1_start,\n",
    "                                                      period_1_end=period_1_end,\n",
    "                                                      metadata_selection=metadata_selection,\n",
    "                                                      custom_dict_file=custom_dict_file)\n",
    "    \n",
    "    api_response = api_instance.post_topic_consensus_transfer_api(payload)\n",
    "    api_ok = True\n",
    "except ApiException as e:\n",
    "    api_error = json.loads(e.body)\n",
    "    print('ERROR:', api_error['message'])\n",
    "    api_ok = False\n",
    "\n",
    "if api_ok:\n",
    "    topics = api_response.result\n",
    "    for i,res in enumerate(topics):\n",
    "        print('Topic', i, 'exposure within validation dataset:')\n",
    "        print('    Keywords:', res.keywords)\n",
    "        print('    Consensus:', res.consensus)\n",
    "        print('---------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract a topic contrasting two subsets of content in a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'trump_tweets' # str | Dataset name.\n",
    "metadata_selection = {\"content\": \"Trump\"} # dict | The metadata selection defining the two categories of documents to contrast and summarize against each other\n",
    "query = '' # str | Dataset-language-specific fulltext query, using mysql MATCH boolean query format (optional)\n",
    "custom_stop_words = [\"real\",\"hillary\"] # List of stop words. (optional)\n",
    "time_period = \"1M\" # str | Alternative 1: time period counting back from today over which the analysis is conducted (optional)\n",
    "period_start = '2018-08-12' # str | Alternative 2: start of period over which the analysis is conducted (optional)\n",
    "period_end = '2018-08-15' # str | Alternative 2: start of period over which the analysis is conducted (optional)\n",
    "excluded_docs = '' # str | List of document IDs that should be excluded from the analysis. Example, [\"docid1\", \"docid2\", ..., \"docidN\"]  (optional)\n",
    "syntax_variables = True # bool | Specifies whether to take into account syntax aspects of each category of documents to help with contrasting them (optional) (default to False)\n",
    "compression = 0.002 # float | Parameter controlling the breadth of the contrasted topic. Contained between 0 and 1, the smaller it is, the more contrasting terms will be captured, with decreasing weight. (optional) (default to 0.000002)\n",
    "remove_redundancies = False # bool | If True, this option removes quasi-duplicates from the analysis and retain only one copy of it. A quasi-duplicate would have the same NLP representation, but not necessarily the exact same text. (optional) (default False)\n",
    "metadata_selection_contrast = {\"content\": \"Trump\"}\n",
    "\n",
    "try:\n",
    "    payload = nucleus_api.TopicContrastModel(\n",
    "        dataset=dataset, \n",
    "        metadata_selection=metadata_selection,\n",
    "        metadata_selection_contrast=metadata_selection_contrast\n",
    "    )\n",
    "    api_response = api_instance.post_topic_contrast_api(payload)\n",
    "    \n",
    "    print('Contrasted Topic')\n",
    "    print('    Keywords:', api_response.result.keywords)\n",
    "    print('    Keywords Weight:', api_response.result.keywords_weight)\n",
    "    \n",
    "except ApiException as e:\n",
    "    api_error = json.loads(e.body)\n",
    "    print('ERROR:', api_error['message'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document-Level Analytics\n",
    "\n",
    "This section goes over all APIs that enable users to analyze documents in a dataset on a standalone basis.\n",
    "- Sentiment analysis\n",
    "- Document summarization and contrasted summarization\n",
    "- Document classification\n",
    "- Named Entity recognition (strict match off pre-determined list)\n",
    "- Content recommendation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of all documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'trump_tweets'\n",
    "# doc_titles, doc_ids, and metadata_selection below are filters to narrow down \n",
    "# documents to be retrieved.\n",
    "# The information of all documents will be retrived when no filters are provided.\n",
    "\n",
    "# doc_titles: list of strings\n",
    "# The titles of the documents to retrieve. Example: [\"title1\", \"title2\", ..., \"titleN\"]  (optional)\n",
    "# doc_titles = ['D_Trump2018_8_18_1_47']   \n",
    "doc_titles = []\n",
    "# doc_ids: list of strings\n",
    "# The docid of the documents to retrieve. Example: [\"docid1\", \"docid2\", ..., \"docidN\"]  (optional)\n",
    "# doc_ids = ['3397215194896514820', '776902852041351634']\n",
    "doc_ids = []\n",
    "\n",
    "# metadata_selection = {\"author\": \"D_Trump16\"} # dict | A selector off metadata. Example: {\"field\": \"value\"}  (optional)\n",
    "metadata_selection = ''\n",
    "\n",
    "try:\n",
    "    payload = nucleus_api.DocInfo(dataset=dataset, \n",
    "                                doc_titles=doc_titles, \n",
    "                                doc_ids=doc_ids,\n",
    "                                metadata_selection=metadata_selection)\n",
    "    api_response = api_instance.post_doc_info(payload)\n",
    "except ApiException as e:\n",
    "    api_error = json.loads(e.body)\n",
    "    print('ERROR:', api_error['message'])\n",
    "\n",
    "for res in api_response.result:\n",
    "    print('Document ID:', res.doc_id)\n",
    "    print('    title:', res.title)\n",
    "    for attr in res.attribute.keys():\n",
    "        if attr == 'time':\n",
    "            print('   ', attr, ':', datetime.datetime.fromtimestamp(float(res.attribute[attr])))\n",
    "        else:\n",
    "            print('   ', attr, ':', res.attribute[attr])\n",
    "    print('---------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of documents with a metadata selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'trump_tweets' # str | Dataset name.\n",
    "metadata_selection = {\"author\": \"D_Trump16\"}      # dict | A selector off metadata. Example: {\"field\": \"value\"}  (optional)\n",
    "\n",
    "try:\n",
    "    payload = nucleus_api.DocInfo(dataset=dataset, metadata_selection=metadata_selection)\n",
    "    api_response = api_instance.post_doc_info(payload)\n",
    "    \n",
    "except ApiException as e:\n",
    "    api_error = json.loads(e.body)\n",
    "    print('ERROR:', api_error['message'])\n",
    "\n",
    "for res in api_response.result:\n",
    "    print('Document ID:', res.doc_id)\n",
    "    print('    title:', res.title)\n",
    "    for attr in res.attribute.keys():\n",
    "        if attr == 'time':\n",
    "            print('   ', attr, ':', datetime.datetime.fromtimestamp(float(res.attribute[attr])))\n",
    "        else:\n",
    "            print('   ', attr, ':', res.attribute[attr])\n",
    "    print('---------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display chosen documents, content and metadata included"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset = 'trump_tweets' # str | Dataset name.\n",
    "#doc_titles = ['D_Trump2018_8_18_1_47']   # str | The title of the documents to retrieve. Example: [\"title1\", \"title2\", ..., \"titleN\"]  (optional)\n",
    "doc_ids = ['4046653213651213725']      # str | The docid of the documents to retrieve. Example: [\"docid1\", \"docid2\", ..., \"docidN\"]  (optional)\n",
    "\n",
    "try:\n",
    "    payload = nucleus_api.DocDisplay(dataset, doc_ids=doc_ids)\n",
    "    api_response = api_instance.post_doc_display(payload)\n",
    "    \n",
    "except ApiException as e:\n",
    "    api_error = json.loads(e.body)\n",
    "    print('ERROR:', api_error['message'])\n",
    "\n",
    "for res in api_response.result:\n",
    "    print('Document ID:', res.doc_id)\n",
    "    print('    Title:', res.title)\n",
    "    print('    Author:', res.attribute['author'])\n",
    "    print('    Time:', datetime.datetime.fromtimestamp(float(res.attribute['time'])))\n",
    "    print('    Content', res.attribute['content'])\n",
    "    print('---------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display chosen documents, content and metadata included, with a metadata selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'trump_tweets' # str | Dataset name.\n",
    "metadata_selection = {\"author\": \"D_Trump16\"}      # dict | A selector off metadata. Example: {\"field\": \"value\"}  (optional)\n",
    "\n",
    "try:\n",
    "    payload = nucleus_api.DocDisplay(dataset=dataset, metadata_selection=metadata_selection)\n",
    "    api_response = api_instance.post_doc_display(payload)\n",
    "    \n",
    "except ApiException as e:\n",
    "    api_error = json.loads(e.body)\n",
    "    print('ERROR:', api_error['message'])\n",
    "\n",
    "for res in api_response.result:\n",
    "    print('Document ID:', res.doc_id)\n",
    "    print('    Title:', res.title)\n",
    "    print('    Author:', res.attribute['author'])\n",
    "    print('    Time:', datetime.datetime.fromtimestamp(float(res.attribute['time'])))\n",
    "    print('    Content', res.attribute['content'])\n",
    "    print('---------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate document recommendations on topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'trump_tweets' # str | Dataset name.\n",
    "#query = '(\"Trump\" OR \"president\")' # str | Fulltext query, using mysql MATCH boolean query format. Example, (\\\"word1\\\" OR \\\"word2\\\") AND (\\\"word3\\\" OR \\\"word4\\\") (optional)\n",
    "query = ''\n",
    "custom_stop_words = [\"real\",\"hillary\"] # str | List of stop words. (optional)\n",
    "num_topics = 8 # int | Number of topics to be extracted from the dataset. (optional) (default to 8)\n",
    "num_keywords = 8 # int | Number of keywords per topic that is extracted from the dataset. (optional) (default to 8)\n",
    "excluded_docs = '' # str | List of document IDs that should be excluded from the analysis. Example, [\"docid1\", \"docid2\", ..., \"docidN\"]  (optional)\n",
    "remove_redundancies = False # bool | If True, this option removes quasi-duplicates from the analysis. A quasi-duplicate would have the same NLP representation, but not necessarily the exact same text. (optional) (default True)\n",
    "\n",
    "try:\n",
    "    payload = nucleus_api.DocumentRecommendModel(dataset=dataset, \n",
    "                                                 query=query, \n",
    "                                                 custom_stop_words=custom_stop_words, \n",
    "                                                 num_topics=num_topics, \n",
    "                                                 num_keywords=num_keywords)\n",
    "    api_response = api_instance.post_doc_recommend_api(payload)\n",
    "except ApiException as e:\n",
    "    api_error = json.loads(e.body)\n",
    "    print('ERROR:', api_error['message'])\n",
    "    \n",
    "for i, res in enumerate(api_response.result):\n",
    "    print('Document recommendations for topic', i, ':')\n",
    "    print('    Keywords:', res.keywords)\n",
    "\n",
    "    for j, doc in enumerate(res.recommendations):\n",
    "        print('    Recommendation', j, ':')\n",
    "        print('        Document ID:', doc.doc_id)\n",
    "        print('        Title:', doc.title)\n",
    "        print('        Attribute:', doc.attribute)\n",
    "        print('        Author:', doc.attribute['author'])\n",
    "        print('        Time:', datetime.datetime.fromtimestamp(float(doc.attribute['time'])))\n",
    "    print('---------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarize a document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset = 'trump_tweets' # str | Dataset name.\n",
    "doc_title = 'D_Trump2018_8_17_14_10' # str | The title of the document to be summarized.\n",
    "custom_stop_words = [\"real\",\"hillary\"] # List of stop words. (optional)\n",
    "summary_length = 6 # int | The maximum number of bullet points a user wants to see in the document summary. (optional) (default to 6)\n",
    "context_amount = 0 # int | The number of sentences surrounding key summary sentences in the documents that they come from. (optional) (default to 0)\n",
    "short_sentence_length = 0 # int | The sentence length below which a sentence is excluded from summarization (optional) (default to 4)\n",
    "long_sentence_length = 40 # int | The sentence length beyond which a sentence is excluded from summarization (optional) (default to 40)\n",
    "\n",
    "try:\n",
    "    payload = nucleus_api.DocumentSummaryModel(dataset=dataset, \n",
    "                                               doc_title=doc_title, \n",
    "                                               custom_stop_words=custom_stop_words, \n",
    "                                                summary_length=summary_length, \n",
    "                                                context_amount=context_amount,\n",
    "                                                short_sentence_length=short_sentence_length,\n",
    "                                                long_sentence_length=long_sentence_length)\n",
    "    api_response = api_instance.post_doc_summary_api(payload)\n",
    "    \n",
    "    print('Summary for', api_response.result.doc_title)\n",
    "    for sent in api_response.result.summary.sentences:\n",
    "        print('    *', sent)\n",
    "    \n",
    "except ApiException as e:\n",
    "    api_error = json.loads(e.body)\n",
    "    print('ERROR:', api_error['message'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarize what makes a document stand-out from the background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'trump_tweets' # str | Dataset name.\n",
    "metadata_selection = {\"content\": \"Trump\"} # dict | The metadata selection defining the two categories of documents to contrast and summarize against each other\n",
    "query = '' # str | Dataset-language-specific fulltext query, using mysql MATCH boolean query format (optional)\n",
    "custom_stop_words = [\"real\",\"hillary\"] # List of stop words. (optional)\n",
    "summary_length = 6 # int | The maximum number of bullet points a user wants to see in the contrasted summary. (optional) (default to 6)\n",
    "context_amount = 0 # int | The number of sentences surrounding key summary sentences in the documents that they come from. (optional) (default to 0)\n",
    "short_sentence_length = 0 # int | The sentence length below which a sentence is excluded from summarization (optional) (default to 4)\n",
    "long_sentence_length = 40 # int | The sentence length beyond which a sentence is excluded from summarization (optional) (default to 40)\n",
    "time_period = \"1M\" # str | Alternative 1: time period counting back from today over which the analysis is conducted (optional)\n",
    "period_start = '2018-08-12' # str | Alternative 2: start of period over which the analysis is conducted (optional)\n",
    "period_end = '2018-08-15' # str | Alternative 2: start of period over which the analysis is conducted (optional)\n",
    "excluded_docs = '' # str | List of document IDs that should be excluded from the analysis. Example, [\"docid1\", \"docid2\", ..., \"docidN\"]  (optional)\n",
    "syntax_variables = True # bool | Specifies whether to take into account syntax aspects of each category of documents to help with contrasting them (optional) (default to False)\n",
    "compression = 0.002 # float | Parameter controlling the breadth of the contrasted summary. Contained between 0 and 1, the smaller it is, the more contrasting terms will be captured, with decreasing weight. (optional) (default to 0.000002)\n",
    "remove_redundancies = False # bool | If True, this option removes quasi-duplicates from the analysis and retain only one copy of it. A quasi-duplicate would have the same NLP representation, but not necessarily the exact same text. (optional) (default False)\n",
    "metadata_selection_contrast = {\"content\": \"Trump\"}\n",
    "\n",
    "try:\n",
    "    payload = nucleus_api.DocumentContrastSummaryModel(\n",
    "        dataset=dataset, \n",
    "        metadata_selection=metadata_selection,\n",
    "        metadata_selection_contrast=metadata_selection_contrast\n",
    "        )\n",
    "    api_response = api_instance.post_document_contrast_summary_api(payload)\n",
    "    \n",
    "    print('Summary for', [x for x in  metadata_selection.values()])\n",
    "    for sent in api_response.result.class_1_content.sentences:\n",
    "        print('    *', sent)\n",
    "    print('======')\n",
    "    for sent in api_response.result.class_2_content.sentences:\n",
    "        print('    *', sent)    \n",
    "except ApiException as e:\n",
    "    api_error = json.loads(e.body)\n",
    "    print('ERROR:', api_error['message'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measure sentiment of document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'trump_tweets' # str | Dataset name.\n",
    "doc_title = 'D_Trump2018_8_17_14_10' # str | The title of the document to be analyzed.\n",
    "custom_stop_words = [\"real\",\"hillary\"] # List of stop words. (optional)\n",
    "num_topics = 8 # int | Number of topics to be extracted from the document. (optional) (default to 8)\n",
    "num_keywords = 8 # int | Number of keywords per topic that is extracted from the document. (optional) (default to 8)\n",
    "\n",
    "try:\n",
    "    payload = nucleus_api.DocumentSentimentModel(dataset=dataset, \n",
    "                                                 doc_title=doc_title, \n",
    "                                                 custom_stop_words=custom_stop_words, \n",
    "                                                 num_topics=num_topics, \n",
    "                                                 num_keywords=num_keywords)\n",
    "    api_response = api_instance.post_doc_sentiment_api(payload)\n",
    "    \n",
    "    print('Sentiment for', api_response.result.doc_title)\n",
    "    print(api_response.result.sentiment)\n",
    "\n",
    "except ValueError as e:\n",
    "    print('ERROR:', e)\n",
    "except ApiException as e:\n",
    "    api_error = json.loads(e.body)\n",
    "    print('ERROR:', api_error['message'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classify documents based on a topic contrasting two categories of content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'trump_tweets' # str | Dataset name.\n",
    "fixed_topics = {\"keywords\": [\"america\", \"jobs\", \"economy\"], \"weights\": [0.5, 0.25, 0.25]} # dict | The contrasting topic used to separate the two categories of documents\n",
    "\n",
    "# Here we want to classify documents that talk about Trump vs documents that don't talk about Trump based on their exposure to the topic [america, jobs, economy]\n",
    "# A more natural classification task for the algo is to define metadata-based categories such as metadata_selection = {\"document_category\": [\"speech\", \"press release\"]}\n",
    "metadata_selection = {\"content\": \"Trump\"} # dict | The metadata selection defining the two categories of documents that a document can be classified into\n",
    "query = '' # str | Dataset-language-specific fulltext query, using mysql MATCH boolean query format (optional)\n",
    "custom_stop_words = [\"real\",\"hillary\"] # List of stop words. (optional)\n",
    "time_period = \"1M\" # str | Alternative 1: time period counting back from today over which the analysis is conducted (optional)\n",
    "period_start = '2018-08-12' # str | Alternative 2: start of period over which the analysis is conducted (optional)\n",
    "period_end = '2018-08-15' # str | Alternative 2: start of period over which the analysis is conducted (optional)\n",
    "excluded_docs = '' # str | List of document IDs that should be excluded from the analysis. Example, [\"docid1\", \"docid2\", ..., \"docidN\"]  (optional)\n",
    "syntax_variables = True # bool | If True, the classifier will include syntax-related variables on top of content variables (optional) (default to False)\n",
    "validation_phase = False # bool | If True, the classifier assumes that the dataset provided is labeled with the 2 classes and will use that to compute accuracy/precision/recall (optional) (default to False)\n",
    "threshold = 0 # float | Threshold value for a document exposure to the contrastic topic, above which the document is assigned to class 1 specified through metadata_selection. (optional) (default to 0)\n",
    "remove_redundancies = False # bool | If True, this option removes quasi-duplicates from the analysis and retain only one copy of it. A quasi-duplicate would have the same NLP representation, but not necessarily the exact same text. (optional) (default False)\n",
    "# classifier_config is the classifier configuration dictionary generated from post_topic_contrast_api\n",
    "# Below is an example showing how to contruct the classifier_config paramenter\n",
    "# classifier_config = {\"keywords\": topic_contrast_response.result.keywords,\n",
    "#                      \"coefs\": topic_contrast_response.result.classifier_config.coefs,\n",
    "#                      \"intercept\": topic_contrast_response.result.classifier_config.intercept}\n",
    "classifier_config = {}\n",
    "metadata_selection_contrast = {\"content\": \"Hillary\"} \n",
    "\n",
    "try:\n",
    "    payload = nucleus_api.DocClassifyModel(\n",
    "        dataset=dataset,\n",
    "        fixed_topics=fixed_topics,\n",
    "        metadata_selection=metadata_selection,\n",
    "        classifier_config=classifier_config,\n",
    "        metadata_selection_contrast=metadata_selection_contrast\n",
    "    )\n",
    "    \n",
    "    api_response = api_instance.post_doc_classify_api(payload)\n",
    "    \n",
    "    print('Detailed Results')\n",
    "    print('    Docids:', api_response.result.detailed_results.doc_ids)\n",
    "    print('    Estimated Category:', api_response.result.detailed_results.estimated_class)\n",
    "    print('    Actual Category:', api_response.result.detailed_results.true_class)\n",
    "    print('\\n')\n",
    "    if validation_phase:\n",
    "        print('Perf Metrics')\n",
    "        print('    Accuracy:', api_response.result.perf_metrics.accuracy)\n",
    "        print('    Recall:', api_response.result.perf_metrics.recall)\n",
    "        print('    Precision:', api_response.result.perf_metrics.precision)\n",
    "        print('    F1:', api_response.result.perf_metrics.f1)\n",
    "        print('    Balanced accuracy:', api_response.result.perf_metrics.balanced_accuracy)\n",
    "except ApiException as e:\n",
    "    api_error = json.loads(e.body)\n",
    "    print('ERROR:', api_error['message'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tag documents based on pre-determined named-entity recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'trump_tweets' # str | Dataset name.\n",
    "payload = nucleus_api.DatasetTagging(\n",
    "    dataset=dataset, \n",
    "    query='new york city OR big apple OR NYC OR New York', \n",
    "    metadata_selection='', \n",
    "    time_period='',\n",
    "    period_start='2010-01-01',\n",
    "    period_end='2019-04-30')\n",
    "\n",
    "try:\n",
    "    api_response = api_instance.post_dataset_tagging(payload)\n",
    "    api_ok = True\n",
    "except ApiException as e:\n",
    "    api_error = json.loads(e.body)\n",
    "    print('ERROR:', api_error['message'])\n",
    "    api_ok = False\n",
    "    \n",
    "if api_ok:\n",
    "    print('    Entities tagged:', api_response.result.entities_tagged)\n",
    "    print('    Docids tagged with the entities:', api_response.result.doc_ids)\n",
    "    print('    Entities count:', api_response.result.entities_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarize files from a URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################################\n",
    "# file_params fields descriptions:  \n",
    "#   file_url              : string, the URL at which the file is stored (could be a S3 bucket address for instance)\n",
    "#   filename              : OPTIONAL string, filename saved on the server. also serves as the doc_title for summarization\n",
    "#   custom_stop_words     : OPTIONAL a string list, user-provided list of stopwords to be excluded from the content analysis leading to document summarization\n",
    "#                            [\"word1\", \"word2\", ...]. DEFAULT: empty\n",
    "#   summary_length        : OPTIONAL an integer, the maximum number of bullet points a user wants to see in the document summary. DEFAULT: 6\n",
    "#   context_amount        : OPTIONAL an integer, the number of sentences surrounding key summary sentences in the original document that a user wants to see in the document summary. DEFAULT: 0\n",
    "#   short_sentence_length : OPTIONAL an integer, the sentence length below which a sentence is excluded from summarization. DEFAULT: 4 words\n",
    "#   long_sentence_length  : OPTIONAL an integer, the sentence length beyond which a sentence is excluded from summarization. DEFAULT: 40 words\n",
    "\n",
    "file_params = {\n",
    "    'file_url': 'https://s3-us-west-2.amazonaws.com/sumup-public/nucleus-sdk/quarles20181109a.docx',\n",
    "    'filename': 'quarles20181109a-newname.pdf',   \n",
    "    'custom_stop_words': [\"document\", \"sometimes\"], \n",
    "    'summary_length': 6,\n",
    "    'context_amount': 0, \n",
    "    'short_sentence_length': 4, \n",
    "    'long_sentence_length': 40}\n",
    "\n",
    "result = nucleus_helper.summarize_file_url(api_instance, file_params)\n",
    "  \n",
    "print('Summary for', result.doc_title, ':')\n",
    "for sent in result.summary.sentences:\n",
    "    print('    *', sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dashboard Modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Authors\n",
    "\n",
    "\"Key Authors\" API identifies the most prolific authors on important subjects in a dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'trump_tweets'\n",
    "query = '' # Dataset-language-specific fulltext query, using SQL MATCH boolean query format. Example: \"(word1 OR word2) AND (word3 OR word4)\" [optional]\n",
    "tracked_queries = [] # List of user-defined queries to track\t[optional]\n",
    "custom_stop_words = [] # List of dataset-language-specific stopwords that should be excluded from the analysis. Example: [\"word1\", \"word2\", ..., \"wordN\"] [optional]\n",
    "num_topics = 8 # Number of topics to be extracted from the dataset per query to aggregate back into a tracker.\t[optional]\n",
    "num_keywords = 8 # Number of keywords per topic that is extracted from the dataset per query.\t[optional]\n",
    "metadata_selection = {} #JSON object specifying metadata-based queries on the dataset, of type {\"metadata_field\": \"selected_values\"} [optional]\n",
    "time_period = '' #Alternative 1: Time period selection\t[optional] [default to '1M']\n",
    "period_start = \"2018-08-13\" #Alternative 2: Start date for the period to analyze within the dataset. Format: \"YYYY-MM-DD\"\t[optional]\n",
    "period_end = \"2018-08-17\" #Alternative 2: End date for the period to analyze within the dataset. Format: \"YYYY-MM-DD\"\t[optional]\n",
    "num_authors = 3 # Max number of key contributors that the user wants to see returned by the analysis. [optional]\n",
    "num_keydocs= 3 # Max number of key contributions from key contributors that the user wants to see returned by the analysis. [optional]\n",
    "excluded_docs = [] # List of document IDs that should be excluded from the analysis. Example: [\"doc_id1\", \"doc_id2\", ..., \"doc_idN\"] [optional]\n",
    "custom_dict_file = {} # JSON records with custom sentiment dictionary: {\"word1\": value1, \"word2\": value2, ..., \"wordN\": valueN}\n",
    "    \n",
    "# Define the KeyAuthorsModel\n",
    "payload = nucleus_api.KeyAuthorsModel(\n",
    "    dataset=dataset, \n",
    "    query=query,\n",
    "    tracked_queries=tracked_queries,\n",
    "    custom_stop_words=custom_stop_words,\n",
    "    num_topics=num_topics,\n",
    "    num_keywords=num_keywords,\n",
    "    metadata_selection=metadata_selection,\n",
    "    time_period=time_period,\n",
    "    period_start=period_start,\n",
    "    period_end=period_end,\n",
    "    num_authors=num_authors, \n",
    "    num_keydocs=num_keydocs,\n",
    "    excluded_docs=excluded_docs,\n",
    "    custom_dict_file=custom_dict_file)\n",
    "\n",
    "try:\n",
    "    api_response = api_instance.post_key_authors_api(payload)\n",
    "    api_ok = True\n",
    "except ApiException as e:\n",
    "    api_error = json.loads(e.body)\n",
    "    print('ERROR:', api_error['message'])\n",
    "    api_ok = False\n",
    "    \n",
    "if api_ok:\n",
    "    for i,t in enumerate(api_response.result):\n",
    "        print('Query/Hot Topics', i)\n",
    "        print('    Query/Hot Topic:', t.query)\n",
    "        print('    Top Authors:')\n",
    "        print('        Negative:', t.top_authors.negative)\n",
    "        print('        Average:', t.top_authors.average)\n",
    "        print('        Positive:', t.top_authors.positive)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Tracker\n",
    "\n",
    "Custom Tracker API keeps a pulse of key metrics on important subjects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'trump_tweets'\n",
    "query = '' #Dataset-language-specific fulltext query, using SQL MATCH boolean query format. Example: \"(word1 OR word2) AND (word3 OR word4)\"\t[optional]\n",
    "tracked_queries = [] #List of user-defined queries to track\t[optional]\n",
    "custom_stop_words = [] #List of dataset-language-specific stopwords that should be excluded from the analysis. Example: [\"word1\", \"word2\", ..., \"wordN\"]\t[optional]\n",
    "num_topics = 8 #Number of topics to be extracted from the dataset per query to aggregate back into a tracker.\t[optional]\n",
    "num_keywords = 8 #Number of keywords per topic that is extracted from the dataset per query.\t[optional]\n",
    "metadata_selection = {} #JSON object specifying metadata-based queries on the dataset, of type {\"metadata_field\": \"selected_values\"}\t[optional]\n",
    "time_period = '' #Alternative 1: Time period selection\t[optional] [default to '1M']\n",
    "period_start = '' #Alternative 2: Start date for the period to analyze within the dataset. Format: \"YYYY-MM-DD\"\t[optional]\n",
    "period_end = '' #Alternative 2: End date for the period to analyze within the dataset. Format: \"YYYY-MM-DD\"\t[optional]\n",
    "n_steps = 10 #Number of steps in the historical analysis over the requested period. Each step is such that they contain an equal number of documents.\t[optional]\n",
    "excluded_docs = [] # List of document IDs that should be excluded from the analysis. Example: [\"doc_id1\", \"doc_id2\", ..., \"doc_idN\"]\t[optional]\n",
    "custom_dict_file = {} # JSON records with custom sentiment dictionary: {\"word1\": value1, \"word2\": value2, ..., \"wordN\": valueN}\n",
    "# Define the CustomTrackerModel\n",
    "payload = nucleus_api.CustomTrackerModel(\n",
    "    dataset=dataset,\n",
    "    query=query,\n",
    "    tracked_queries=tracked_queries,\n",
    "    custom_stop_words=custom_stop_words,\n",
    "    num_topics=num_topics,\n",
    "    num_keywords=num_keywords,\n",
    "    metadata_selection=metadata_selection,\n",
    "    time_period=time_period,\n",
    "    period_start=period_start,\n",
    "    period_end=period_end,\n",
    "    n_steps=n_steps,\n",
    "    excluded_docs=excluded_docs,\n",
    "    custom_dict_file=custom_dict_file\n",
    ")\n",
    "\n",
    "try:\n",
    "    api_response = api_instance.post_custom_tracker_api(payload)\n",
    "    api_ok = True\n",
    "except ApiException as e:\n",
    "    api_error = json.loads(e.body)\n",
    "    print('ERROR:', api_error['message'])\n",
    "    api_ok = False\n",
    "    \n",
    "if api_ok:\n",
    "    for i,t in enumerate(api_response.result):\n",
    "        print('Query/Hot Topics', i)\n",
    "        print('    Query/Hot Topic:', t.query)\n",
    "        print('    Consensuses:', t.consensuses)\n",
    "        print('    Sentiments:', t.sentiments)\n",
    "        print('    Strengths:', t.strengths)\n",
    "        print('    Timestamps:', t.time_stamps)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Smart Alert\n",
    "\n",
    "Smart Alert API brings only novel information to your attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'trump_tweets'  \n",
    "period_start = \"2018-08-13\" #Alternative 2: Start date for the period to analyze within the dataset. Format: \"YYYY-MM-DD\"\t[optional]\n",
    "period_end = \"2018-08-17\" #Alternative 2: End date for the period to analyze within the dataset. Format: \"YYYY-MM-DD\"\t[optional]\n",
    "\n",
    "# Define SmartAlertsModel\n",
    "payload = nucleus_api.SmartAlertsModel(dataset=dataset)\n",
    "\n",
    "try:\n",
    "    api_response = api_instance.post_smart_alerts_api(payload)\n",
    "    api_ok = True\n",
    "except ApiException as e:\n",
    "    api_error = json.loads(e.body)\n",
    "    print('ERROR:', api_error['message'])\n",
    "    api_ok = False\n",
    "    \n",
    "if api_ok:\n",
    "    for i,s in enumerate(api_response.result):\n",
    "        print('New sentences', i)\n",
    "        print('    Query:', s.query)\n",
    "        print('    New sentences:', s.new_sents)\n",
    "        print('    New sentence IDs:', s.new_sents_docids)\n",
    "        print('    New sentence titles:', s.new_sents_titles)\n",
    "        print('    New sentence URLs:', s.new_sents_urls)\n",
    "        print('    New words:', s.new_words)\n",
    "        print('    Novel documents:', s.novel_docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "265px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
