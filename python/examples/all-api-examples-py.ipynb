{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) 2018-2019 SumUp Analytics, Inc. All Rights Reserved.\n",
    "\n",
    "NOTICE: All information contained herein is, and remains the property of SumUp Analytics Inc. and its suppliers, if any. The intellectual and technical concepts contained herein are proprietary to SumUp Analytics Inc. and its suppliers and may be covered by U.S. and Foreign Patents, patents in process, and are protected by trade secret or copyright law.\n",
    "\n",
    "Dissemination of this information or reproduction of this material is strictly forbidden unless prior written permission is obtained from SumUp Analytics Inc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization, configure API host and key, and create new API instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running example in Jupyter Notebook\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import json\n",
    "import datetime\n",
    "import time\n",
    "import nucleus_api\n",
    "from nucleus_api.rest import ApiException\n",
    "import nucleus_api.api.nucleus_api as nucleus_helper\n",
    "from pprint import pprint\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# Determine if in Jupyter notebook or not\n",
    "try:\n",
    "    ip = get_ipython()\n",
    "    running_notebook = True\n",
    "except NameError:\n",
    "    running_notebook = False\n",
    "\n",
    "if running_notebook:\n",
    "    print('Running example in Jupyter Notebook')\n",
    "else:\n",
    "    print('Running example in script mode')\n",
    "    \n",
    "configuration = nucleus_api.Configuration()\n",
    "configuration.host = 'UPDATE-WITH-API-SERVER-HOSTNAME'\n",
    "configuration.api_key['x-api-key'] = 'UPDATE-WITH-API-KEY'\n",
    "\n",
    "# Create API instance\n",
    "api_instance = nucleus_api.NucleusApi(nucleus_api.ApiClient(configuration))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Append file from local drive to dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- Append file from local drive to dataset -----------\n",
      "quarles20181109a.pdf ( 51187 bytes) has been added to dataset dataset_test\n",
      "-------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print('--------- Append file from local drive to dataset -----------')\n",
    "file = 'quarles20181109a.pdf'         # file | \n",
    "dataset = 'dataset_test'              # str | Destination dataset where the file will be inserted.\n",
    "metadata = {\"time\": \"1/2/2018\", \n",
    "            \"author\": \"Test Author\"}  # Optional json containing additional document metadata\n",
    "\n",
    "try:\n",
    "    api_response = api_instance.post_upload_file(file, dataset)\n",
    "    fp = api_response.result\n",
    "    print(fp.filename, '(', fp.size, 'bytes) has been added to dataset', dataset,)    \n",
    "except ApiException as e:\n",
    "    print(\"Exception when calling DatasetsApi->post_upload_file: %s\\n\" % e)\n",
    "\n",
    "print('-------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Append all PDFs from a folder to dataset in parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- Append all files from local folder to dataset in parallel -----------\n",
      "fomcminutes20181219.pdf ( 495324 bytes) has been added to dataset dataset_test\n",
      "fomcminutes20181108.pdf ( 369090 bytes) has been added to dataset dataset_test\n",
      "-------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print('--------- Append all files from local folder to dataset in parallel -----------')\n",
    "folder = 'fomc-minutes'         \n",
    "dataset = 'dataset_test'# str | Destination dataset where the file will be inserted.\n",
    "\n",
    "# get all files from folder recursively\n",
    "file_iters = []\n",
    "for root, dirs, files in os.walk(folder):\n",
    "    for file in files:\n",
    "        if Path(file).suffix == '.pdf':\n",
    "            file_iters.append(os.path.join(root, file))\n",
    "\n",
    "file_props = nucleus_helper.import_files(api_instance, dataset, file_iters, processes=1)\n",
    "for fp in file_props:\n",
    "    print(fp.filename, '(', fp.size, 'bytes) has been added to dataset', dataset)\n",
    "    \n",
    "print('-------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Append file from URL to dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------ Append file from URL to dataset ---------------\n",
      "https://s3-us-west-2.amazonaws.com/sumup-public/nucleus-sdk/quarles20181109a.docx ( 16881  bytes) has been added to dataset dataset_test\n",
      "-------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print('------------ Append file from URL to dataset ---------------')\n",
    "\n",
    "dataset = 'dataset_test'\n",
    "file_url = 'https://s3-us-west-2.amazonaws.com/sumup-public/nucleus-sdk/quarles20181109a.docx'\n",
    "# Optional filename saved on the server for the URL. If not specified, Nucleus will make\n",
    "# an intelligent guess from the file URL\n",
    "filename = 'quarles20181109a-newname.pdf'  \n",
    "payload = nucleus_api.UploadURLModel(\n",
    "                dataset=dataset,\n",
    "                file_url=file_url,\n",
    "                filename=filename  \n",
    "            ) # UploadURLModel | \n",
    "\n",
    "try:\n",
    "    api_response = api_instance.post_upload_url(payload)\n",
    "    url_prop = api_response.result\n",
    "    print(url_prop.file_url, '(', url_prop.size, ' bytes) has been added to dataset', dataset)\n",
    "\n",
    "except ApiException as e:\n",
    "    print(\"Exception when calling DatasetsApi->post_upload_url: %s\\n\" % e)\n",
    "    \n",
    "print('-------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Append files from URLs to dataset in parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------ Append file from URL to dataset ---------------\n",
      "https://s3-us-west-2.amazonaws.com/sumup-public/nucleus-sdk/quarles20181109a.docx ( 16881  bytes) has been added to dataset dataset_test\n",
      "https://s3-us-west-2.amazonaws.com/sumup-public/nucleus-sdk/quarles20181109b.docx ( 16881  bytes) has been added to dataset dataset_test\n",
      "https://s3-us-west-2.amazonaws.com/sumup-public/nucleus-sdk/quarles20181109c.docx ( 16881  bytes) has been added to dataset dataset_test\n",
      "https://s3-us-west-2.amazonaws.com/sumup-public/nucleus-sdk/quarles20181109d.docx ( 16881  bytes) has been added to dataset dataset_test\n",
      "-------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print('------------ Append file from URL to dataset ---------------')\n",
    "\n",
    "dataset = 'dataset_test'\n",
    "file_urls = [\n",
    "    'https://s3-us-west-2.amazonaws.com/sumup-public/nucleus-sdk/quarles20181109a.docx',\n",
    "    'https://s3-us-west-2.amazonaws.com/sumup-public/nucleus-sdk/quarles20181109b.docx',\n",
    "    'https://s3-us-west-2.amazonaws.com/sumup-public/nucleus-sdk/quarles20181109c.docx',\n",
    "    'https://s3-us-west-2.amazonaws.com/sumup-public/nucleus-sdk/quarles20181109d.docx'\n",
    "]\n",
    "\n",
    "url_props = nucleus_helper.import_urls(api_instance, dataset, file_urls, processes=1)\n",
    "\n",
    "for up in url_props:\n",
    "    print(up.file_url, '(', up.size, ' bytes) has been added to dataset', dataset)\n",
    "    \n",
    "print('-------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Append jsons from csv to dataset in parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- Append json from CSV to dataset -----------------\n",
      "100 JSON records ( 20885 bytes) appended to trump_tweets\n",
      "-------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# This dataset will be used to test all topics and documents APIs\n",
    "print('----------- Append json from CSV to dataset -----------------')\n",
    "csv_file = 'trump-tweets-100.csv'\n",
    "dataset = 'trump_tweets'\n",
    "\n",
    "with open(csv_file, encoding='utf-8-sig') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    json_props = nucleus_helper.import_jsons(api_instance, dataset, reader, processes=1)\n",
    "    \n",
    "    total_size = 0\n",
    "    total_jsons = 0\n",
    "    for jp in json_props:\n",
    "        total_size += jp.size\n",
    "        total_jsons += 1\n",
    "        \n",
    "    print(total_jsons, 'JSON records (', total_size, 'bytes) appended to', dataset)\n",
    "\n",
    "print('-------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List available datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-03-28 16:53:42,276 WARNING Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'SSLError(SSLError(1, '[SSL: DECRYPTION_FAILED_OR_BAD_RECORD_MAC] decryption failed or bad record mac (_ssl.c:2171)'),)': /prod/datasets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------- List available datasets ---------------------\n",
      "20 datasets in the database:\n",
      "     aaaaa\n",
      "     crash0\n",
      "     dataset_test\n",
      "     jim_wu_sumup_ai_c1_1551662645858\n",
      "     jim_wu_sumup_ai_chinese_report_1552024272731\n",
      "     jim_wu_sumup_ai_fed_1547146082933\n",
      "     jim_wu_sumup_ai_real_estate_1549815645846\n",
      "     jim_wu_sumup_ai_t2_1550975321013\n",
      "     jim_wu_sumup_ai_test_1549816166057\n",
      "     jim_wu_sumup_ai_test_1550080405584\n",
      "     jim_wu_sumup_ai_txt_encode_1546896541648\n",
      "     jim_wu_sumup_ai_wuxi_sogou_1548286213377\n",
      "     jim_wu_sumup_aimit_tech_review1545314792621\n",
      "     trump_tweets\n",
      "     trump_tweets_full\n",
      "     zte_1553815688159\n",
      "     zte3_1553815870582\n",
      "     zte4_1553815991365\n",
      "     zte5_1553816224366\n",
      "     zte6_1553816699503\n",
      "-------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print('---------------- List available datasets ---------------------')\n",
    "try:\n",
    "    api_response = api_instance.get_list_datasets()\n",
    "except ApiException as e:\n",
    "    print(\"Exception when calling DatasetsApi->get_list_datasets: %s\\n\" % e)\n",
    "\n",
    "list_datasets = api_response.result\n",
    "\n",
    "print(len(list_datasets), 'datasets in the database:')\n",
    "for ds in list_datasets:\n",
    "    print('    ', ds)\n",
    "\n",
    "    \n",
    "print('-------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get dataset information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------- Get dataset information -------------------\n",
      "Information about dataset dataset_test\n",
      "    Language: en\n",
      "    Number of documents: 9\n",
      "    Time range: 2019-03-28 23:11:21 to 2019-03-28 23:52:14\n",
      "-------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print('--------------- Get dataset information -------------------')\n",
    "dataset = 'dataset_test' # str | Dataset name.\n",
    "query = '' # str | Fulltext query, using mysql MATCH boolean query format. (optional)\n",
    "metadata_selection = '' # str | json object of {\\\"metadata_field\\\":[\\\"selected_values\\\"]} (optional)\n",
    "time_period = '' # str | Time period selection (optional)\n",
    "period_start = \"\" # str | Start date for the period to analyze within the dataset. Format: \"YYYY-MM-DD HH:MM:SS\"\n",
    "period_end = \"\" # str | End date for the period to analyze within the dataset. Format: \"YYYY-MM-DD HH:MM:SS\"\n",
    "\n",
    "try:\n",
    "    payload = nucleus_api.DatasetInfo(dataset=dataset, \n",
    "                                    query=query, \n",
    "                                    metadata_selection=metadata_selection, \n",
    "                                    time_period=time_period)\n",
    "    api_response = api_instance.post_dataset_info(payload)\n",
    "except ApiException as e:\n",
    "    print(\"Exception when calling DatasetsApi->post_dataset_info: %s\\n\" % e)\n",
    "\n",
    "print('Information about dataset', dataset)\n",
    "print('    Language:', api_response.result.detected_language)\n",
    "print('    Number of documents:', api_response.result.num_documents)\n",
    "print('    Time range:', datetime.datetime.fromtimestamp(float(api_response.result.time_range[0])),\n",
    "             'to', datetime.datetime.fromtimestamp(float(api_response.result.time_range[1])))\n",
    "\n",
    "print('-------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tag documents in a dataset with a provided list of entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('---------------- Tag dataset ------------------------')\n",
    "\n",
    "try:\n",
    "    payload = nucleus_api.DatasetTaggingModel(dataset='dataset_test', \n",
    "                                        query='Trump OR Clinton', \n",
    "                                        metadata_selection='', \n",
    "                                        time_period='')\n",
    "    api_response = api_instance.post_dataset_tagging(payload)\n",
    "except ApiException as e:\n",
    "    print(\"Exception when calling DatasetsApi->post_dataset_tagging: %s\\n\" % e)\n",
    "    \n",
    "print('Information about dataset', dataset)\n",
    "print('    Entity Tagged:', api_response.result.entity_tagged)\n",
    "print('    Docids tagged with Entity:', api_response.result.docids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#print('--------------------- Delete document -----------------------')\n",
    "#dataset = 'dataset_test'\n",
    "\n",
    "#docid = '1'\n",
    "#payload = nucleus_api.Deletedocumentmodel(dataset=dataset,\n",
    "#                                          docid=docid) # Deletedocumentmodel | \n",
    "\n",
    "#try:\n",
    "#    api_response = api_instance.post_delete_document(payload)\n",
    "#except ApiException as e:\n",
    "#    print(\"Exception when calling DatasetsApi->post_delete_document: %s\\n\" % e)\n",
    "\n",
    "\n",
    "#print('Document', docid, 'from dataset', dataset, 'has been deleted.')\n",
    "## print(api_response)     # raw API response\n",
    "#print('-------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------- Delete dataset ------------------------\n",
      "Dataset {'job_id': '242812', 'result': {'result': 'Dataset deleted'}} has been deleted\n",
      "api_response= {'job_id': None,\n",
      " 'result': ['aaaaa',\n",
      "            'crash0',\n",
      "            'jim_wu_sumup_ai_c1_1551662645858',\n",
      "            'jim_wu_sumup_ai_chinese_report_1552024272731',\n",
      "            'jim_wu_sumup_ai_fed_1547146082933',\n",
      "            'jim_wu_sumup_ai_real_estate_1549815645846',\n",
      "            'jim_wu_sumup_ai_t2_1550975321013',\n",
      "            'jim_wu_sumup_ai_test_1549816166057',\n",
      "            'jim_wu_sumup_ai_test_1550080405584',\n",
      "            'jim_wu_sumup_ai_txt_encode_1546896541648',\n",
      "            'jim_wu_sumup_ai_wuxi_sogou_1548286213377',\n",
      "            'jim_wu_sumup_aimit_tech_review1545314792621',\n",
      "            'trump_tweets',\n",
      "            'trump_tweets_full',\n",
      "            'zte_1553815688159',\n",
      "            'zte3_1553815870582',\n",
      "            'zte4_1553815991365',\n",
      "            'zte5_1553816224366',\n",
      "            'zte6_1553816699503']}\n",
      "-------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print('--------------------- Delete dataset ------------------------')\n",
    "\n",
    "dataset = 'dataset_test'  \n",
    "payload = nucleus_api.Deletedatasetmodel(dataset=dataset) # Deletedatasetmodel | \n",
    "\n",
    "\n",
    "try:\n",
    "    api_response = api_instance.post_delete_dataset(payload)\n",
    "    print('Dataset', api_response, 'has been deleted')\n",
    "except ApiException as e:\n",
    "    print(\"Exception when calling DatasetsApi->post_delete_dataset: %s\\n\" % e)\n",
    "    \n",
    "# List datasets again to check if the specified dataset has been deleted\n",
    "try:\n",
    "    api_response = api_instance.get_list_datasets()\n",
    "    print('api_response=', api_response)\n",
    "except ApiException as e:\n",
    "    print(\"Exception when calling DatasetsApi->get_list_datasets: %s\\n\" % e)\n",
    "    \n",
    "print('-------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get list of topics from dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------- Get list of topics from dataset --------------\n",
      "Topic 1 keywords:\n",
      "    Keywords: illegal aliens;united permission;permission illegal;enter united;aliens treated;aliens illegal;treated entered;entered legally\n",
      "    Keyword weights: 0.12414163288726007;0.12414163288726007;0.06550428489458975;0.06550428489458975;0.12414163288726007;0.24828326577452015;0.12414163288726007;0.12414163288726007\n",
      "    Strength: 0.12742688789427106\n",
      "    Document IDs: 3397215194896514820 4825367511331474696\n",
      "    Document exposures: 0.5276908390973678 0.4723091609026322\n",
      "---------------\n",
      "Topic 2 keywords:\n",
      "    Keywords: america great;statement america;andrew cuomo;great great;hillary clinton;worse hightax;deplorables statement;clinton deplorables\n",
      "    Keyword weights: 0.11278538882812676;0.16510769827619076;0.14793575224541514;0.16335112934755439;0.13469233979388007;0.09204256383627767;0.09204256383627767;0.09204256383627767\n",
      "    Strength: 0.14146142741122336\n",
      "    Document IDs: 776902852041351634 2205902445999073018 2953120351605742471 3545423942726121399 3917087015415381110 5864841412738683134 14722230792170818214\n",
      "    Document exposures: 0.03909377948080913 0.11260347191672704 0.15952913771957106 0.18111117830485382 0.0725175227750597 0.3910411784443268 0.04410373135865229\n",
      "---------------\n",
      "Topic 3 keywords:\n",
      "    Keywords: peter strzok;fired fbi;agent peter;fbi agent;strzok fraud;rigged investigation;investigation started;fraud rigged\n",
      "    Keyword weights: 0.17978137366134256;0.13599273038925175;0.17978137366134256;0.17978137366134256;0.08116578715668014;0.08116578715668014;0.08116578715668014;0.08116578715668014\n",
      "    Strength: 0.1192995092693477\n",
      "    Document IDs: 12975889410889565542 13560383080821693213 13941797014944461845\n",
      "    Document exposures: 0.31823472648469103 0.26307203314084276 0.41869324037446626\n",
      "---------------\n",
      "Topic 4 keywords:\n",
      "    Keywords: bruce ohr;ohr justice;justice department;christopher steele;steele dirt;dirt trump;department believe;accused helping\n",
      "    Keyword weights: 0.12130435792971807;0.2586690062769517;0.08788885097706696;0.08788885097706696;0.08788885097706696;0.08788885097706696;0.13423561594253125;0.13423561594253125\n",
      "    Strength: 0.15846709449896385\n",
      "    Document IDs: 1665055058996720522 4465556743861629171 8093533503511286363 11445006349852095298 12936417737022695482 12975889410889565542\n",
      "    Document exposures: 0.12739333828608393 0.30605743993335555 0.08976759099029641 0.13712226022228974 0.16292552285581 0.1767338477121643\n",
      "---------------\n",
      "Topic 5 keywords:\n",
      "    Keywords: witch hunt;donald trump;rigged witch;frame donald;unfortunate situation;trump rigged;situation decided;decided frame\n",
      "    Keyword weights: 0.17897098332839637;0.1789709833283964;0.12680989466950424;0.09710956100104969;0.09710956100104969;0.09710956100104969;0.12680989466950424;0.09710956100104969\n",
      "    Strength: 0.1446553402288602\n",
      "    Document IDs: 221453750667218444 722130111977512250 1293348775788793788 3176133214934205196 5217366909427623007 5821020073909755150 13560383080821693213 14722230792170818214\n",
      "    Document exposures: 0.05613012034502661 0.05434775532922656 0.11198837925071124 0.054347755329226546 0.4294513626831707 0.05810019433776668 0.05810019433776669 0.1775342383871049\n",
      "---------------\n",
      "Topic 6 keywords:\n",
      "    Keywords: trump campaign;lou dobbs;evidence collusion;dobbs special;democrats evidence;conflicts angry;campaign russia;angry democrats\n",
      "    Keyword weights: 0.14009919252367095;0.12284297249661844;0.12284297249661844;0.12284297249661844;0.12284297249661844;0.12284297249661844;0.12284297249661844;0.12284297249661844\n",
      "    Strength: 0.12282877388130622\n",
      "    Document IDs: 3325720912382988533 4746121785136787662 7781278670982937886 13801474846969815614\n",
      "    Document exposures: 0.045978553185804925 0.45345372486784763 0.04711399707849979 0.45345372486784763\n",
      "---------------\n",
      "Topic 7 keywords:\n",
      "    Keywords: fake news;news media;wacky omarosa;overtime wacky;omarosa modern;lowlife omarosa;form communication;communication fake\n",
      "    Keyword weights: 0.18183054344146596;0.14263218300043012;0.10766965205652194;0.10766965205652194;0.10766965205652194;0.1371890132754942;0.10766965205652194;0.10766965205652194\n",
      "    Strength: 0.10571522912763984\n",
      "    Document IDs: 950604085993420810 4767189974744133712 6138110761129291407 7014079786619530089 9082401855629236607 14304543781752005996\n",
      "    Document exposures: 0.2343418906883944 0.41698889470306866 0.09908420947209089 0.09286176954564208 0.08090191825788882 0.0758213173329152\n",
      "---------------\n",
      "Topic 8 keywords:\n",
      "    Keywords: john brennan;cia director;tuckercarlson speaking;speaking john;limited intellectually;intellectually cia;good question;director good\n",
      "    Keyword weights: 0.1690198491366519;0.11353184912659307;0.14978905610378965;0.11353184912659307;0.11353184912659307;0.11353184912659307;0.11353184912659307;0.11353184912659307\n",
      "    Strength: 0.0801457376883877\n",
      "    Document IDs: 3176133214934205196 7805744376908173076 14355443980237980691\n",
      "    Document exposures: 0.10783940285681638 0.6820806640598782 0.21007993308330533\n",
      "---------------\n",
      "-------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print('------------- Get list of topics from dataset --------------')\n",
    "dataset = 'trump_tweets'\n",
    "#query = '(\"Trump\" OR \"president\")' # str | Fulltext query, using mysql MATCH boolean query format. Example, (\\\"word1\\\" OR \\\"word2\\\") AND (\\\"word3\\\" OR \\\"word4\\\") (optional)\n",
    "query = ''\n",
    "custom_stop_words = [\"real\",\"hillary\"] # str | List of stop words. (optional)\n",
    "num_topics = 8 # int | Number of topics to be extracted from the dataset. (optional) (default to 8)\n",
    "metadata_selection = \"\" # dict | JSON object specifying metadata-based queries on the dataset, of type {\"metadata_field\": \"selected_values\"} (optional)\n",
    "time_period = \"\"     # str | Time period selection. Choices: [\"1M\",\"3M\",\"6M\",\"12M\",\"3Y\",\"5Y\",\"\"] (optional)\n",
    "\n",
    "try:\n",
    "    payload = nucleus_api.Topics(dataset=dataset,                                \n",
    "                                query=query,                   \n",
    "                                custom_stop_words=custom_stop_words,     \n",
    "                                num_topics=num_topics,\n",
    "                                metadata_selection=metadata_selection,\n",
    "                                time_period=time_period)\n",
    "    api_response = api_instance.post_topic_api(payload)        \n",
    "except ApiException as e:\n",
    "    print(\"Exception when calling TopicsApi->post_topic_api: %s\\n\" % e)\n",
    "    \n",
    "for i, res in enumerate(api_response.result):\n",
    "    print('Topic', i, 'keywords:')\n",
    "    print('    Keywords:', res.topic)\n",
    "    keywords_weight_str = \";\".join(str(x) for x in res.keywords_weight)\n",
    "    print('    Keyword weights:', keywords_weight_str)\n",
    "    print('    Strength:', res.strength)\n",
    "    doc_topic_exposure_sel = []  # list of non-zero doc_topic_exposure\n",
    "    doc_id_sel = []        # list of doc ids matching doc_topic_exposure_sel\n",
    "    for j in range(len(res.doc_topic_exposure)):\n",
    "        doc_topic_exp = float(res.doc_topic_exposure[j])\n",
    "        if doc_topic_exp != 0:\n",
    "            doc_topic_exposure_sel.append(doc_topic_exp)\n",
    "            doc_id_sel.append(res.doc_id[j])\n",
    "    \n",
    "    doc_id_sel_str = ' '.join(str(x) for x in doc_id_sel)\n",
    "    doc_topic_exposure_sel_str = ' '.join(str(x) for x in doc_topic_exposure_sel)\n",
    "    print('    Document IDs:', doc_id_sel_str)\n",
    "    print('    Document exposures:', doc_topic_exposure_sel_str)\n",
    "\n",
    "    print('---------------')\n",
    "    \n",
    "print('-------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get list of topics from dataset with a time range selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('------------- Get list of topics from dataset --------------')\n",
    "dataset = 'trump_tweets'\n",
    "#query = '(\"Trump\" OR \"president\")' # str | Fulltext query, using mysql MATCH boolean query format. Example, (\\\"word1\\\" OR \\\"word2\\\") AND (\\\"word3\\\" OR \\\"word4\\\") (optional)\n",
    "query = ''\n",
    "custom_stop_words = [\"real\",\"hillary\"] # str | List of stop words. (optional)\n",
    "num_topics = 8 # int | Number of topics to be extracted from the dataset. (optional) (default to 8)\n",
    "metadata_selection = \"\" # dict | JSON object specifying metadata-based queries on the dataset, of type {\"metadata_field\": \"selected_values\"} (optional)\n",
    "period_start = \"2016-10-15 04:30:00\" # str | Start date for the period to analyze within the dataset. Format: \"YYYY-MM-DD HH:MM:SS\"\n",
    "period_end = \"2019-01-01 12:00:05\" # str | End date for the period to analyze within the dataset. Format: \"YYYY-MM-DD HH:MM:SS\"\n",
    "\n",
    "try:\n",
    "    payload = nucleus_api.Topics(dataset=dataset,                                \n",
    "                                query=query,                   \n",
    "                                custom_stop_words=custom_stop_words,     \n",
    "                                num_topics=num_topics,\n",
    "                                metadata_selection=metadata_selection,\n",
    "                                period_start=period_start,\n",
    "                                period_end=period_end)\n",
    "    api_response = api_instance.post_topic_api(payload)        \n",
    "except ApiException as e:\n",
    "    print(\"Exception when calling TopicsApi->post_topic_api: %s\\n\" % e)\n",
    "    \n",
    "for i, res in enumerate(api_response.result):\n",
    "    print('Topic', i, 'keywords:')\n",
    "    print('    Keywords:', res.topic)\n",
    "    keywords_weight_str = \";\".join(str(x) for x in res.keywords_weight)\n",
    "    print('    Keyword weights:', keywords_weight_str)\n",
    "    print('    Strength:', res.strength)\n",
    "    doc_topic_exposure_sel = []  # list of non-zero doc_topic_exposure\n",
    "    doc_id_sel = []        # list of doc ids matching doc_topic_exposure_sel\n",
    "    for j in range(len(res.doc_topic_exposure)):\n",
    "        doc_topic_exp = float(res.doc_topic_exposure[j])\n",
    "        if doc_topic_exp != 0:\n",
    "            doc_topic_exposure_sel.append(doc_topic_exp)\n",
    "            doc_id_sel.append(res.doc_id[j])\n",
    "    \n",
    "    doc_id_sel_str = ' '.join(str(x) for x in doc_id_sel)\n",
    "    doc_topic_exposure_sel_str = ' '.join(str(x) for x in doc_topic_exposure_sel)\n",
    "    print('    Document IDs:', doc_id_sel_str)\n",
    "    print('    Document exposures:', doc_topic_exposure_sel_str)\n",
    "\n",
    "    print('---------------')\n",
    "    \n",
    "print('-------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get list of topics from dataset with a metadata selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('------------- Get list of topics from dataset --------------')\n",
    "dataset = 'trump_tweets'\n",
    "#query = '(\"Trump\" OR \"president\")' # str | Fulltext query, using mysql MATCH boolean query format. Example, (\\\"word1\\\" OR \\\"word2\\\") AND (\\\"word3\\\" OR \\\"word4\\\") (optional)\n",
    "query = ''\n",
    "custom_stop_words = [\"real\",\"hillary\"] # str | List of stop words. (optional)\n",
    "num_topics = 8 # int | Number of topics to be extracted from the dataset. (optional) (default to 8)\n",
    "metadata_selection = {\"author\": \"D_Trump16\"} # dict | JSON object specifying metadata-based queries on the dataset, of type {\"metadata_field\": \"selected_values\"} (optional)\n",
    "\n",
    "try:\n",
    "    payload = nucleus_api.Topics(dataset=dataset,                                \n",
    "                                query=query,                   \n",
    "                                custom_stop_words=custom_stop_words,     \n",
    "                                num_topics=num_topics,\n",
    "                                metadata_selection=metadata_selection)\n",
    "    api_response = api_instance.post_topic_api(payload)        \n",
    "except ApiException as e:\n",
    "    print(\"Exception when calling TopicsApi->post_topic_api: %s\\n\" % e)\n",
    "    \n",
    "for i, res in enumerate(api_response.result):\n",
    "    print('Topic', i, 'keywords:')\n",
    "    print('    Keywords:', res.topic)\n",
    "    keywords_weight_str = \";\".join(str(x) for x in res.keywords_weight)\n",
    "    print('    Keyword weights:', keywords_weight_str)\n",
    "    print('    Strength:', res.strength)\n",
    "    doc_topic_exposure_sel = []  # list of non-zero doc_topic_exposure\n",
    "    doc_id_sel = []        # list of doc ids matching doc_topic_exposure_sel\n",
    "    for j in range(len(res.doc_topic_exposure)):\n",
    "        doc_topic_exp = float(res.doc_topic_exposure[j])\n",
    "        if doc_topic_exp != 0:\n",
    "            doc_topic_exposure_sel.append(doc_topic_exp)\n",
    "            doc_id_sel.append(res.doc_id[j])\n",
    "    \n",
    "    doc_id_sel_str = ' '.join(str(x) for x in doc_id_sel)\n",
    "    doc_topic_exposure_sel_str = ' '.join(str(x) for x in doc_topic_exposure_sel)\n",
    "    print('    Document IDs:', doc_id_sel_str)\n",
    "    print('    Document exposures:', doc_topic_exposure_sel_str)\n",
    "\n",
    "    print('---------------')\n",
    "    \n",
    "print('-------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get topic summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('------------------- Get topic summary -----------------------')\n",
    "dataset = 'trump_tweets' # str | Dataset name.\n",
    "#query = '(\"Trump\" OR \"president\")' # str | Fulltext query, using mysql MATCH boolean query format. Example, (\\\"word1\\\" OR \\\"word2\\\") AND (\\\"word3\\\" OR \\\"word4\\\") (optional)\n",
    "query = ''\n",
    "custom_stop_words = [\"real\",\"hillary\"] # str | List of stop words. (optional)\n",
    "num_topics = 8 # int | Number of topics to be extracted from the dataset. (optional) (default to 8)\n",
    "num_keywords = 8 # int | Number of keywords per topic that is extracted from the dataset. (optional) (default to 8)\n",
    "summary_length = 6 # int | The maximum number of bullet points a user wants to see in each topic summary. (optional) (default to 6)\n",
    "context_amount = 0 # int | The number of sentences surrounding key summary sentences in the documents that they come from. (optional) (default to 0)\n",
    "num_docs = 20 # int | The maximum number of key documents to use for summarization. (optional) (default to 20)\n",
    "excluded_docs = '' # str | List of document IDs that should be excluded from the analysis. Example, [\"docid1\", \"docid2\", ..., \"docidN\"]  (optional)\n",
    "\n",
    "metadata_selection =\"\" # dict | JSON object specifying metadata-based queries on the dataset, of type {\"metadata_field\": \"selected_values\"} (optional)\n",
    "time_period = \"\"     # str | Time period selection. Choices: [\"1M\",\"3M\",\"6M\",\"12M\",\"3Y\",\"5Y\",\"\"]  (optional)\n",
    "period_start = \"\" # str | Start date for the period to analyze within the dataset. Format: \"YYYY-MM-DD HH:MM:SS\"\n",
    "period_end = \"\" # str | End date for the period to analyze within the dataset. Format: \"YYYY-MM-DD HH:MM:SS\"\n",
    "api_response = None\n",
    "\n",
    "try:\n",
    "    payload = nucleus_api.TopicSummaryModel\t(\n",
    "        dataset=dataset, \n",
    "        query=query,\n",
    "        custom_stop_words=custom_stop_words, \n",
    "        num_topics=num_topics, \n",
    "        num_keywords=num_keywords,\n",
    "        metadata_selection=metadata_selection,\n",
    "        summary_length=summary_length, \n",
    "        context_amount=context_amount, \n",
    "        num_docs=num_docs)\n",
    "    api_response = api_instance.post_topic_summary_api(payload)        \n",
    "except ApiException as e:\n",
    "    print(\"Exception when calling TopicsApi->post_topic_summary_api: %s\\n\" % e)\n",
    "\n",
    "if api_response != None:\n",
    "    for i, res in enumerate(api_response.result):\n",
    "        print('Topic', i, 'summary:')\n",
    "        print('    Keywords:', res.topic)\n",
    "        for j in range(len(res.summary)):\n",
    "            print(res.summary[j])\n",
    "            print('    Document ID:', res.summary[j].sourceid)\n",
    "            print('        Title:', res.summary[j].title)\n",
    "            print('        Sentences:', res.summary[j].sentences)\n",
    "            print('        Author:', res.summary[j].attribute['author'])\n",
    "            print('        Time:', datetime.datetime.fromtimestamp(float(res.summary[j].attribute['time'])))\n",
    "        \n",
    "        print('---------------')\n",
    "    \n",
    "print('-------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get topic sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('---------------- Get topic sentiment ------------------------')\n",
    "dataset = 'trump_tweets' # str | Dataset name.\n",
    "#query = '(\"Trump\" OR \"president\")' # str | Fulltext query, using mysql MATCH boolean query format. Example, (\\\"word1\\\" OR \\\"word2\\\") AND (\\\"word3\\\" OR \\\"word4\\\") (optional)\n",
    "query = ''\n",
    "custom_stop_words = [\"real\",\"hillary\"] # str | List of stop words. (optional)\n",
    "num_topics = 8 # int | Number of topics to be extracted from the dataset. (optional) (default to 8)\n",
    "num_keywords = 8 # int | Number of keywords per topic that is extracted from the dataset. (optional) (default to 8)\n",
    "excluded_docs = '' # str | List of document IDs that should be excluded from the analysis. Example, [\"docid1\", \"docid2\", ..., \"docidN\"]  (optional)\n",
    "custom_dict_file = {\"great\": 1.0, \"awful\": -1.0, \"clinton\":-1.0, \"trump\":1.0} # file | Custom sentiment dictionary JSON file. Example, {\"field1\": value1, ..., \"fieldN\": valueN} (optional)\n",
    "\n",
    "metadata_selection =\"\" # dict | JSON object specifying metadata-based queries on the dataset, of type {\"metadata_field\": \"selected_values\"} (optional)\n",
    "time_period = \"\"     # str | Time period selection. Choices: [\"1M\",\"3M\",\"6M\",\"12M\",\"3Y\",\"5Y\",\"\"] (optional)\n",
    "period_start = \"\" # str | Start date for the period to analyze within the dataset. Format: \"YYYY-MM-DD HH:MM:SS\"\n",
    "period_end = \"\" # str | End date for the period to analyze within the dataset. Format: \"YYYY-MM-DD HH:MM:SS\"\n",
    "\n",
    "try:\n",
    "    payload = nucleus_api.TopicSentimentModel(\n",
    "        dataset=dataset, \n",
    "        query=query, \n",
    "        custom_stop_words=custom_stop_words, \n",
    "        num_topics=num_topics, \n",
    "        num_keywords=num_keywords,\n",
    "        custom_dict_file=custom_dict_file)\n",
    "    api_response = api_instance.post_topic_sentiment_api(payload)\n",
    "    \n",
    "except ApiException as e:\n",
    "    print(\"Exception when calling TopicsApi->post_topic_sentiment_api: %s\\n\" % e)\n",
    "\n",
    "for i, res in enumerate(api_response.result):\n",
    "    print('Topic', i, 'sentiment:')\n",
    "    print('    Keywords:', res.topic)\n",
    "    print('    Sentiment:', res.sentiment)\n",
    "    print('    Strength:', res.strength)\n",
    "    \n",
    "    doc_id_str = ' '.join(str(x) for x in res.doc_id)\n",
    "    doc_sentiment_str = ' '.join(str(x) for x in res.doc_sentiment)\n",
    "    doc_score_str = ' '.join(str(x) for x in res.doc_score)\n",
    "    print('    Document IDs:', doc_id_str)\n",
    "    print('    Document Sentiments:', doc_sentiment_str)\n",
    "    print('    Document Scores:', doc_score_str)\n",
    "    \n",
    "    print('---------------')\n",
    "    \n",
    "print('-------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get topic consensus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('---------------- Get topic consensus ------------------------')\n",
    "dataset = 'trump_tweets' # str | Dataset name.\n",
    "query = '' # str | Fulltext query, using mysql MATCH boolean query format. Example, (\\\"word1\\\" OR \\\"word2\\\") AND (\\\"word3\\\" OR \\\"word4\\\") (optional)\n",
    "custom_stop_words = [\"real\",\"hillary\"] # str | List of stop words. (optional)\n",
    "num_topics = 8 # int | Number of topics to be extracted from the dataset. (optional) (default to 8)\n",
    "num_keywords = 8 # int | Number of keywords per topic that is extracted from the dataset. (optional) (default to 8)\n",
    "excluded_docs = [''] # str | List of document IDs that should be excluded from the analysis. Example, [\"docid1\", \"docid2\", ..., \"docidN\"]  (optional)\n",
    "custom_dict_file = {\"great\": 1.0, \"awful\": -1.0, \"clinton\":-1.0, \"trump\":1.0} # file | Custom sentiment dictionary JSON file. Example, {\"field1\": value1, ..., \"fieldN\": valueN} (optional)\n",
    "\n",
    "metadata_selection =\"\" # dict | JSON object specifying metadata-based queries on the dataset, of type {\"metadata_field\": \"selected_values\"} (optional)\n",
    "time_period = \"\"     # str | Time period selection. Choices: [\"1M\",\"3M\",\"6M\",\"12M\",\"3Y\",\"5Y\",\"\"] (optional)\n",
    "period_start = \"\" # str | Start date for the period to analyze within the dataset. Format: \"YYYY-MM-DD HH:MM:SS\"\n",
    "period_end = \"\" # str | End date for the period to analyze within the dataset. Format: \"YYYY-MM-DD HH:MM:SS\"\n",
    "\n",
    "try:\n",
    "    payload = nucleus_api.TopicConsensusModel(\n",
    "        dataset=dataset, \n",
    "        query=query, \n",
    "        custom_stop_words=custom_stop_words, \n",
    "        num_topics=num_topics, \n",
    "        num_keywords=num_keywords,\n",
    "        custom_dict_file=custom_dict_file)\n",
    "    api_response = api_instance.post_topic_consensus_api(payload)\n",
    "except ApiException as e:\n",
    "    print(\"Exception when calling TopicsApi->post_topic_consensus_api: %s\\n\" % e)\n",
    "    \n",
    "for i, res in enumerate(api_response.result):\n",
    "    print('Topic', i, 'consensus:')\n",
    "    print('    Keywords:', res.topic)\n",
    "    print('    Consensus:', res.consensus)\n",
    "    print('    Strength:', res.strength)\n",
    "    \n",
    "    print('---------------')\n",
    "    \n",
    "print('-------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get topic historical analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('------------ Get topic historical analysis ----------------')\n",
    "\n",
    "dataset = 'trump_tweets'   # str | Dataset name.\n",
    "update_period = 'd' # str | Frequency at which the historical anlaysis is performed. choices=[\"d\",\"m\",\"H\",\"M\"] (default to d)\n",
    "query = '' # str | Fulltext query, using mysql MATCH boolean query format. Example, (\\\"word1\\\" OR \\\"word2\\\") AND (\\\"word3\\\" OR \\\"word4\\\") (optional)\n",
    "custom_stop_words = [\"real\",\"hillary\"] # str | List of stop words (optional)\n",
    "num_topics = 8 # int | Number of topics to be extracted from the dataset. (optional) (default to 8)\n",
    "num_keywords = 8 # int | Number of keywords per topic that is extracted from the dataset. (optional) (default to 8)\n",
    "inc_step = 1 # int | Number of increments of the udpate period in between two historical computations. (optional) (default to 1)\n",
    "excluded_docs = [''] # str | List of document IDs that should be excluded from the analysis. Example, [\"docid1\", \"docid2\", ..., \"docidN\"]  (optional)\n",
    "custom_dict_file = {} # file | Custom sentiment dictionary JSON file. Example, {\"field1\": value1, ..., \"fieldN\": valueN} (optional)\n",
    "\n",
    "metadata_selection =\"\" # dict | JSON object specifying metadata-based queries on the dataset, of type {\"metadata_field\": \"selected_values\"} (optional)\n",
    "time_period = \"12M\"     # str | Time period selection. Choices: [\"1M\",\"3M\",\"6M\",\"12M\",\"3Y\",\"5Y\",\"\"] (optional)\n",
    "period_start = \"\" # str | Start date for the period to analyze within the dataset. Format: \"YYYY-MM-DD HH:MM:SS\"\n",
    "period_end = \"\" # str | End date for the period to analyze within the dataset. Format: \"YYYY-MM-DD HH:MM:SS\"\n",
    "api_response = None\n",
    "try:\n",
    "    payload = nucleus_api.TopicHistoryModel(\n",
    "        dataset=dataset, \n",
    "        time_period=time_period, \n",
    "        update_period=update_period, \n",
    "        query=query, \n",
    "        custom_stop_words=custom_stop_words, \n",
    "        num_topics=num_topics, \n",
    "        num_keywords=num_keywords, \n",
    "        metadata_selection=metadata_selection, \n",
    "        inc_step=inc_step, \n",
    "        excluded_docs=excluded_docs,\n",
    "        custom_dict_file=custom_dict_file)\n",
    "    api_response = api_instance.post_topic_historical_analysis_api(payload)\n",
    "except ApiException as e:\n",
    "    print(\"Exception when calling TopicsApi->post_topic_historical_analysis_api: %s\\n\" % e)\n",
    "\n",
    "if api_response != None:\n",
    "    results = api_response.result\n",
    "\n",
    "    # chart the historical metrics when running in Jupyter Notebook\n",
    "    if running_notebook:\n",
    "        print('Plotting historical metrics data...')\n",
    "        historical_metrics = []\n",
    "        for res in results:\n",
    "            # conctruct a list of historical metrics dictionaries for charting\n",
    "            historical_metrics.append({\n",
    "                'topic'    : res.topic,\n",
    "                'time_stamps' : np.array(res.time_stamps),\n",
    "                'strength' : np.array(res.strength, dtype=np.float32),\n",
    "                'consensus': np.array(res.consensus, dtype=np.float32), \n",
    "                'sentiment': np.array(res.sentiment, dtype=np.float32)})\n",
    "\n",
    "        selected_topics = range(len(historical_metrics)) \n",
    "        nucleus_helper.topic_charts_historical(historical_metrics, selected_topics, True)\n",
    "    else:\n",
    "        print('Printing historical metrics data...')\n",
    "        print('NOTE: historical metrics data can be plotted when running the example in Jupyter Notebook')\n",
    "\n",
    "        for i, res in enumerate(results):\n",
    "            print('Topic', i, res.topic)\n",
    "            print('    Timestamps:', res.time_stamps)\n",
    "            print('    Strength:', res.strength)\n",
    "            print('    Consensus:', res.consensus)\n",
    "            print('    Sentiment:', res.sentiment)\n",
    "            print('----------------')\n",
    "\n",
    "print('-------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get author connectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('----------------- Get author connectivity -------------------')\n",
    "dataset = dataset # str | Dataset name.\n",
    "target_author = 'D_Trump16' # str | Name of the author to be analyzed.\n",
    "query = '' # str | Fulltext query, using mysql MATCH boolean query format. Subject covered by the author, on which to focus the analysis of connectivity. Example, (\\\"word1\\\" OR \\\"word2\\\") AND (\\\"word3\\\" OR \\\"word4\\\") (optional)\n",
    "custom_stop_words = [\"real\",\"hillary\"] # str | List of words possibly used by the target author that are considered not information-bearing. (optional)\n",
    "excluded_docs = [''] # str | List of document IDs that should be excluded from the analysis. Example, [\"docid1\", \"docid2\", ..., \"docidN\"]  (optional)\n",
    "\n",
    "metadata_selection =\"\" # dict | JSON object specifying metadata-based queries on the dataset, of type {\"metadata_field\": \"selected_values\"} (optional)\n",
    "time_period = \"12M\"     # str | Time period selection. Choices: [\"1M\",\"3M\",\"6M\",\"12M\",\"3Y\",\"5Y\",\"\"] (optional)\n",
    "period_start = \"\" # str | Start date for the period to analyze within the dataset. Format: \"YYYY-MM-DD HH:MM:SS\"\n",
    "period_end = \"\" # str | End date for the period to analyze within the dataset. Format: \"YYYY-MM-DD HH:MM:SS\"\n",
    "\n",
    "try:\n",
    "    payload = nucleus_api.AuthorConnection(dataset=dataset, \n",
    "                                            target_author=target_author, \n",
    "                                            query=query, \n",
    "                                            custom_stop_words=custom_stop_words, \n",
    "                                            time_period=time_period, \n",
    "                                            metadata_selection=metadata_selection, \n",
    "                                            excluded_docs=excluded_docs)\n",
    "    api_response = api_instance.post_author_connectivity_api(payload)    \n",
    "except ApiException as e:\n",
    "    print(\"Exception when calling TopicsApi->post_author_connectivity_api: %s\\n\" % e)\n",
    "\n",
    "res = api_response.result\n",
    "print('Mainstream connections:')\n",
    "for mc in res.mainstream_connection:\n",
    "    print('    Topic:', mc.topic)\n",
    "    print('    Authors:', \" \".join(str(x) for x in mc.authors))\n",
    "    \n",
    "print('Niche connections:')\n",
    "for nc in res.niche_connection:\n",
    "    print('    Topic:', nc.topic)\n",
    "    print('    Authors:', \" \".join(str(x) for x in nc.authors))  \n",
    "    \n",
    "#pprint(api_response)   # raw API response\n",
    "print('-------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get topic delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('------------------- Get topic deltas -----------------------')\n",
    "dataset = 'trump_tweets' \n",
    "#dataset = dataset # str | Dataset name.\n",
    "#query = '(\"Trump\" OR \"president\")' # str | Fulltext query, using mysql MATCH boolean query format. Example, (\\\"word1\\\" OR \\\"word2\\\") AND (\\\"word3\\\" OR \\\"word4\\\") (optional)\n",
    "query = ''\n",
    "custom_stop_words = [\"\"] # str | List of stop words. (optional)\n",
    "num_topics = 8 # int | Number of topics to be extracted from the dataset. (optional) (default to 8)\n",
    "num_keywords = 8 # int | Number of keywords per topic that is extracted from the dataset. (optional) (default to 8)\n",
    "metadata_selection =\"\" # dict | JSON object specifying metadata-based queries on the dataset, of type {\"metadata_field\": \"selected_values\"} (optional)\n",
    "period_0_start = '2018-08-12 00:00:00'\n",
    "period_0_end = '2018-08-15 13:00:00'\n",
    "period_1_start = '2018-08-16 00:00:00'\n",
    "period_1_end = '2018-08-19 00:00:00'\n",
    "excluded_docs = '' # str | List of document IDs that should be excluded from the analysis. Example, [\"docid1\", \"docid2\", ..., \"docidN\"]  (optional)\n",
    "\n",
    "try:\n",
    "    payload = nucleus_api.TopicDeltaModel(\n",
    "        dataset=dataset, \n",
    "        query=query, \n",
    "        custom_stop_words=custom_stop_words, \n",
    "        num_topics=num_topics, \n",
    "        num_keywords=num_keywords,\n",
    "        period_0_start=period_0_start,\n",
    "        period_0_end=period_0_end,\n",
    "        period_1_start=period_1_start,\n",
    "        period_1_end=period_1_end,\n",
    "        metadata_selection=metadata_selection)\n",
    "    api_response = api_instance.post_topic_delta_api(payload)        \n",
    "except ApiException as e:\n",
    "    print(\"Exception when calling TopicsApi->post_topic_delta_api: %s\\n\" % e)\n",
    "\n",
    "for i, res in enumerate(api_response.result):\n",
    "    print('Topic', i, 'changes in exposure:')\n",
    "    print('    Keywords:', res.topic)\n",
    "    print('    Document ID:', res.doc_id_t0, res.doc_id_t1)\n",
    "    print('    Per Source Change in Exposure:', res.doc_topic_exposure_delta)\n",
    "    print('---------------')\n",
    "    \n",
    "print('-------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get document information without content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'trump_tweets' # str | Dataset name.\n",
    "\n",
    "# doc_titles, doc_ids, and metadata_selection below are filters to narrow down \n",
    "# documents to be retrieved.\n",
    "# The information of all documents will be retrived when no filters are provided.\n",
    "\n",
    "# doc_titles: list of strings\n",
    "# The titles of the documents to retrieve. Example: [\"title1\", \"title2\", ..., \"titleN\"]  (optional)\n",
    "# doc_titles = ['D_Trump2018_8_18_1_47']   \n",
    "doc_titles = []\n",
    "# doc_ids: list of strings\n",
    "# The docid of the documents to retrieve. Example: [\"docid1\", \"docid2\", ..., \"docidN\"]  (optional)\n",
    "# doc_ids = ['3397215194896514820', '776902852041351634']\n",
    "doc_ids = []\n",
    "\n",
    "# metadata_selection = {\"author\": \"D_Trump16\"} # dict | A selector off metadata. Example: {\"field\": \"value\"}  (optional)\n",
    "metadata_selection = ''\n",
    "\n",
    "try:\n",
    "    payload = nucleus_api.DocInfo(\n",
    "        dataset=dataset, \n",
    "        doc_titles=doc_titles, \n",
    "        doc_ids=doc_ids,\n",
    "        metadata_selection=metadata_selection)\n",
    "    api_response = api_instance.post_doc_info(payload)\n",
    "except ApiException as e:\n",
    "    print(\"Exception when calling post_doc_info: %s\\n\" % e)\n",
    "    \n",
    "for res in api_response.result:\n",
    "    print('Document ID:', res.sourceid)\n",
    "    print('    Title:', res.title)\n",
    "    print('    Author:', res.attribute['author'])\n",
    "    print('    Time:', datetime.datetime.fromtimestamp(float(res.attribute['time'])))\n",
    "\n",
    "    print('---------------')\n",
    "    \n",
    "print('-------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display document info with a metadata selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'trump_tweets' # str | Dataset name.\n",
    "metadata_selection = {\"author\": \"D_Trump16\"}      # dict | A selector off metadata. Example: {\"field\": \"value\"}  (optional)\n",
    "\n",
    "try:\n",
    "    payload = nucleus_api.DocInfo(dataset=dataset, metadata_selection=metadata_selection)\n",
    "    api_response = api_instance.post_doc_info(payload)\n",
    "    \n",
    "except ApiException as e:\n",
    "    print(\"Exception when calling DocumentsApi->post_doc_info_api: %s\\n\" % e)\n",
    "\n",
    "for res in api_response.result:\n",
    "    print('Document ID:', res.sourceid)\n",
    "    print('    Title:', res.title)\n",
    "    print('    Author:', res.attribute['author'])\n",
    "    print('    Time:', datetime.datetime.fromtimestamp(float(res.attribute['time'])))\n",
    "\n",
    "    print('---------------')\n",
    "\n",
    "print('-------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display document details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset = 'trump_tweets' # str | Dataset name.\n",
    "#doc_titles = ['D_Trump2018_8_18_1_47']   # str | The title of the documents to retrieve. Example: [\"title1\", \"title2\", ..., \"titleN\"]  (optional)\n",
    "doc_ids = ['776902852041351634']      # str | The docid of the documents to retrieve. Example: [\"docid1\", \"docid2\", ..., \"docidN\"]  (optional)\n",
    "\n",
    "try:\n",
    "    payload = nucleus_api.DocDisplay(dataset, doc_ids=doc_ids)\n",
    "    api_response = api_instance.post_doc_display(payload)\n",
    "    \n",
    "except ApiException as e:\n",
    "    print(\"Exception when calling DocumentsApi->post_doc_display_api: %s\\n\" % e)\n",
    "\n",
    "for res in api_response.result:\n",
    "    print('Document ID:', res.sourceid)\n",
    "    print('    Title:', res.title)\n",
    "    print('    Author:', res.attribute['author'])\n",
    "    print('    Time:', datetime.datetime.fromtimestamp(float(res.attribute['time'])))\n",
    "    print('    Content', res.content)\n",
    "\n",
    "    print('---------------')\n",
    "\n",
    "print('-------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display document details with a metadata selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'trump_tweets' # str | Dataset name.\n",
    "metadata_selection = {\"author\": \"D_Trump16\"}      # dict | A selector off metadata. Example: {\"field\": \"value\"}  (optional)\n",
    "\n",
    "try:\n",
    "    payload = nucleus_api.DocDisplay(dataset=dataset, metadata_selection=metadata_selection)\n",
    "    api_response = api_instance.post_doc_display(payload)\n",
    "    \n",
    "except ApiException as e:\n",
    "    print(\"Exception when calling DocumentsApi->post_doc_display_api: %s\\n\" % e)\n",
    "\n",
    "for res in api_response.result:\n",
    "    print('Document ID:', res.sourceid)\n",
    "    print('    Title:', res.title)\n",
    "    print('    Author:', res.attribute['author'])\n",
    "    print('    Time:', datetime.datetime.fromtimestamp(float(res.attribute['time'])))\n",
    "    print('    Content', res.content)\n",
    "\n",
    "    print('---------------')\n",
    "\n",
    "print('-------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get document recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('------------- Get document recommendations -----------------')\n",
    "\n",
    "dataset = 'trump_tweets' # str | Dataset name.\n",
    "#query = '(\"Trump\" OR \"president\")' # str | Fulltext query, using mysql MATCH boolean query format. Example, (\\\"word1\\\" OR \\\"word2\\\") AND (\\\"word3\\\" OR \\\"word4\\\") (optional)\n",
    "query = ''\n",
    "custom_stop_words = [\"real\",\"hillary\"] # str | List of stop words. (optional)\n",
    "num_topics = 8 # int | Number of topics to be extracted from the dataset. (optional) (default to 8)\n",
    "num_keywords = 8 # int | Number of keywords per topic that is extracted from the dataset. (optional) (default to 8)\n",
    "excluded_docs = '' # str | List of document IDs that should be excluded from the analysis. Example, [\"docid1\", \"docid2\", ..., \"docidN\"]  (optional)\n",
    "\n",
    "try:\n",
    "    payload = nucleus_api.DocumentRecommendModel(\n",
    "        dataset=dataset, \n",
    "        query=query, \n",
    "        custom_stop_words=custom_stop_words, \n",
    "        num_topics=num_topics, \n",
    "        num_keywords=num_keywords)\n",
    "    api_response = api_instance.post_doc_recommend_api(payload)\n",
    "except ApiException as e:\n",
    "    print(\"Exception when calling DocumentsApi->post_doc_recommend_api: %s\\n\" % e)\n",
    "    \n",
    "for i, res in enumerate(api_response.result):\n",
    "    print('Document recommendations for topic', i, ':')\n",
    "    print('    Keywords:', res.topic)\n",
    "\n",
    "    for j, doc in enumerate(res.recommendations):\n",
    "        print('    Recommendation', j, ':')\n",
    "        print('        Document ID:', doc.sourceid)\n",
    "        print('        Title:', doc.title)\n",
    "        print('        Attribute:', doc.attribute)\n",
    "        print('        Author:', doc.attribute['author'])\n",
    "        print('        Time:', datetime.datetime.fromtimestamp(float(doc.attribute['time'])))\n",
    "    \n",
    "    print('---------------')\n",
    "    \n",
    "print('-------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get document summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('------------------ Get document summary  --------------------')\n",
    "\n",
    "dataset = 'trump_tweets' # str | Dataset name.\n",
    "doc_title = 'D_Trump2018_8_17_14_10' # str | The title of the document to be summarized.\n",
    "custom_stop_words = [\"real\",\"hillary\"] # List of stop words. (optional)\n",
    "summary_length = 6 # int | The maximum number of bullet points a user wants to see in the document summary. (optional) (default to 6)\n",
    "context_amount = 0 # int | The number of sentences surrounding key summary sentences in the documents that they come from. (optional) (default to 0)\n",
    "short_sentence_length = 0 # int | The sentence length below which a sentence is excluded from summarization (optional) (default to 4)\n",
    "long_sentence_length = 40 # int | The sentence length beyond which a sentence is excluded from summarization (optional) (default to 40)\n",
    "\n",
    "try:\n",
    "    payload = nucleus_api.DocumentSummaryModel(\n",
    "        dataset=dataset, \n",
    "        doc_title=doc_title, \n",
    "        custom_stop_words=custom_stop_words, \n",
    "        summary_length=summary_length, \n",
    "        context_amount=context_amount,\n",
    "        short_sentence_length=short_sentence_length,\n",
    "        long_sentence_length=long_sentence_length)\n",
    "    api_response = api_instance.post_doc_summary_api(payload)\n",
    "    \n",
    "    print('Summary for', api_response.result.doc_title)\n",
    "    for sent in api_response.result.summary.sentences:\n",
    "        print('    *', sent)\n",
    "    \n",
    "except ApiException as e:\n",
    "    print(\"Exception when calling DocumentsApi->post_doc_summary_api: %s\\n\" % e)\n",
    "\n",
    "print('-------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarize file from URL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################################\n",
    "# file_params fields descriptions:  \n",
    "#   file_url              : string, the URL at which the file is stored (could be a S3 bucket address for instance)\n",
    "#   filename              : OPTIONAL string, filename saved on the server. also serves as the doc_title for summarization\n",
    "#   custom_stop_words     : OPTIONAL a string list, user-provided list of stopwords to be excluded from the content analysis leading to document summarization\n",
    "#                            [\"word1\", \"word2\", ...]. DEFAULT: empty\n",
    "#   summary_length        : OPTIONAL an integer, the maximum number of bullet points a user wants to see in the document summary. DEFAULT: 6\n",
    "#   context_amount        : OPTIONAL an integer, the number of sentences surrounding key summary sentences in the original document that a user wants to see in the document summary. DEFAULT: 0\n",
    "#   short_sentence_length : OPTIONAL an integer, the sentence length below which a sentence is excluded from summarization. DEFAULT: 4 words\n",
    "#   long_sentence_length  : OPTIONAL an integer, the sentence length beyond which a sentence is excluded from summarization. DEFAULT: 40 words\n",
    "#\n",
    "file_params = {\n",
    "    'file_url': 'https://s3-us-west-2.amazonaws.com/sumup-public/nucleus-sdk/quarles20181109a.docx',\n",
    "    'filename': 'quarles20181109a-newname.pdf',   \n",
    "    'custom_stop_words': [\"document\", \"sometimes\"], \n",
    "    'summary_length': 6,\n",
    "    'context_amount': 0, \n",
    "    'short_sentence_length': 4, \n",
    "    'long_sentence_length': 40}\n",
    "\n",
    "\n",
    "result = nucleus_helper.summarize_file_url(api_instance, file_params)\n",
    "  \n",
    "print('Summary for', result.doc_title, ':')\n",
    "for sent in result.summary.sentences:\n",
    "    print('    *', sent)\n",
    "\n",
    "print('-------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get document sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('------------------ Get document sentiment  --------------------')\n",
    "\n",
    "dataset = 'trump_tweets' # str | Dataset name.\n",
    "doc_title = 'D_Trump2018_8_17_14_10' # str | The title of the document to be analyzed.\n",
    "custom_stop_words = [\"real\",\"hillary\"] # List of stop words. (optional)\n",
    "num_topics = 8 # int | Number of topics to be extracted from the document. (optional) (default to 8)\n",
    "num_keywords = 8 # int | Number of keywords per topic that is extracted from the document. (optional) (default to 8)\n",
    "\n",
    "try:\n",
    "    payload = nucleus_api.DocumentSentimentModel(\n",
    "        dataset=dataset, \n",
    "        doc_title=doc_title, \n",
    "        custom_stop_words=custom_stop_words, \n",
    "        num_topics=num_topics, \n",
    "        num_keywords=num_keywords)\n",
    "    api_response = api_instance.post_doc_sentiment_api(payload)\n",
    "    \n",
    "    print('Sentiment for', api_response.result.doc_title)\n",
    "    print(api_response.result.sentiment)\n",
    "    \n",
    "except ApiException as e:\n",
    "    print(\"Exception when calling DocumentsApi->post_doc_sentiment_api: %s\\n\" % e)\n",
    "\n",
    "print('-------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
