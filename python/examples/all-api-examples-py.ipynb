{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial initialization\n",
    "\n",
    "NOTICE: All information contained herein is, and remains the property of SumUp Analytics Inc. and its suppliers, if any. The intellectual and technical concepts contained herein are proprietary to SumUp Analytics Inc. and its suppliers and may be covered by U.S. and Foreign Patents, patents in process, and are protected by trade secret or copyright law.\n",
    "\n",
    "Dissemination of this information or reproduction of this material is strictly forbidden unless prior written permission is obtained from SumUp Analytics Inc.\n",
    "\n",
    "Copyright (c) 2018-2020 SumUp Analytics, Inc. All Rights Reserved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure API host and key, and create a new API instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running example in Jupyter Notebook\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import json\n",
    "import datetime\n",
    "import time\n",
    "import nucleus_api\n",
    "from nucleus_api.rest import ApiException\n",
    "import nucleus_api.api.nucleus_api as nucleus_helper\n",
    "from pprint import pprint\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# Determine if in Jupyter notebook or not\n",
    "try:\n",
    "    ip = get_ipython()\n",
    "    running_notebook = True\n",
    "except NameError:\n",
    "    running_notebook = False\n",
    "\n",
    "if running_notebook:\n",
    "    print('Running example in Jupyter Notebook')\n",
    "else:\n",
    "    print('Running example in script mode')\n",
    "    \n",
    "configuration = nucleus_api.Configuration()\n",
    "configuration.host = 'UPDATE-WITH-API-SERVER-HOSTNAME'\n",
    "configuration.api_key['x-api-key'] = 'UPDATE-WITH-API-KEY'\n",
    "\n",
    "configuration.host = 'http://localhost:5000'\n",
    "configuration.api_key['x-api-key'] = '3TxyL0lzQ98561ejgP6Z-w'  # jw admin\n",
    "\n",
    "# Create API instance\n",
    "api_instance = nucleus_api.NucleusApi(nucleus_api.ApiClient(configuration))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manage datasets\n",
    "\n",
    "In this section, we walk you through our dataset ingestion and management APIs. You will learn how to:\n",
    "- Create a new dataset from different origin locations\n",
    "- Customize the metadata you want to include in a dataset\n",
    "- Append documents to an existing dataset\n",
    "- Delete specific documents or entire datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Append a specific file from a local drive to a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Start polling job status of 169821\n",
      "INFO: Job 169821 completed.\n",
      "quarles20181109a.pdf ( 51187 bytes) has been added to dataset dataset_test\n"
     ]
    }
   ],
   "source": [
    "dataset = \"dataset_test\"\n",
    "file = 'quarles20181109a.pdf'         \n",
    "metadata = {\"time\": \"1/2/2018\", \n",
    "            \"author\": \"Test Author\"}  # Optional json containing additional document metadata\n",
    "try:\n",
    "    api_response = api_instance.post_upload_file(file, dataset, metadata=metadata)\n",
    "    fp = api_response.result\n",
    "    print(fp.filename, '(', fp.size, 'bytes) has been added to dataset', dataset,)    \n",
    "except ApiException as e:\n",
    "    api_error = json.loads(e.body)\n",
    "    print('ERROR:', api_error['message'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Append all PDFs from a folder to a dataset using parallel injection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Start polling job status of 169822\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-04-24 11:05:20,875 WARNING Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(54, 'Connection reset by peer'))': /jobs?id=169822\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Job 169822 completed.\n",
      "INFO: Start polling job status of 169823\n",
      "INFO: Job 169823 completed.\n",
      "fomcminutes20181219.pdf ( 495324 bytes) has been added to dataset dataset_test\n",
      "fomcminutes20181108.pdf ( 369090 bytes) has been added to dataset dataset_test\n"
     ]
    }
   ],
   "source": [
    "folder = 'fomc-minutes'         \n",
    "dataset = 'dataset_test'# str | Destination dataset where the file will be inserted.\n",
    "\n",
    "# build file iterable. Each item in the iterable is in the format below:\n",
    "# {'filename': filename,   # filename to be uploaded. REQUIRED\n",
    "#  'metadata': {           # metadata for the file. Optional\n",
    "#      'key1': val1,       # keys can have arbiturary names as long as the names only\n",
    "#      'key2': val2        # contain alphanumeric (0-9|a-z|A-Z) and underscore (_)\n",
    "#   } \n",
    "# }\n",
    "\n",
    "file_iter = []\n",
    "for root, dirs, files in os.walk(folder):\n",
    "    for file in files:\n",
    "        if Path(file).suffix == '.pdf':\n",
    "            file_dict = {'filename': os.path.join(root, file),\n",
    "                         'metadata': {'field1': 'financial'}}\n",
    "            file_iter.append(file_dict)\n",
    "\n",
    "file_props = nucleus_helper.upload_files(api_instance, dataset, file_iter, processes=1)\n",
    "for fp in file_props:\n",
    "    print(fp.filename, '(', fp.size, 'bytes) has been added to dataset', dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Append a file from a URL to a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://s3-us-west-2.amazonaws.com/sumup-public/nucleus-sdk/quarles20181109a.docx ( 16881  bytes) has been added to dataset dataset_test\n"
     ]
    }
   ],
   "source": [
    "dataset = 'dataset_test'\n",
    "file_url = 'https://s3-us-west-2.amazonaws.com/sumup-public/nucleus-sdk/quarles20181109a.docx'\n",
    "# Optional filename saved on the server for the URL. If not specified, Nucleus will make an intelligent guess from the file URL\n",
    "filename = 'quarles20181109a-newname.pdf'  \n",
    "payload = nucleus_api.UploadURLModel(dataset=dataset,\n",
    "                                     file_url=file_url,\n",
    "                                     filename=filename)\n",
    "try:\n",
    "    api_response = api_instance.post_upload_url(payload)\n",
    "    url_prop = api_response.result\n",
    "    print(url_prop.file_url, '(', url_prop.size, ' bytes) has been added to dataset', dataset)\n",
    "\n",
    "except ApiException as e:\n",
    "    api_error = json.loads(e.body)\n",
    "    print('ERROR:', api_error['message'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Append multiple URLs to a dataset using parallel injection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://s3-us-west-2.amazonaws.com/sumup-public/nucleus-sdk/quarles20181109a.docx ( 16881  bytes) has been added to dataset dataset_test\n",
      "https://s3-us-west-2.amazonaws.com/sumup-public/nucleus-sdk/quarles20181109b.docx ( 16881  bytes) has been added to dataset dataset_test\n",
      "https://s3-us-west-2.amazonaws.com/sumup-public/nucleus-sdk/quarles20181109c.docx ( 16881  bytes) has been added to dataset dataset_test\n",
      "https://s3-us-west-2.amazonaws.com/sumup-public/nucleus-sdk/quarles20181109d.docx ( 16881  bytes) has been added to dataset dataset_test\n"
     ]
    }
   ],
   "source": [
    "dataset = 'dataset_test'\n",
    "file_urls = ['https://s3-us-west-2.amazonaws.com/sumup-public/nucleus-sdk/quarles20181109a.docx',\n",
    "             'https://s3-us-west-2.amazonaws.com/sumup-public/nucleus-sdk/quarles20181109b.docx',\n",
    "             'https://s3-us-west-2.amazonaws.com/sumup-public/nucleus-sdk/quarles20181109c.docx',\n",
    "             'https://s3-us-west-2.amazonaws.com/sumup-public/nucleus-sdk/quarles20181109d.docx']\n",
    "\n",
    "url_props = nucleus_helper.upload_urls(api_instance, dataset, file_urls, processes=1)\n",
    "\n",
    "for up in url_props:\n",
    "    print(up.file_url, '(', up.size, ' bytes) has been added to dataset', dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Append a JSON to a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_documents': 9, 'size': 33}\n"
     ]
    }
   ],
   "source": [
    "dataset = 'dataset_test'\n",
    "\n",
    "# The fields \"title\", \"time\", and \"content\" are mandatory in the JSON record.\n",
    "# Users can add any custom fields to the JSON record and all the information will be saved as metadata for the document.\n",
    "document = {\"title\": \"This a test json title field\",\n",
    "            \"time\": \"2019-01-01\",\n",
    "            \"content\": \"This is a test json content field\"}\n",
    "\n",
    "payload = nucleus_api.Appendjsonparams(dataset=dataset,\n",
    "                                       document=document)\n",
    "try:\n",
    "    api_response = api_instance.post_append_json_to_dataset(payload)\n",
    "    print(api_response.result)\n",
    "except ApiException as e:\n",
    "    api_error = json.loads(e.body)\n",
    "    print('ERROR:', api_error['message'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The JSON record must have \"title\", \"time\", and \"content\" fields.\n",
    "\n",
    "Users can add custom fields to the JSON record and all the information will be saved as metadata for the dataset.\n",
    "\n",
    "This metadata can subsequently be used in the analytics APIs to apply custom selections of documents in your dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Append JSONs from a CSV file to a dataset using parallel injection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 JSON records ( 532480 bytes) appended to trump_tweets\n"
     ]
    }
   ],
   "source": [
    "csv_file = 'trump-tweets-100.csv'\n",
    "dataset = 'trump_tweets'\n",
    "\n",
    "with open(csv_file, encoding='utf-8-sig') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    json_props = nucleus_helper.upload_jsons(api_instance, dataset, reader, processes=1)\n",
    "    \n",
    "    total_size = 0\n",
    "    total_jsons = 0\n",
    "    for jp in json_props:\n",
    "        total_size += jp.size\n",
    "        total_jsons += 1\n",
    "        \n",
    "    print(total_jsons, 'JSON records (', total_size, 'bytes) appended to', dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CSV file must have \"title\", \"time\", and \"content\" columns.\n",
    "\n",
    "Users can add any column to their CSV file and all the information will be saved as metadata for the dataset.\n",
    "\n",
    "This metadata can subsequently be used in the analytics APIs to apply custom selections of documents in your dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a dataset using embedded datafeeds:\n",
    "\n",
    "The Nucleus platform makes available a collection of datafeeds in read-only mode to users.\n",
    "- Central Banks: content in native language and English official translation where needed, grouped by content category, covering 14 Federal banks and all US Regional banks\n",
    "- SEC filings: 10Ks, 10Qs, 8Ks, 6Ks, 20Fs and S1s, including revised /A files for all companies filing with the SEC\n",
    "- News Media RSS: 200 English RSS feeds covering the fields of AI, Finance, Economics, News, Crypto, Culture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Central Banks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_central_bank = 'sumup/central_banks_chinese'\n",
    "metadata_selection_central_bank = {'bank': 'people_bank_of_china', \n",
    "                                   'document_category': ('speech', 'press release', 'publication')}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connect to these feeds by language using the following naming structure for the dataset name: 'sumup/central_banks_LANGUAGE'\n",
    "\n",
    "with LANGUAGE in {english, chinese, japanese, german, portuguese, spanish, russian, french, italian}\n",
    "\n",
    "You can then define a custom metadata selection off this feed by specifiying a set of banks and a set of document categories\n",
    "- document_category in {speech, press release, publication, formal research}\n",
    "- bank in {federal_reserve, bank_of_canada, banco_de_mexico, bank_of_brazil, ecb, bank_of_england, bundesbank, bank_of_france, bank_of_italy, bank_of_spain, russian_fed, people_bank_of_china, bank_of_japan, bank_of_australia, atlanta_fed, boston_fed, chicago_fed, cleveland_fed, dallas_fed, kansas_city_fed, minneapolis_fed, new_york_fed, philadelphia_fed, richmond_fed, san_francisco_fed, st_louis_fed}\n",
    "\n",
    "When passing these parameters to any of the analytics APIs, you can also specify a time period selection using either of:\n",
    "- The time_period input argument\n",
    "- The period_start and period_end input arguments\n",
    "\n",
    "Examples of such calls are detailed in this tutorial, within the sections discussing analytics APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### News RSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'sumup/rss_feed_ai'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connect to these feeds by field using the following naming structure for the dataset name: 'sumup/rss_feed_FIELD'\n",
    "\n",
    "with FIELD in {ai, finance, economics, news, crypto, culture}\n",
    "\n",
    "When passing these parameters to any of the analytics APIs, you can also specify a time period selection using either of:\n",
    "- The time_period input argument\n",
    "- The period_start and period_end input arguments\n",
    "\n",
    "Examples of such calls are detailed in this tutorial, within the sections discussing analytics APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SEC Filings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEC filings selected:\n",
      "    Company count: 5402\n",
      "    Date range: [datetime.date(1993, 2, 11), datetime.date(2020, 4, 24)]\n"
     ]
    }
   ],
   "source": [
    "# GET THE LIST OF ALL THE COMPANIES AVAILABLE IN THE FEED\n",
    "\n",
    "payload = nucleus_api.EdgarFields(tickers=[], \n",
    "                                  filing_types=[], \n",
    "                                  sections=[])\n",
    "try:\n",
    "    api_response = api_instance.post_available_sec_filings(payload)\n",
    "    api_ok = True\n",
    "except ApiException as e:\n",
    "    api_error = json.loads(e.body)\n",
    "    print('ERROR:', api_error['message'])\n",
    "    api_ok = False\n",
    "    \n",
    "if api_ok:\n",
    "    print('SEC filings selected:')\n",
    "    print('    Company count:', len(api_response.result.tickers))\n",
    "    print('    Date range:', api_response.result.date_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEC filings for: {'IBM': 'International Business Machines Corporation'}\n",
      "    Types: ['10-K', '10-Q', '10-Q/A', '8-K', '8-K/A']\n",
      "    Count: 531\n",
      "    Date ranges: [datetime.date(1994, 5, 10), datetime.date(2020, 4, 7)]\n"
     ]
    }
   ],
   "source": [
    "# GET THE LIST OF AVAILABLE FILING TYPES FOR A COMPANY\n",
    "\n",
    "payload = nucleus_api.EdgarFields(tickers=[\"IBM\"], # Select IBM company\n",
    "                                  filing_types=[], \n",
    "                                  sections=[])\n",
    "try:\n",
    "    api_response = api_instance.post_available_sec_filings(payload)\n",
    "    api_ok = True\n",
    "except ApiException as e:\n",
    "    api_error = json.loads(e.body)\n",
    "    print('ERROR:', api_error['message'])\n",
    "    api_ok = False\n",
    "\n",
    "if api_ok:\n",
    "    print('SEC filings for:', api_response.result.tickers)\n",
    "    print('    Types:', api_response.result.filing_types)\n",
    "    print('    Count:', api_response.result.count)\n",
    "    print('    Date ranges:', api_response.result.date_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sections in ['10-K'] filings for {'IBM': 'International Business Machines Corporation'}\n",
      "    Business\n",
      "    Certain Relationships and Related Transactions, and Director Independence\n",
      "    Changes in and Disagreements with Accountants on Accounting and Financial Disclosure\n",
      "    Controls and Procedures\n",
      "    Directors, Executive Officers and Corporate Governance\n",
      "    Executive Compensation\n",
      "    Exhibits, Financial Statement Schedules\n",
      "    Financial Statements and Supplementary Data\n",
      "    Legal Proceedings\n",
      "    Management's Discussion and Analysis of Financial Condition and Results of Operations\n",
      "    Market for Registrant's Common Equity, Related Stockholder Matters and Issuer Purchases of Equity Securities\n",
      "    Other Information\n",
      "    Principal Accountant Fees and Services\n",
      "    Properties\n",
      "    Quantitative and Qualitative Disclosures about Market Risk\n",
      "    Risk Factors\n",
      "    Security Ownership of Certain Beneficial Owners and Management and Related Stockholder Matters\n",
      "    Selected Financial Data\n",
      "    Submission of Matters to a Vote of Security Holders\n",
      "    Unresolved Staff Comments\n"
     ]
    }
   ],
   "source": [
    "# GET THE LIST OF AVAILABLE SECTIONS IN A GIVEN FILING TYPE FOR A GIVEN COMPANY\n",
    "\n",
    "payload = nucleus_api.EdgarFields(tickers=[\"IBM\"], # Select IBM company\n",
    "                                  filing_types=[\"10-K\"], # Get list of sections available in 10-Ks\n",
    "                                  sections=[])\n",
    "try:\n",
    "    api_response = api_response = api_instance.post_available_sec_filings(payload)\n",
    "    api_ok = True\n",
    "except ApiException as e:\n",
    "    api_error = json.loads(e.body)\n",
    "    print('ERROR:', api_error['message'])\n",
    "    api_ok = False\n",
    "    \n",
    "if api_ok:\n",
    "    print('Sections in {} filings for {}'.format(api_response.result.filing_types, api_response.result.tickers))\n",
    "    for section in api_response.result.sections:\n",
    "        print('    {}'.format(section))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset dataset_sec1 created successfully from SEC filings\n"
     ]
    }
   ],
   "source": [
    "# BUILD A DATASET FROM A CUSTOM SELECTION OF SEC FILINGS\n",
    "\n",
    "dataset = \"dataset_sec1\" \n",
    "\n",
    "# Dataset from a particular section for a ticker\n",
    "payload = nucleus_api.EdgarQuery(destination_dataset=dataset,\n",
    "                                 tickers=[\"BABA\"], \n",
    "                                 filing_types=[\"20-F\"], \n",
    "                                 sections=[\"Quantitative and Qualitative Disclosures about Market Risk\"])\n",
    "try:\n",
    "    api_response = api_instance.post_create_dataset_from_sec_filings(payload)\n",
    "    api_ok = True\n",
    "except ApiException as e:\n",
    "    api_error = json.loads(e.body)\n",
    "    print('ERROR:', api_error['message'])\n",
    "    api_ok = False\n",
    "    \n",
    "if api_ok:\n",
    "    print('Dataset {} created successfully from SEC filings'.format(api_response.result['destination_dataset']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset dataset_sec2 created successfully from SEC filings\n"
     ]
    }
   ],
   "source": [
    "# BUILD A DATASET FROM A CUSTOM SELECTION OF SEC FILINGS\n",
    "\n",
    "dataset = \"dataset_sec2\" \n",
    "period_start = \"2018-01-01\" \n",
    "period_end= \"2019-06-01\"\n",
    "\n",
    "# Dataset is all 8Ks for  the last 18 months\n",
    "payload = nucleus_api.EdgarQuery(destination_dataset=dataset,\n",
    "                                 tickers=[\"NFLX\"], \n",
    "                                 filing_types=[\"8-K\"], \n",
    "                                 sections=[],\n",
    "                                 period_start=period_start,\n",
    "                                 period_end=period_end)\n",
    "try:\n",
    "    api_response = api_instance.post_create_dataset_from_sec_filings(payload)\n",
    "    api_ok = True\n",
    "except ApiException as e:\n",
    "    api_error = json.loads(e.body)\n",
    "    print('ERROR:', api_error['message'])\n",
    "    api_ok = False\n",
    "    \n",
    "if api_ok:\n",
    "    print('Dataset {} created successfully from SEC filings'.format(api_response.result['destination_dataset']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SEC filings have a more complex structure, therefore Nucleus provides a specific set of APIs to interact with this data and to allow you to create tailored datasets\n",
    "\n",
    "There are two payloads available to you:\n",
    "- nucleus_api.EdgarFields, which allows you to navigate the SEC filings' content alongside specific requirements using the post_available_sec_filings API\n",
    "- nucleus_api.EdgarQuery, which allows you to create SEC filings' datasets with specific requirements using the post_create_dataset_from_sec_filings API\n",
    "\n",
    "These payloads expose 5 optional arguments:\n",
    "- tickers, which are as-recorded in the EDGAR database\n",
    "- filing_types, which are to be chosen among {10-K, 10-Q, 8-K, 6-K, 20-F, S-1, 10-K/A, 10-Q/A, 8-K/A, 6-K/A, 20-F/A, S-1/A}\n",
    "- sections, which are the standardized section names as-recorded in the EDGAR database for each form type\n",
    "- period_start, the starting date of the period you are interested in\n",
    "- period_end, the end date of the period you are interested in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List all the datasets available to a user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 datasets in the database:\n",
      "     dataset_sec2\n",
      "     dataset_sec1\n",
      "     trump_tweets\n",
      "     dataset_test\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    api_response = api_instance.get_list_datasets()\n",
    "except ApiException as e:\n",
    "    print(\"Exception when calling DatasetsApi->get_list_datasets: %s\\n\" % e)\n",
    "\n",
    "list_datasets = api_response.result\n",
    "\n",
    "print(len(list_datasets), 'datasets in the database:')\n",
    "for ds in list_datasets:\n",
    "    print('    ', ds.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve summary information for a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Information about dataset dataset_sec2\n",
      "    Language: en\n",
      "    Number of documents: 33\n",
      "    Time range: 2018-01-22 00:00:00 to 2019-04-16 00:00:00\n"
     ]
    }
   ],
   "source": [
    "dataset = 'dataset_sec2' # str | Dataset name.\n",
    "query = '' # str | Fulltext query, using mysql MATCH boolean query format. (optional)\n",
    "metadata_selection = '' # str | json object of {\\\"metadata_field\\\":[\\\"selected_values\\\"]} (optional)\n",
    "time_period = '' # str | Time period selection (optional)\n",
    "period_start = \"\" # str | Start date for the period to analyze within the dataset. Format: \"YYYY-MM-DD HH:MM:SS\"\n",
    "period_end = \"\" # str | End date for the period to analyze within the dataset. Format: \"YYYY-MM-DD HH:MM:SS\"\n",
    "\n",
    "try:\n",
    "    payload = nucleus_api.DatasetInfo(dataset=dataset, \n",
    "                                    query=query, \n",
    "                                    metadata_selection=metadata_selection, \n",
    "                                    time_period=time_period)\n",
    "    api_response = api_instance.post_dataset_info(payload)\n",
    "    print('Information about dataset', dataset)\n",
    "    print('    Language:', api_response.result.detected_language)\n",
    "    print('    Number of documents:', api_response.result.num_documents)\n",
    "    print('    Time range:', datetime.datetime.fromtimestamp(float(api_response.result.time_range[0])),\n",
    "             'to', datetime.datetime.fromtimestamp(float(api_response.result.time_range[1])))\n",
    "except ApiException as e:\n",
    "    api_error = json.loads(e.body)\n",
    "    print('ERROR:', api_error['message'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete documents from a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document ['1'] from dataset dataset_test deleted.\n"
     ]
    }
   ],
   "source": [
    "dataset = 'dataset_test'\n",
    "\n",
    "doc_ids = ['1']\n",
    "payload = nucleus_api.DeleteDocumentModel(dataset=dataset,\n",
    "                                          doc_ids=doc_ids)\n",
    "try:\n",
    "    api_response = api_instance.post_delete_document(payload)\n",
    "except ApiException as e:\n",
    "    print(\"Exception when calling DatasetsApi->post_delete_document: %s\\n\" % e)\n",
    "\n",
    "\n",
    "print('Document', doc_ids, 'from dataset', dataset, 'deleted.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset_test\n"
     ]
    }
   ],
   "source": [
    "dataset = 'dataset_test'\n",
    "payload = nucleus_api.DeleteDatasetModel(dataset=dataset) # Deletedatasetmodel | \n",
    "\n",
    "try:\n",
    "    api_response = api_instance.post_delete_dataset(payload)\n",
    "    print(api_response.result['dataset_deleted'])\n",
    "except ApiException as e:\n",
    "    api_error = json.loads(e.body)\n",
    "    print('ERROR:', api_error['message'])\n",
    "    \n",
    "# List datasets again to check if the specified dataset has been deleted\n",
    "try:\n",
    "    api_response = api_instance.get_list_datasets()\n",
    "except ApiException as e:\n",
    "    api_error = json.loads(e.body)\n",
    "    print('ERROR:', api_error['message'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform topics-centered analytics\n",
    "\n",
    "This section goes over all APIs that enable users to identify, extract and analyze topics found in a dataset.\n",
    "- Topic modeling\n",
    "- Topic transfer learning for propagation analysis of topics' strength, sentiment and consensus\n",
    "- Sentiment analysis\n",
    "- Consensus analysis\n",
    "- Cross-documents topic summarization\n",
    "- Historical analysis of topics' strength, sentiment, and consensus\n",
    "- Authors similarity analysis\n",
    "- Contrasted topic modeling: topic best separating two sub-categories of documents in a corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify and extract a list of topics in a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0 keywords:\n",
      "    Keywords: illegal aliens;united permission;permission illegal;enter united;aliens treated;aliens illegal;treated entered;entered legally\n",
      "    Keyword weights: 0.1241;0.1241;0.1241;0.2483;0.1241;0.1241;0.0655;0.0655\n",
      "    Strength: 0.1322\n",
      "    Document IDs: 2692713684604938265 9742569426091859106\n",
      "    Document exposures: 0.527689 0.472311\n",
      "---------------\n",
      "Topic 1 keywords:\n",
      "    Keywords: bruce ohr;ohr doj;legal jeopardy;doj legal;ohr justice;justice department;christopher steele;jeopardy astonishingthat;jeopardy astonishing;astonishingthat employed;astonishing employed\n",
      "    Keyword weights: 0.2431;0.113;0.113;0.113;0.0565;0.0565;0.0659;0.0659;0.0565;0.0565;0.0601\n",
      "    Strength: 0.1671\n",
      "    Document IDs: 4437198275925723018 5294521276627635033 6817024102264760936 7148720741304645705 14824073781424371042 15178489020767442896\n",
      "    Document exposures: 0.214732 0.136323 0.111913 0.328009 0.090493 0.11853\n",
      "---------------\n",
      "Topic 2 keywords:\n",
      "    Keywords: america great;statement america;andrew cuomo;great great;worse hightax;hightax andrew;deplorables statement;cuomo statement;clinton deplorables\n",
      "    Keyword weights: 0.1365;0.1499;0.1513;0.1248;0.0875;0.0875;0.0875;0.0875;0.0875\n",
      "    Strength: 0.1278\n",
      "    Document IDs: 681266336486897660 2473572527244144057 4046653213651213725 4349543959441164718 9687935645391984655\n",
      "    Document exposures: 0.16757 0.07648 0.447117 0.118358 0.190475\n",
      "---------------\n",
      "Topic 3 keywords:\n",
      "    Keywords: peter strzok;fired fbi;agent peter;fbi agent;strzok fraud;rigged investigation;investigation started;fraud rigged\n",
      "    Keyword weights: 0.1798;0.1798;0.1798;0.136;0.0812;0.0812;0.0812;0.0812\n",
      "    Strength: 0.1237\n",
      "    Document IDs: 2654048984553148490 6817024102264760936 10234332166402996482\n",
      "    Document exposures: 0.263083 0.318243 0.418674\n",
      "---------------\n",
      "Topic 4 keywords:\n",
      "    Keywords: trump campaign;lou dobbs;evidence collusion;dobbs special;democrats evidence;conflicts angry;collusion trump;campaign russia;angry democrats\n",
      "    Keyword weights: 0.1231;0.1096;0.1096;0.1096;0.1096;0.1096;0.1096;0.1096;0.1096\n",
      "    Strength: 0.1255\n",
      "    Document IDs: 5336597423569459183 7290488938710124315 13115764764473553109 15091083406720907936\n",
      "    Document exposures: 0.041886 0.458619 0.040876 0.458619\n",
      "---------------\n",
      "Topic 5 keywords:\n",
      "    Keywords: witch hunt;donald trump;rigged witch;frame donald;unfortunate situation;trump rigged;situation decided;decided frame\n",
      "    Keyword weights: 0.179;0.1268;0.0971;0.0971;0.0971;0.1268;0.179;0.0971\n",
      "    Strength: 0.1501\n",
      "    Document IDs: 2654048984553148490 3295270030761385310 5983550775501810643 9220064279401475464 13125097700312206949 13206638373098679225 16578282956147621208 17901578373306652528\n",
      "    Document exposures: 0.0581 0.054348 0.429451 0.054348 0.111988 0.05613 0.177534 0.0581\n",
      "---------------\n",
      "Topic 6 keywords:\n",
      "    Keywords: fake news;news media;wacky omarosa;presidential lowlife;overtime wacky;omarosa modern;omarosa legitimate;modern form;media overtime;lowlife omarosa;form communication;communication fake\n",
      "    Keyword weights: 0.1115;0.078;0.078;0.078;0.078;0.078;0.078;0.094;0.078;0.078;0.0924;0.078\n",
      "    Strength: 0.0901\n",
      "    Document IDs: 1436959758999674726 3946302249027831785 9740888651068443724 12437126850273027242 14171720646613873024 14368578770105844206\n",
      "    Document exposures: 0.058825 0.527792 0.068986 0.187864 0.08449 0.072045\n",
      "---------------\n",
      "Topic 7 keywords:\n",
      "    Keywords: john brennan;cia director;tuckercarlson speaking;speaking john;limited intellectually;intellectually cia;director good;brennan limited\n",
      "    Keyword weights: 0.169;0.1498;0.1135;0.1135;0.1135;0.1135;0.1135;0.1135\n",
      "    Strength: 0.0831\n",
      "    Document IDs: 427773076591013776 7508075727870080337 9220064279401475464\n",
      "    Document exposures: 0.210038 0.682152 0.10781\n",
      "---------------\n"
     ]
    }
   ],
   "source": [
    "dataset = 'trump_tweets'\n",
    "#query = '(\"Trump\" OR \"president\")' # str | Fulltext query, using mysql MATCH boolean query format. Example, (\\\"word1\\\" OR \\\"word2\\\") AND (\\\"word3\\\" OR \\\"word4\\\") (optional)\n",
    "query = ''\n",
    "custom_stop_words = [\"real\",\"hillary\"] # str | List of stop words. (optional)\n",
    "num_topics = 8 # int | Number of topics to be extracted from the dataset. (optional) (default to 8)\n",
    "metadata_selection = \"\" # dict | JSON object specifying metadata-based queries on the dataset, of type {\"metadata_field\": \"selected_values\"} (optional)\n",
    "time_period = \"\"     # str | Time period selection. Choices: [\"1M\",\"3M\",\"6M\",\"12M\",\"3Y\",\"5Y\",\"\"] (optional)\n",
    "\n",
    "try:\n",
    "    payload = nucleus_api.Topics(dataset=dataset,                                \n",
    "                                query=query,                   \n",
    "                                custom_stop_words=custom_stop_words,     \n",
    "                                num_topics=num_topics,\n",
    "                                metadata_selection=metadata_selection,\n",
    "                                time_period=time_period)\n",
    "    api_response = api_instance.post_topic_api(payload)        \n",
    "except ApiException as e:\n",
    "    api_error = json.loads(e.body)\n",
    "    print('ERROR:', api_error['message'])\n",
    "\n",
    "doc_ids = api_response.result.doc_ids\n",
    "topics = api_response.result.topics\n",
    "for i, res in enumerate(topics):\n",
    "    print('Topic', i, 'keywords:')\n",
    "    print('    Keywords:', res.keywords)\n",
    "    keywords_weight_str = \";\".join(str(x) for x in res.keywords_weight)\n",
    "    print('    Keyword weights:', keywords_weight_str)\n",
    "    print('    Strength:', res.strength)\n",
    "    doc_topic_exposure_sel = []  # list of non-zero doc_topic_exposures\n",
    "    doc_id_sel = []        # list of doc ids matching doc_topic_exposure_sel\n",
    "    for j in range(len(res.doc_topic_exposures)):\n",
    "        doc_topic_exp = float(res.doc_topic_exposures[j])\n",
    "        if doc_topic_exp != 0:\n",
    "            doc_topic_exposure_sel.append(doc_topic_exp)\n",
    "            doc_id_sel.append(doc_ids[j])\n",
    "    \n",
    "    doc_id_sel_str = ' '.join(str(x) for x in doc_id_sel)\n",
    "    doc_topic_exposure_sel_str = ' '.join(str(x) for x in doc_topic_exposure_sel)\n",
    "    print('    Document IDs:', doc_id_sel_str)\n",
    "    print('    Document exposures:', doc_topic_exposure_sel_str)\n",
    "    print('---------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify and extract a list of topics in a dataset with a time range selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0 keywords:\n",
      "    Keywords: illegal aliens;united permission;permission illegal;enter united;aliens treated;aliens illegal;treated entered;entered legally\n",
      "    Keyword weights: 0.1241;0.1241;0.1241;0.2483;0.1241;0.1241;0.0655;0.0655\n",
      "    Strength: 0.1322\n",
      "    Document IDs: 2692713684604938265 9742569426091859106\n",
      "    Document exposures: 0.527689 0.472311\n",
      "---------------\n",
      "Topic 1 keywords:\n",
      "    Keywords: bruce ohr;ohr doj;legal jeopardy;doj legal;ohr justice;justice department;christopher steele;jeopardy astonishingthat;jeopardy astonishing;astonishingthat employed;astonishing employed\n",
      "    Keyword weights: 0.2431;0.113;0.113;0.113;0.0565;0.0565;0.0659;0.0659;0.0565;0.0565;0.0601\n",
      "    Strength: 0.1671\n",
      "    Document IDs: 4437198275925723018 5294521276627635033 6817024102264760936 7148720741304645705 14824073781424371042 15178489020767442896\n",
      "    Document exposures: 0.214732 0.136323 0.111913 0.328009 0.090493 0.11853\n",
      "---------------\n",
      "Topic 2 keywords:\n",
      "    Keywords: america great;statement america;andrew cuomo;great great;worse hightax;hightax andrew;deplorables statement;cuomo statement;clinton deplorables\n",
      "    Keyword weights: 0.1365;0.1499;0.1513;0.1248;0.0875;0.0875;0.0875;0.0875;0.0875\n",
      "    Strength: 0.1278\n",
      "    Document IDs: 681266336486897660 2473572527244144057 4046653213651213725 4349543959441164718 9687935645391984655\n",
      "    Document exposures: 0.16757 0.07648 0.447117 0.118358 0.190475\n",
      "---------------\n",
      "Topic 3 keywords:\n",
      "    Keywords: peter strzok;fired fbi;agent peter;fbi agent;strzok fraud;rigged investigation;investigation started;fraud rigged\n",
      "    Keyword weights: 0.1798;0.1798;0.1798;0.136;0.0812;0.0812;0.0812;0.0812\n",
      "    Strength: 0.1237\n",
      "    Document IDs: 2654048984553148490 6817024102264760936 10234332166402996482\n",
      "    Document exposures: 0.263083 0.318243 0.418674\n",
      "---------------\n",
      "Topic 4 keywords:\n",
      "    Keywords: trump campaign;lou dobbs;evidence collusion;dobbs special;democrats evidence;conflicts angry;collusion trump;campaign russia;angry democrats\n",
      "    Keyword weights: 0.1231;0.1096;0.1096;0.1096;0.1096;0.1096;0.1096;0.1096;0.1096\n",
      "    Strength: 0.1255\n",
      "    Document IDs: 5336597423569459183 7290488938710124315 13115764764473553109 15091083406720907936\n",
      "    Document exposures: 0.041886 0.458619 0.040876 0.458619\n",
      "---------------\n",
      "Topic 5 keywords:\n",
      "    Keywords: witch hunt;donald trump;rigged witch;frame donald;unfortunate situation;trump rigged;situation decided;decided frame\n",
      "    Keyword weights: 0.179;0.1268;0.0971;0.0971;0.0971;0.1268;0.179;0.0971\n",
      "    Strength: 0.1501\n",
      "    Document IDs: 2654048984553148490 3295270030761385310 5983550775501810643 9220064279401475464 13125097700312206949 13206638373098679225 16578282956147621208 17901578373306652528\n",
      "    Document exposures: 0.0581 0.054348 0.429451 0.054348 0.111988 0.05613 0.177534 0.0581\n",
      "---------------\n",
      "Topic 6 keywords:\n",
      "    Keywords: fake news;news media;wacky omarosa;presidential lowlife;overtime wacky;omarosa modern;omarosa legitimate;modern form;media overtime;lowlife omarosa;form communication;communication fake\n",
      "    Keyword weights: 0.1115;0.078;0.078;0.078;0.078;0.078;0.078;0.094;0.078;0.078;0.0924;0.078\n",
      "    Strength: 0.0901\n",
      "    Document IDs: 1436959758999674726 3946302249027831785 9740888651068443724 12437126850273027242 14171720646613873024 14368578770105844206\n",
      "    Document exposures: 0.058825 0.527792 0.068986 0.187864 0.08449 0.072045\n",
      "---------------\n",
      "Topic 7 keywords:\n",
      "    Keywords: john brennan;cia director;tuckercarlson speaking;speaking john;limited intellectually;intellectually cia;director good;brennan limited\n",
      "    Keyword weights: 0.169;0.1498;0.1135;0.1135;0.1135;0.1135;0.1135;0.1135\n",
      "    Strength: 0.0831\n",
      "    Document IDs: 427773076591013776 7508075727870080337 9220064279401475464\n",
      "    Document exposures: 0.210038 0.682152 0.10781\n",
      "---------------\n"
     ]
    }
   ],
   "source": [
    "dataset = 'trump_tweets'\n",
    "#query = '(\"Trump\" OR \"president\")' # str | Fulltext query, using mysql MATCH boolean query format. Example, (\\\"word1\\\" OR \\\"word2\\\") AND (\\\"word3\\\" OR \\\"word4\\\") (optional)\n",
    "query = ''\n",
    "custom_stop_words = [\"real\",\"hillary\"] # str | List of stop words. (optional)\n",
    "num_topics = 8 # int | Number of topics to be extracted from the dataset. (optional) (default to 8)\n",
    "metadata_selection = \"\" # dict | JSON object specifying metadata-based queries on the dataset, of type {\"metadata_field\": \"selected_values\"} (optional)\n",
    "period_start = \"2016-10-15\" # str | Start date for the period to analyze within the dataset. Format: \"YYYY-MM-DD\"\n",
    "period_end = \"2019-01-01\" # str | End date for the period to analyze within the dataset. Format: \"YYYY-MM-DD\"\n",
    "\n",
    "try:\n",
    "    payload = nucleus_api.Topics(dataset=dataset,                                \n",
    "                                 query=query,                   \n",
    "                                 custom_stop_words=custom_stop_words,     \n",
    "                                 num_topics=num_topics,\n",
    "                                 metadata_selection=metadata_selection,\n",
    "                                 period_start=period_start,\n",
    "                                 period_end=period_end)\n",
    "    api_response = api_instance.post_topic_api(payload)        \n",
    "except ApiException as e:\n",
    "    api_error = json.loads(e.body)\n",
    "    print('ERROR:', api_error['message'])\n",
    "    \n",
    "doc_ids = api_response.result.doc_ids\n",
    "topics = api_response.result.topics\n",
    "for i, res in enumerate(topics):\n",
    "    print('Topic', i, 'keywords:')\n",
    "    print('    Keywords:', res.keywords)\n",
    "    keywords_weight_str = \";\".join(str(x) for x in res.keywords_weight)\n",
    "    print('    Keyword weights:', keywords_weight_str)\n",
    "    print('    Strength:', res.strength)\n",
    "    doc_topic_exposure_sel = []  # list of non-zero doc_topic_exposure\n",
    "    doc_id_sel = []        # list of doc ids matching doc_topic_exposure_sel\n",
    "    for j in range(len(res.doc_topic_exposures)):\n",
    "        doc_topic_exp = float(res.doc_topic_exposures[j])\n",
    "        if doc_topic_exp != 0:\n",
    "            doc_topic_exposure_sel.append(doc_topic_exp)\n",
    "            doc_id_sel.append(doc_ids[j])\n",
    "    \n",
    "    doc_id_sel_str = ' '.join(str(x) for x in doc_id_sel)\n",
    "    doc_topic_exposure_sel_str = ' '.join(str(x) for x in doc_topic_exposure_sel)\n",
    "    print('    Document IDs:', doc_id_sel_str)\n",
    "    print('    Document exposures:', doc_topic_exposure_sel_str)\n",
    "    print('---------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify and extract a list of topics in a dataset with a metadata selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0 keywords:\n",
      "    Keywords: worse hightax;hightax andrew;andrew cuomo\n",
      "    Keyword weights: 0.3333;0.3333;0.3333\n",
      "    Strength: 0.3333\n",
      "    Document IDs: 4046653213651213725\n",
      "    Document exposures: 1.0\n",
      "---------------\n",
      "Topic 1 keywords:\n",
      "    Keywords: statement america;cuomo statement;america great\n",
      "    Keyword weights: 0.3333;0.3333;0.3333\n",
      "    Strength: 0.3333\n",
      "    Document IDs: 4046653213651213725\n",
      "    Document exposures: 1.0\n",
      "---------------\n",
      "Topic 2 keywords:\n",
      "    Keywords: great great;deplorables statement;clinton deplorables\n",
      "    Keyword weights: 0.3333;0.3333;0.3333\n",
      "    Strength: 0.3333\n",
      "    Document IDs: 4046653213651213725\n",
      "    Document exposures: 1.0\n",
      "---------------\n"
     ]
    }
   ],
   "source": [
    "dataset = 'trump_tweets'\n",
    "#query = '(\"Trump\" OR \"president\")' # str | Fulltext query, using mysql MATCH boolean query format. Example, (\\\"word1\\\" OR \\\"word2\\\") AND (\\\"word3\\\" OR \\\"word4\\\") (optional)\n",
    "query = ''\n",
    "custom_stop_words = [\"real\",\"hillary\"] # str | List of stop words. (optional)\n",
    "num_topics = 8 # int | Number of topics to be extracted from the dataset. (optional) (default to 8)\n",
    "metadata_selection = {\"author\": \"D_Trump16\"} # dict | JSON object specifying metadata-based queries on the dataset, of type {\"metadata_field\": \"selected_values\"} (optional)\n",
    "\n",
    "try:\n",
    "    payload = nucleus_api.Topics(dataset=dataset,                                \n",
    "                                 query=query,                   \n",
    "                                 custom_stop_words=custom_stop_words,     \n",
    "                                 num_topics=num_topics,\n",
    "                                 metadata_selection=metadata_selection)\n",
    "    api_response = api_instance.post_topic_api(payload)        \n",
    "except ApiException as e:\n",
    "    api_error = json.loads(e.body)\n",
    "    print('ERROR:', api_error['message'])\n",
    "    \n",
    "doc_ids = api_response.result.doc_ids\n",
    "topics = api_response.result.topics\n",
    "for i, res in enumerate(topics):\n",
    "    print('Topic', i, 'keywords:')\n",
    "    print('    Keywords:', res.keywords)\n",
    "    keywords_weight_str = \";\".join(str(x) for x in res.keywords_weight)\n",
    "    print('    Keyword weights:', keywords_weight_str)\n",
    "    print('    Strength:', res.strength)\n",
    "    doc_topic_exposure_sel = []  # list of non-zero doc_topic_exposure\n",
    "    doc_id_sel = []        # list of doc ids matching doc_topic_exposure_sel\n",
    "    for j in range(len(res.doc_topic_exposures)):\n",
    "        doc_topic_exp = float(res.doc_topic_exposures[j])\n",
    "        if doc_topic_exp != 0:\n",
    "            doc_topic_exposure_sel.append(doc_topic_exp)\n",
    "            doc_id_sel.append(doc_ids[j])\n",
    "    \n",
    "    doc_id_sel_str = ' '.join(str(x) for x in doc_id_sel)\n",
    "    doc_topic_exposure_sel_str = ' '.join(str(x) for x in doc_topic_exposure_sel)\n",
    "    print('    Document IDs:', doc_id_sel_str)\n",
    "    print('    Document exposures:', doc_topic_exposure_sel_str)\n",
    "    print('---------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify and extract a list of topics in a dataset without removing redundant content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0 keywords:\n",
      "    Keywords: total endorsement;complete total;pete complete;bob total;america great;witch hunt;great great;rigged witch\n",
      "    Keyword weights: 0.0008;0.0005;0.0006;0.0632;0.4175;0.4831;0.0004;0.0339\n",
      "    Strength: 0.3166\n",
      "    Document IDs: 681266336486897660 2473572527244144057 2654048984553148490 3295270030761385310 4046653213651213725 4349543959441164718 5197981696652006791 5294521276627635033 5501029233133759464 5983550775501810643 6817024102264760936 8932963129356075453 9210234284146319434 9687935645391984655 9995371484769371289 10022945975448422485 10908843577559309852 13125097700312206949 14286875122783287072 15284262488783149410 16020903065363723393 16578282956147621208 17583854768923684322\n",
      "    Document exposures: 9.1e-05 0.000206 7.3e-05 7.6e-05 0.000196 0.000107 0.159343 0.000109 0.060438 0.000142 0.000117 0.115334 0.000217 0.000188 0.134681 0.108337 9.1e-05 0.000154 0.119382 0.119382 0.061866 9.1e-05 0.119382\n",
      "---------------\n",
      "Topic 1 keywords:\n",
      "    Keywords: andrew cuomo;statement america;york dumb;pushback governor;lack greatness;governor andrew;dumb statement;cuomo york;america lack\n",
      "    Keyword weights: 0.1048;0.1048;0.141;0.1048;0.1048;0.1048;0.1254;0.1048;0.1048\n",
      "    Strength: 0.1207\n",
      "    Document IDs: 681266336486897660 2473572527244144057 4046653213651213725 4349543959441164718 9210234284146319434 9687935645391984655 10908843577559309852\n",
      "    Document exposures: 0.327998 0.062052 0.111762 0.054423 0.065409 0.050358 0.327998\n",
      "---------------\n",
      "Topic 2 keywords:\n",
      "    Keywords: illegal aliens;united permission;permission illegal;enter united;aliens treated;aliens illegal;treated entered;entered legally\n",
      "    Keyword weights: 0.1241;0.1241;0.1241;0.2483;0.1241;0.1241;0.0655;0.0655\n",
      "    Strength: 0.1014\n",
      "    Document IDs: 2692713684604938265 9742569426091859106\n",
      "    Document exposures: 0.527689 0.472311\n",
      "---------------\n",
      "Topic 3 keywords:\n",
      "    Keywords: bruce ohr;ohr doj;legal jeopardy;doj legal;ohr justice;justice department;christopher steele;jeopardy astonishingthat;jeopardy astonishing;astonishingthat employed;astonishing employed\n",
      "    Keyword weights: 0.2432;0.113;0.113;0.113;0.0565;0.0565;0.066;0.066;0.0565;0.0565;0.0601\n",
      "    Strength: 0.1282\n",
      "    Document IDs: 4437198275925723018 5294521276627635033 6817024102264760936 7148720741304645705 14824073781424371042 15178489020767442896\n",
      "    Document exposures: 0.237569 0.150375 0.124806 0.237569 0.100907 0.148775\n",
      "---------------\n",
      "Topic 4 keywords:\n",
      "    Keywords: fake news;news media;wacky omarosa;presidential lowlife;overtime wacky;omarosa modern;omarosa legitimate;modern form;media overtime;lowlife omarosa;form communication;communication fake\n",
      "    Keyword weights: 0.1219;0.077;0.077;0.077;0.077;0.077;0.077;0.0937;0.077;0.077;0.0911;0.077\n",
      "    Strength: 0.0756\n",
      "    Document IDs: 1436959758999674726 3946302249027831785 9740888651068443724 11599059962808147623 12437126850273027242 14171720646613873024 14368578770105844206\n",
      "    Document exposures: 0.057456 0.52679 0.046083 0.061718 0.160637 0.083078 0.064238\n",
      "---------------\n",
      "Topic 5 keywords:\n",
      "    Keywords: peter strzok;fired fbi;agent peter;fbi agent;strzok fraud;rigged investigation;investigation started;fraud rigged\n",
      "    Keyword weights: 0.1798;0.1798;0.1798;0.136;0.0812;0.0812;0.0812;0.0812\n",
      "    Strength: 0.095\n",
      "    Document IDs: 2654048984553148490 6817024102264760936 10234332166402996482\n",
      "    Document exposures: 0.253933 0.300756 0.44531\n",
      "---------------\n",
      "Topic 6 keywords:\n",
      "    Keywords: donald trump;trump dan;spygate attempted;service agent;secret service;sabotage donald;dan bongino;book spygate;author book;attempted sabotage;agent author\n",
      "    Keyword weights: 0.1329;0.0867;0.0867;0.0867;0.0867;0.0867;0.0867;0.0867;0.0867;0.0867;0.0867\n",
      "    Strength: 0.0657\n",
      "    Document IDs: 5983550775501810643 9220064279401475464 13206638373098679225 16578282956147621208 17901578373306652528\n",
      "    Document exposures: 0.09876 0.61478 0.08393 0.107363 0.095167\n",
      "---------------\n",
      "Topic 7 keywords:\n",
      "    Keywords: trump campaign;lou dobbs;evidence collusion;dobbs special;democrats evidence;conflicts angry;collusion trump;campaign russia;angry democrats\n",
      "    Keyword weights: 0.1231;0.1096;0.1096;0.1096;0.1096;0.1096;0.1096;0.1096;0.1096\n",
      "    Strength: 0.0963\n",
      "    Document IDs: 5336597423569459183 7290488938710124315 13115764764473553109 15091083406720907936\n",
      "    Document exposures: 0.045196 0.455348 0.044107 0.455348\n",
      "---------------\n"
     ]
    }
   ],
   "source": [
    "dataset = 'trump_tweets'\n",
    "#query = '(\"Trump\" OR \"president\")' # str | Fulltext query, using mysql MATCH boolean query format. Example, (\\\"word1\\\" OR \\\"word2\\\") AND (\\\"word3\\\" OR \\\"word4\\\") (optional)\n",
    "query = ''\n",
    "custom_stop_words = [\"real\",\"hillary\"] # str | List of stop words. (optional)\n",
    "num_topics = 8 # int | Number of topics to be extracted from the dataset. (optional) (default to 8)\n",
    "metadata_selection = \"\" # dict | JSON object specifying metadata-based queries on the dataset, of type {\"metadata_field\": \"selected_values\"} (optional)\n",
    "remove_redundancies = False # bool | If True, this option removes quasi-duplicates from the analysis. A quasi-duplicate would have the same NLP representation, but not necessarily the exact same text. (optional) (default True)\n",
    "\n",
    "try:\n",
    "    payload = nucleus_api.Topics(dataset=dataset,                                \n",
    "                                 query=query,                   \n",
    "                                 custom_stop_words=custom_stop_words,     \n",
    "                                 num_topics=num_topics,\n",
    "                                 metadata_selection=metadata_selection,\n",
    "                                 remove_redundancies=remove_redundancies)\n",
    "    api_response = api_instance.post_topic_api(payload)        \n",
    "except ApiException as e:\n",
    "    api_error = json.loads(e.body)\n",
    "    print('ERROR:', api_error['message'])\n",
    "    \n",
    "doc_ids = api_response.result.doc_ids\n",
    "topics = api_response.result.topics\n",
    "for i, res in enumerate(topics):\n",
    "    print('Topic', i, 'keywords:')\n",
    "    print('    Keywords:', res.keywords)\n",
    "    keywords_weight_str = \";\".join(str(x) for x in res.keywords_weight)\n",
    "    print('    Keyword weights:', keywords_weight_str)\n",
    "    print('    Strength:', res.strength)\n",
    "    doc_topic_exposure_sel = []  # list of non-zero doc_topic_exposure\n",
    "    doc_id_sel = []        # list of doc ids matching doc_topic_exposure_sel\n",
    "    for j in range(len(res.doc_topic_exposures)):\n",
    "        doc_topic_exp = float(res.doc_topic_exposures[j])\n",
    "        if doc_topic_exp != 0:\n",
    "            doc_topic_exposure_sel.append(doc_topic_exp)\n",
    "            doc_id_sel.append(doc_ids[j])\n",
    "    \n",
    "    doc_id_sel_str = ' '.join(str(x) for x in doc_id_sel)\n",
    "    doc_topic_exposure_sel_str = ' '.join(str(x) for x in doc_topic_exposure_sel)\n",
    "    print('    Document IDs:', doc_id_sel_str)\n",
    "    print('    Document exposures:', doc_topic_exposure_sel_str)\n",
    "    print('---------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate a summary for each topic identified in a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0 summary:\n",
      "    Keywords: illegal aliens;united permission;permission illegal;enter united;aliens treated;aliens illegal;treated entered;entered legally\n",
      "{'attribute': {'author': 'D_Trump57',\n",
      "               'counts': None,\n",
      "               'source': 'Twitter for iPhone',\n",
      "               'time': 1534339080},\n",
      " 'doc_id': '2692713684604938265',\n",
      " 'sentences': \"['People who enter the United States without our permission \"\n",
      "              'are illegal aliens and illegal aliens should not be treated the '\n",
      "              \"same as people who entered the US legally.']\",\n",
      " 'title': 'D_Trump2018_8_15_13_18'}\n",
      "    Document ID: 2692713684604938265\n",
      "        Title: D_Trump2018_8_15_13_18\n",
      "        Sentences: ['People who enter the United States without our permission are illegal aliens and illegal aliens should not be treated the same as people who entered the US legally.']\n",
      "        Author: D_Trump57\n",
      "        Time: 2018-08-15 06:18:00\n",
      "{'attribute': {'author': 'D_Trump63',\n",
      "               'counts': None,\n",
      "               'source': 'Twitter for iPhone',\n",
      "               'time': 1534337040},\n",
      " 'doc_id': '9742569426091859106',\n",
      " 'sentences': \"['People who enter the United States without our permission \"\n",
      "              'are illegal aliens and illegal aliens should not be treated the '\n",
      "              \"same as people who enters the US legally.']\",\n",
      " 'title': 'D_Trump2018_8_15_12_44'}\n",
      "    Document ID: 9742569426091859106\n",
      "        Title: D_Trump2018_8_15_12_44\n",
      "        Sentences: ['People who enter the United States without our permission are illegal aliens and illegal aliens should not be treated the same as people who enters the US legally.']\n",
      "        Author: D_Trump63\n",
      "        Time: 2018-08-15 05:44:00\n",
      "---------------\n",
      "Topic 1 summary:\n",
      "    Keywords: bruce ohr;ohr doj;legal jeopardy;doj legal;ohr justice;justice department;christopher steele;jeopardy astonishingthat;jeopardy astonishing;astonishingthat employed;astonishing employed\n",
      "{'attribute': {'author': 'D_Trump13',\n",
      "               'counts': None,\n",
      "               'source': 'Twitter for iPhone',\n",
      "               'time': 1534544940},\n",
      " 'doc_id': '14824073781424371042',\n",
      " 'sentences': \"['Fox News has learned that Bruce Ohr wrote Christopher Steele \"\n",
      "              'following the firing of James Comey saying that he was afraid '\n",
      "              \"the anti-Trump Russia probe will be exposed.']\",\n",
      " 'title': 'D_Trump2018_8_17_22_29'}\n",
      "    Document ID: 14824073781424371042\n",
      "        Title: D_Trump2018_8_17_22_29\n",
      "        Sentences: ['Fox News has learned that Bruce Ohr wrote Christopher Steele following the firing of James Comey saying that he was afraid the anti-Trump Russia probe will be exposed.']\n",
      "        Author: D_Trump13\n",
      "        Time: 2018-08-17 15:29:00\n",
      "{'attribute': {'author': 'D_Trump33',\n",
      "               'counts': None,\n",
      "               'source': 'Twitter for iPhone',\n",
      "               'time': 1534462620},\n",
      " 'doc_id': '6817024102264760936',\n",
      " 'sentences': \"['The FBI received documents from Bruce Ohr (of the Justice \"\n",
      "              \"Department  whose wife Nelly worked for Fusion GPS).']\",\n",
      " 'title': 'D_Trump2018_8_16_23_37'}\n",
      "    Document ID: 6817024102264760936\n",
      "        Title: D_Trump2018_8_16_23_37\n",
      "        Sentences: ['The FBI received documents from Bruce Ohr (of the Justice Department  whose wife Nelly worked for Fusion GPS).']\n",
      "        Author: D_Trump33\n",
      "        Time: 2018-08-16 16:37:00\n",
      "{'attribute': {'author': 'D_Trump70',\n",
      "               'counts': None,\n",
      "               'source': 'Twitter for iPhone',\n",
      "               'time': 1534247700},\n",
      " 'doc_id': '15178489020767442896',\n",
      " 'sentences': \"['Bruce Ohr of the Justice Department (can you believe he is \"\n",
      "              'still there) is accused of helping disgraced Christopher Steele '\n",
      "              \"find dirt on Trump.']\",\n",
      " 'title': 'D_Trump2018_8_14_11_55'}\n",
      "    Document ID: 15178489020767442896\n",
      "        Title: D_Trump2018_8_14_11_55\n",
      "        Sentences: ['Bruce Ohr of the Justice Department (can you believe he is still there) is accused of helping disgraced Christopher Steele find dirt on Trump.']\n",
      "        Author: D_Trump70\n",
      "        Time: 2018-08-14 04:55:00\n",
      "{'attribute': {'author': 'D_Trump11',\n",
      "               'counts': None,\n",
      "               'source': 'Twitter for iPhone',\n",
      "               'time': 1534550400},\n",
      " 'doc_id': '4437198275925723018',\n",
      " 'sentences': \"['Bruce Ohr of DOJ is in legal jeopardy its astonishing that \"\n",
      "              \"hes still employed.']\",\n",
      " 'title': 'D_Trump2018_8_18_1_46'}\n",
      "    Document ID: 4437198275925723018\n",
      "        Title: D_Trump2018_8_18_1_46\n",
      "        Sentences: ['Bruce Ohr of DOJ is in legal jeopardy its astonishing that hes still employed.']\n",
      "        Author: D_Trump11\n",
      "        Time: 2018-08-17 17:00:00\n",
      "{'attribute': {'author': 'D_Trump12',\n",
      "               'counts': None,\n",
      "               'source': 'Twitter for iPhone',\n",
      "               'time': 1534550400},\n",
      " 'doc_id': '7148720741304645705',\n",
      " 'sentences': \"['Bruce Ohr of DOJ is in legal jeopardy its astonishingthat \"\n",
      "              \"hes still employed.']\",\n",
      " 'title': 'D_Trump2018_8_18_1_37'}\n",
      "    Document ID: 7148720741304645705\n",
      "        Title: D_Trump2018_8_18_1_37\n",
      "        Sentences: ['Bruce Ohr of DOJ is in legal jeopardy its astonishingthat hes still employed.']\n",
      "        Author: D_Trump12\n",
      "        Time: 2018-08-17 17:00:00\n",
      "---------------\n",
      "Topic 2 summary:\n",
      "    Keywords: america great;statement america;andrew cuomo;great great;worse hightax;hightax andrew;deplorables statement;cuomo statement;clinton deplorables\n",
      "{'attribute': {'author': 'D_Trump18',\n",
      "               'counts': None,\n",
      "               'source': 'Twitter for iPhone',\n",
      "               'time': 1534515000},\n",
      " 'doc_id': '681266336486897660',\n",
      " 'sentences': \"['Big pushback on Governor Andrew Cuomo of New York for his \"\n",
      "              \"really dumb statement about Americas lack of greatness.']\",\n",
      " 'title': 'D_Trump2018_8_17_14_10'}\n",
      "    Document ID: 681266336486897660\n",
      "        Title: D_Trump2018_8_17_14_10\n",
      "        Sentences: ['Big pushback on Governor Andrew Cuomo of New York for his really dumb statement about Americas lack of greatness.']\n",
      "        Author: D_Trump18\n",
      "        Time: 2018-08-17 07:10:00\n",
      "{'attribute': {'author': 'D_Trump24',\n",
      "               'counts': None,\n",
      "               'source': 'Twitter for iPhone',\n",
      "               'time': 1534506240},\n",
      " 'doc_id': '9687935645391984655',\n",
      " 'sentences': \"['How does a politician Cuomo known for pushing people and \"\n",
      "              'businesses out of his state not to mention having the highest '\n",
      "              'taxes in the US survive making the statement WERE NOT GOING TO '\n",
      "              \"MAKE AMERICA GREAT AGAIN IT WAS NEVER THAT GREAT?']\",\n",
      " 'title': 'D_Trump2018_8_17_11_44'}\n",
      "    Document ID: 9687935645391984655\n",
      "        Title: D_Trump2018_8_17_11_44\n",
      "        Sentences: ['How does a politician Cuomo known for pushing people and businesses out of his state not to mention having the highest taxes in the US survive making the statement WERE NOT GOING TO MAKE AMERICA GREAT AGAIN IT WAS NEVER THAT GREAT?']\n",
      "        Author: D_Trump24\n",
      "        Time: 2018-08-17 04:44:00\n",
      "{'attribute': {'author': 'D_Trump16',\n",
      "               'counts': None,\n",
      "               'source': 'Twitter for iPhone',\n",
      "               'time': 1534533900},\n",
      " 'doc_id': '4046653213651213725',\n",
      " 'sentences': '[\"Which is worse Hightax Andrew Cuomo\\'s statement WERE NOT '\n",
      "              'GOING TO MAKE AMERICA GREAT AGAIN IT WAS NEVER THAT GREAT or '\n",
      "              'Hillary Clintons DEPLORABLES statement...\"]',\n",
      " 'title': 'D_Trump2018_8_17_19_25'}\n",
      "    Document ID: 4046653213651213725\n",
      "        Title: D_Trump2018_8_17_19_25\n",
      "        Sentences: [\"Which is worse Hightax Andrew Cuomo's statement WERE NOT GOING TO MAKE AMERICA GREAT AGAIN IT WAS NEVER THAT GREAT or Hillary Clintons DEPLORABLES statement...\"]\n",
      "        Author: D_Trump16\n",
      "        Time: 2018-08-17 12:25:00\n",
      "{'attribute': {'author': 'D_Trump17',\n",
      "               'counts': None,\n",
      "               'source': 'Twitter for iPhone',\n",
      "               'time': 1534515420},\n",
      " 'doc_id': '4349543959441164718',\n",
      " 'sentences': \"['When a politician admits that Were not going to make \"\n",
      "              'America great again there doesnt seem to be much reason to '\n",
      "              \"ever vote for him.', 'This could be a career threatening \"\n",
      "              'statement by Andrew Cuomo with many wanting him to resign-he '\n",
      "              \"will get higher ratings than his brother Chris!']\",\n",
      " 'title': 'D_Trump2018_8_17_14_17'}\n",
      "    Document ID: 4349543959441164718\n",
      "        Title: D_Trump2018_8_17_14_17\n",
      "        Sentences: ['When a politician admits that Were not going to make America great again there doesnt seem to be much reason to ever vote for him.', 'This could be a career threatening statement by Andrew Cuomo with many wanting him to resign-he will get higher ratings than his brother Chris!']\n",
      "        Author: D_Trump17\n",
      "        Time: 2018-08-17 07:17:00\n",
      "---------------\n",
      "Topic 3 summary:\n",
      "    Keywords: peter strzok;fired fbi;agent peter;fbi agent;strzok fraud;rigged investigation;investigation started;fraud rigged\n",
      "{'attribute': {'author': 'D_Trump68',\n",
      "               'counts': None,\n",
      "               'source': 'Twitter for iPhone',\n",
      "               'time': 1534251660},\n",
      " 'doc_id': '10234332166402996482',\n",
      " 'sentences': \"['Fired FBI Agent Peter Strzok is a fraud as is the rigged \"\n",
      "              \"investigation he started.']\",\n",
      " 'title': 'D_Trump2018_8_14_13_1'}\n",
      "    Document ID: 10234332166402996482\n",
      "        Title: D_Trump2018_8_14_13_1\n",
      "        Sentences: ['Fired FBI Agent Peter Strzok is a fraud as is the rigged investigation he started.']\n",
      "        Author: D_Trump68\n",
      "        Time: 2018-08-14 06:01:00\n",
      "{'attribute': {'author': 'D_Trump33',\n",
      "               'counts': None,\n",
      "               'source': 'Twitter for iPhone',\n",
      "               'time': 1534462620},\n",
      " 'doc_id': '6817024102264760936',\n",
      " 'sentences': \"['Disgraced and fired FBI Agent Peter Strzok.']\",\n",
      " 'title': 'D_Trump2018_8_16_23_37'}\n",
      "    Document ID: 6817024102264760936\n",
      "        Title: D_Trump2018_8_16_23_37\n",
      "        Sentences: ['Disgraced and fired FBI Agent Peter Strzok.']\n",
      "        Author: D_Trump33\n",
      "        Time: 2018-08-16 16:37:00\n",
      "{'attribute': {'author': 'D_Trump91',\n",
      "               'counts': None,\n",
      "               'source': 'Twitter for iPhone',\n",
      "               'time': 1534176240},\n",
      " 'doc_id': '2654048984553148490',\n",
      " 'sentences': \"['Agent Peter Strzok was just fired from the FBI - finally.']\",\n",
      " 'title': 'D_Trump2018_8_13_16_4'}\n",
      "    Document ID: 2654048984553148490\n",
      "        Title: D_Trump2018_8_13_16_4\n",
      "        Sentences: ['Agent Peter Strzok was just fired from the FBI - finally.']\n",
      "        Author: D_Trump91\n",
      "        Time: 2018-08-13 09:04:00\n",
      "---------------\n",
      "Topic 4 summary:\n",
      "    Keywords: trump campaign;lou dobbs;evidence collusion;dobbs special;democrats evidence;conflicts angry;collusion trump;campaign russia;angry democrats\n",
      "{'attribute': {'author': 'D_Trump66',\n",
      "               'counts': None,\n",
      "               'source': 'Twitter for iPhone',\n",
      "               'time': 1534252500},\n",
      " 'doc_id': '7290488938710124315',\n",
      " 'sentences': \"['Lou Dobbs: This cannot go forward...this Special Counsel \"\n",
      "              'with all of his conflicts with his 17 Angry Democrats without '\n",
      "              \"any evidence of collusion by the Trump Campaign and Russia.']\",\n",
      " 'title': 'D_Trump2018_8_14_13_15'}\n",
      "    Document ID: 7290488938710124315\n",
      "        Title: D_Trump2018_8_14_13_15\n",
      "        Sentences: ['Lou Dobbs: This cannot go forward...this Special Counsel with all of his conflicts with his 17 Angry Democrats without any evidence of collusion by the Trump Campaign and Russia.']\n",
      "        Author: D_Trump66\n",
      "        Time: 2018-08-14 06:15:00\n",
      "{'attribute': {'author': 'D_Trump27',\n",
      "               'counts': None,\n",
      "               'source': 'Twitter for iPhone',\n",
      "               'time': 1534464000},\n",
      " 'doc_id': '13115764764473553109',\n",
      " 'sentences': \"['Director Brennans recent statements purport to know as fact \"\n",
      "              \"that the Trump campaign colluded with a foreign power.']\",\n",
      " 'title': 'D_Trump2018_8_17_1_54'}\n",
      "    Document ID: 13115764764473553109\n",
      "        Title: D_Trump2018_8_17_1_54\n",
      "        Sentences: ['Director Brennans recent statements purport to know as fact that the Trump campaign colluded with a foreign power.']\n",
      "        Author: D_Trump27\n",
      "        Time: 2018-08-16 17:00:00\n",
      "{'attribute': {'author': 'D_Trump43',\n",
      "               'counts': None,\n",
      "               'source': 'Twitter for iPhone',\n",
      "               'time': 1534377600},\n",
      " 'doc_id': '5336597423569459183',\n",
      " 'sentences': \"['Mark Levin When they had power they didnt stop the Russians \"\n",
      "              'the Chinese the North Koreans they funded the Iranians  are '\n",
      "              'responsible for the greatest scandal in American history by '\n",
      "              'interfering with our election  trying to undermine the Trump '\n",
      "              \"Campaign and Trump Presidency.']\",\n",
      " 'title': 'D_Trump2018_8_16_2_31'}\n",
      "    Document ID: 5336597423569459183\n",
      "        Title: D_Trump2018_8_16_2_31\n",
      "        Sentences: ['Mark Levin When they had power they didnt stop the Russians the Chinese the North Koreans they funded the Iranians  are responsible for the greatest scandal in American history by interfering with our election  trying to undermine the Trump Campaign and Trump Presidency.']\n",
      "        Author: D_Trump43\n",
      "        Time: 2018-08-15 17:00:00\n",
      "{'attribute': {'author': 'D_Trump73',\n",
      "               'counts': None,\n",
      "               'source': 'Twitter for iPhone',\n",
      "               'time': 1534245660},\n",
      " 'doc_id': '15091083406720907936',\n",
      " 'sentences': \"['Lou Dobbs: This cannot go forward...this Special Councel \"\n",
      "              'with all of his conflicts with his 17 Angry Democrats without '\n",
      "              \"any evidence of collusion by the Trump Campaign and Russia.']\",\n",
      " 'title': 'D_Trump2018_8_14_11_21'}\n",
      "    Document ID: 15091083406720907936\n",
      "        Title: D_Trump2018_8_14_11_21\n",
      "        Sentences: ['Lou Dobbs: This cannot go forward...this Special Councel with all of his conflicts with his 17 Angry Democrats without any evidence of collusion by the Trump Campaign and Russia.']\n",
      "        Author: D_Trump73\n",
      "        Time: 2018-08-14 04:21:00\n",
      "---------------\n",
      "Topic 5 summary:\n",
      "    Keywords: witch hunt;donald trump;rigged witch;frame donald;unfortunate situation;trump rigged;situation decided;decided frame\n",
      "{'attribute': {'author': 'D_Trump69',\n",
      "               'counts': None,\n",
      "               'source': 'Twitter for iPhone',\n",
      "               'time': 1534248360},\n",
      " 'doc_id': '16578282956147621208',\n",
      " 'sentences': \"['They were all in on it clear Hillary Clinton and FRAME \"\n",
      "              \"Donald Trump for things he didnt do.']\",\n",
      " 'title': 'D_Trump2018_8_14_12_6'}\n",
      "    Document ID: 16578282956147621208\n",
      "        Title: D_Trump2018_8_14_12_6\n",
      "        Sentences: ['They were all in on it clear Hillary Clinton and FRAME Donald Trump for things he didnt do.']\n",
      "        Author: D_Trump69\n",
      "        Time: 2018-08-14 05:06:00\n",
      "{'attribute': {'author': 'D_Trump47',\n",
      "               'counts': None,\n",
      "               'source': 'Twitter for iPhone',\n",
      "               'time': 1534377600},\n",
      " 'doc_id': '9220064279401475464',\n",
      " 'sentences': \"['Former Secret Service Agent and author of new book Spygate \"\n",
      "              \"the Attempted Sabotage of Donald J Trump Dan Bongino.']\",\n",
      " 'title': 'D_Trump2018_8_16_1_0'}\n",
      "    Document ID: 9220064279401475464\n",
      "        Title: D_Trump2018_8_16_1_0\n",
      "        Sentences: ['Former Secret Service Agent and author of new book Spygate the Attempted Sabotage of Donald J Trump Dan Bongino.']\n",
      "        Author: D_Trump47\n",
      "        Time: 2018-08-15 17:00:00\n",
      "{'attribute': {'author': 'D_Trump91',\n",
      "               'counts': None,\n",
      "               'source': 'Twitter for iPhone',\n",
      "               'time': 1534176240},\n",
      " 'doc_id': '2654048984553148490',\n",
      " 'sentences': \"['Based on the fact that Strzok was in charge of the Witch Hunt \"\n",
      "              \"will it be dropped?']\",\n",
      " 'title': 'D_Trump2018_8_13_16_4'}\n",
      "    Document ID: 2654048984553148490\n",
      "        Title: D_Trump2018_8_13_16_4\n",
      "        Sentences: ['Based on the fact that Strzok was in charge of the Witch Hunt will it be dropped?']\n",
      "        Author: D_Trump91\n",
      "        Time: 2018-08-13 09:04:00\n",
      "{'attribute': {'author': 'D_Trump53',\n",
      "               'counts': None,\n",
      "               'source': 'Twitter for iPhone',\n",
      "               'time': 1534342080},\n",
      " 'doc_id': '3295270030761385310',\n",
      " 'sentences': \"['The Rigged Russian Witch Hunt goes on and on as the \"\n",
      "              'originators and founders of this scam continue to be fired '\n",
      "              \"and demoted for their corrupt and illegal activity.']\",\n",
      " 'title': 'D_Trump2018_8_15_14_8'}\n",
      "    Document ID: 3295270030761385310\n",
      "        Title: D_Trump2018_8_15_14_8\n",
      "        Sentences: ['The Rigged Russian Witch Hunt goes on and on as the originators and founders of this scam continue to be fired and demoted for their corrupt and illegal activity.']\n",
      "        Author: D_Trump53\n",
      "        Time: 2018-08-15 07:08:00\n",
      "{'attribute': {'author': 'D_Trump46',\n",
      "               'counts': None,\n",
      "               'source': 'Twitter for iPhone',\n",
      "               'time': 1534377600},\n",
      " 'doc_id': '5983550775501810643',\n",
      " 'sentences': \"['We have the unfortunate situation where they then decided \"\n",
      "              'they were going to frame Donald Trump concerning the Rigged '\n",
      "              \"Witch Hunt.']\",\n",
      " 'title': 'D_Trump2018_8_16_1_14'}\n",
      "    Document ID: 5983550775501810643\n",
      "        Title: D_Trump2018_8_16_1_14\n",
      "        Sentences: ['We have the unfortunate situation where they then decided they were going to frame Donald Trump concerning the Rigged Witch Hunt.']\n",
      "        Author: D_Trump46\n",
      "        Time: 2018-08-15 17:00:00\n",
      "{'attribute': {'author': 'D_Trump99',\n",
      "               'counts': None,\n",
      "               'source': 'Twitter for iPhone',\n",
      "               'time': 1534076640},\n",
      " 'doc_id': '13206638373098679225',\n",
      " 'sentences': \"['@JudgeJeanine  Bob Mueller isnt your whole investigation \"\n",
      "              'premised on a Fake Dossier paid for by Hillary created by a man '\n",
      "              \"who hates Donald Trump  used to con a FISA Court Judge.']\",\n",
      " 'title': 'D_Trump2018_8_12_12_24'}\n",
      "    Document ID: 13206638373098679225\n",
      "        Title: D_Trump2018_8_12_12_24\n",
      "        Sentences: ['@JudgeJeanine  Bob Mueller isnt your whole investigation premised on a Fake Dossier paid for by Hillary created by a man who hates Donald Trump  used to con a FISA Court Judge.']\n",
      "        Author: D_Trump99\n",
      "        Time: 2018-08-12 05:24:00\n",
      "---------------\n",
      "Topic 6 summary:\n",
      "    Keywords: fake news;news media;wacky omarosa;presidential lowlife;overtime wacky;omarosa modern;omarosa legitimate;modern form;media overtime;lowlife omarosa;form communication;communication fake\n",
      "{'attribute': {'author': 'D_Trump38',\n",
      "               'counts': None,\n",
      "               'source': 'Twitter for iPhone',\n",
      "               'time': 1534428600},\n",
      " 'doc_id': '14368578770105844206',\n",
      " 'sentences': \"['The fact is that the Press is FREE to write and say anything \"\n",
      "              'it wants but much of what it says is FAKE NEWS pushing a '\n",
      "              \"political agenda or just plain trying to hurt people.']\",\n",
      " 'title': 'D_Trump2018_8_16_14_10'}\n",
      "    Document ID: 14368578770105844206\n",
      "        Title: D_Trump2018_8_16_14_10\n",
      "        Sentences: ['The fact is that the Press is FREE to write and say anything it wants but much of what it says is FAKE NEWS pushing a political agenda or just plain trying to hurt people.']\n",
      "        Author: D_Trump38\n",
      "        Time: 2018-08-16 07:10:00\n",
      "{'attribute': {'author': 'D_Trump89',\n",
      "               'counts': None,\n",
      "               'source': 'Twitter for iPhone',\n",
      "               'time': 1534176780},\n",
      " 'doc_id': '14171720646613873024',\n",
      " 'sentences': \"['Wacky Omarosa already has a fully signed Non-Disclosure \"\n",
      "              \"Agreement!']\",\n",
      " 'title': 'D_Trump2018_8_13_16_13'}\n",
      "    Document ID: 14171720646613873024\n",
      "        Title: D_Trump2018_8_13_16_13\n",
      "        Sentences: ['Wacky Omarosa already has a fully signed Non-Disclosure Agreement!']\n",
      "        Author: D_Trump89\n",
      "        Time: 2018-08-13 09:13:00\n",
      "{'attribute': {'author': 'D_Trump8',\n",
      "               'counts': None,\n",
      "               'source': 'Twitter for iPhone',\n",
      "               'time': 1534591920},\n",
      " 'doc_id': '1436959758999674726',\n",
      " 'sentences': \"['If you are weeding out Fake News there is nothing so Fake as \"\n",
      "              'CNN  MSNBC  yet I do not ask that their sick behavior be '\n",
      "              \"removed.']\",\n",
      " 'title': 'D_Trump2018_8_18_11_32'}\n",
      "    Document ID: 1436959758999674726\n",
      "        Title: D_Trump2018_8_18_11_32\n",
      "        Sentences: ['If you are weeding out Fake News there is nothing so Fake as CNN  MSNBC  yet I do not ask that their sick behavior be removed.']\n",
      "        Author: D_Trump8\n",
      "        Time: 2018-08-18 04:32:00\n",
      "{'attribute': {'author': 'D_Trump93',\n",
      "               'counts': None,\n",
      "               'source': 'Twitter for iPhone',\n",
      "               'time': 1534170060},\n",
      " 'doc_id': '3946302249027831785',\n",
      " 'sentences': \"['While I know its not presidential to take on a lowlife \"\n",
      "              'like Omarosa and while I would rather not be doing so this is a '\n",
      "              'modern day form of communication and I know the Fake News Media '\n",
      "              'will be working overtime to make even Wacky Omarosa look '\n",
      "              \"legitimate as possible.']\",\n",
      " 'title': 'D_Trump2018_8_13_14_21'}\n",
      "    Document ID: 3946302249027831785\n",
      "        Title: D_Trump2018_8_13_14_21\n",
      "        Sentences: ['While I know its not presidential to take on a lowlife like Omarosa and while I would rather not be doing so this is a modern day form of communication and I know the Fake News Media will be working overtime to make even Wacky Omarosa look legitimate as possible.']\n",
      "        Author: D_Trump93\n",
      "        Time: 2018-08-13 07:21:00\n",
      "{'attribute': {'author': 'D_Trump40',\n",
      "               'counts': None,\n",
      "               'source': 'Twitter for iPhone',\n",
      "               'time': 1534423800},\n",
      " 'doc_id': '12437126850273027242',\n",
      " 'sentences': \"['THE FAKE NEWS MEDIA IS THE OPPOSITION PARTY.']\",\n",
      " 'title': 'D_Trump2018_8_16_12_50'}\n",
      "    Document ID: 12437126850273027242\n",
      "        Title: D_Trump2018_8_16_12_50\n",
      "        Sentences: ['THE FAKE NEWS MEDIA IS THE OPPOSITION PARTY.']\n",
      "        Author: D_Trump40\n",
      "        Time: 2018-08-16 05:50:00\n",
      "{'attribute': {'author': 'D_Trump95',\n",
      "               'counts': None,\n",
      "               'source': 'Twitter for iPhone',\n",
      "               'time': 1534166820},\n",
      " 'doc_id': '9740888651068443724',\n",
      " 'sentences': \"['Wacky Omarosa who got fired 3 times on the Apprentice now got \"\n",
      "              \"fired for the last time.']\",\n",
      " 'title': 'D_Trump2018_8_13_13_27'}\n",
      "    Document ID: 9740888651068443724\n",
      "        Title: D_Trump2018_8_13_13_27\n",
      "        Sentences: ['Wacky Omarosa who got fired 3 times on the Apprentice now got fired for the last time.']\n",
      "        Author: D_Trump95\n",
      "        Time: 2018-08-13 06:27:00\n",
      "---------------\n",
      "Topic 7 summary:\n",
      "    Keywords: john brennan;cia director;tuckercarlson speaking;speaking john;limited intellectually;intellectually cia;director good;brennan limited\n",
      "{'attribute': {'author': 'D_Trump47',\n",
      "               'counts': None,\n",
      "               'source': 'Twitter for iPhone',\n",
      "               'time': 1534377600},\n",
      " 'doc_id': '9220064279401475464',\n",
      " 'sentences': \"['John Brennan is a stain on the Country we deserve better \"\n",
      "              \"than this.']\",\n",
      " 'title': 'D_Trump2018_8_16_1_0'}\n",
      "    Document ID: 9220064279401475464\n",
      "        Title: D_Trump2018_8_16_1_0\n",
      "        Sentences: ['John Brennan is a stain on the Country we deserve better than this.']\n",
      "        Author: D_Trump47\n",
      "        Time: 2018-08-15 17:00:00\n",
      "{'attribute': {'author': 'D_Trump30',\n",
      "               'counts': None,\n",
      "               'source': 'Twitter for iPhone',\n",
      "               'time': 1534464000},\n",
      " 'doc_id': '7508075727870080337',\n",
      " 'sentences': \"['@TuckerCarlson speaking of John Brennan: How did somebody so \"\n",
      "              'obviously limited intellectually get to be CIA Director in the '\n",
      "              \"first place? Now that is a really good question!']\",\n",
      " 'title': 'D_Trump2018_8_17_0_45'}\n",
      "    Document ID: 7508075727870080337\n",
      "        Title: D_Trump2018_8_17_0_45\n",
      "        Sentences: ['@TuckerCarlson speaking of John Brennan: How did somebody so obviously limited intellectually get to be CIA Director in the first place? Now that is a really good question!']\n",
      "        Author: D_Trump30\n",
      "        Time: 2018-08-16 17:00:00\n",
      "{'attribute': {'author': 'D_Trump4',\n",
      "               'counts': None,\n",
      "               'source': 'Twitter for iPhone',\n",
      "               'time': 1534597920},\n",
      " 'doc_id': '427773076591013776',\n",
      " 'sentences': \"['Has anyone looked at the mistakes that John Brennan made \"\n",
      "              \"while serving as CIA Director?']\",\n",
      " 'title': 'D_Trump2018_8_18_13_12'}\n",
      "    Document ID: 427773076591013776\n",
      "        Title: D_Trump2018_8_18_13_12\n",
      "        Sentences: ['Has anyone looked at the mistakes that John Brennan made while serving as CIA Director?']\n",
      "        Author: D_Trump4\n",
      "        Time: 2018-08-18 06:12:00\n",
      "---------------\n"
     ]
    }
   ],
   "source": [
    "dataset = 'trump_tweets'\n",
    "#query = '(\"Trump\" OR \"president\")' # str | Fulltext query, using mysql MATCH boolean query format. Example, (\\\"word1\\\" OR \\\"word2\\\") AND (\\\"word3\\\" OR \\\"word4\\\") (optional)\n",
    "query = ''\n",
    "custom_stop_words = [\"real\",\"hillary\"] # str | List of stop words. (optional)\n",
    "num_topics = 8 # int | Number of topics to be extracted from the dataset. (optional) (default to 8)\n",
    "num_keywords = 8 # int | Number of keywords per topic that is extracted from the dataset. (optional) (default to 8)\n",
    "summary_length = 6 # int | The maximum number of bullet points a user wants to see in each topic summary. (optional) (default to 6)\n",
    "context_amount = 0 # int | The number of sentences surrounding key summary sentences in the documents that they come from. (optional) (default to 0)\n",
    "num_docs = 20 # int | The maximum number of key documents to use for summarization. (optional) (default to 20)\n",
    "excluded_docs = '' # str | List of document IDs that should be excluded from the analysis. Example, [\"docid1\", \"docid2\", ..., \"docidN\"]  (optional)\n",
    "remove_redundancies = False # bool | If True, this option removes quasi-duplicates from the analysis. A quasi-duplicate would have the same NLP representation, but not necessarily the exact same text. (optional) (default True)\n",
    "\n",
    "metadata_selection =\"\" # dict | JSON object specifying metadata-based queries on the dataset, of type {\"metadata_field\": \"selected_values\"} (optional)\n",
    "time_period = \"\"     # str | Time period selection. Choices: [\"1M\",\"3M\",\"6M\",\"12M\",\"3Y\",\"5Y\",\"\"]  (optional)\n",
    "period_start = \"\" # str | Start date for the period to analyze within the dataset. Format: \"YYYY-MM-DD HH:MM:SS\"\n",
    "period_end = \"\" # str | End date for the period to analyze within the dataset. Format: \"YYYY-MM-DD HH:MM:SS\"\n",
    "api_response = None\n",
    "\n",
    "try:\n",
    "    payload = nucleus_api.TopicSummaryModel\t(dataset=dataset, \n",
    "                                             query=query,\n",
    "                                             custom_stop_words=custom_stop_words, \n",
    "                                             num_topics=num_topics, \n",
    "                                             num_keywords=num_keywords,\n",
    "                                             metadata_selection=metadata_selection,\n",
    "                                             summary_length=summary_length, \n",
    "                                             context_amount=context_amount, \n",
    "                                             num_docs=num_docs)\n",
    "    api_response = api_instance.post_topic_summary_api(payload)\n",
    "    api_ok = True\n",
    "except ApiException as e:\n",
    "    api_error = json.loads(e.body)\n",
    "    print('ERROR:', api_error['message'])\n",
    "    api_ok = False\n",
    "\n",
    "if api_ok:\n",
    "    for i,res in enumerate(api_response.result):\n",
    "        print('Topic', i, 'summary:')\n",
    "        print('    Keywords:', res.keywords)\n",
    "        for j in range(len(res.summary)):\n",
    "            print(res.summary[j])\n",
    "            print('    Document ID:', res.summary[j].doc_id)\n",
    "            print('        Title:', res.summary[j].title)\n",
    "            print('        Sentences:', res.summary[j].sentences)\n",
    "            print('        Author:', res.summary[j].attribute['author'])\n",
    "            print('        Time:', datetime.datetime.fromtimestamp(float(res.summary[j].attribute['time'])))\n",
    "        print('---------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measure the sentiment on each topic identified in a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0 sentiment:\n",
      "    Keywords: illegal aliens;united permission;permission illegal;enter united;aliens treated;aliens illegal;treated entered;entered legally\n",
      "    Sentiment: 0.0\n",
      "    Strength: 0.1322\n",
      "    Document IDs: 2692713684604938265 9742569426091859106\n",
      "    Document Sentiments: 0.0 0.0\n",
      "    Document Exposures: [0.527689, 0.472311]\n",
      "---------------\n",
      "Topic 1 sentiment:\n",
      "    Keywords: bruce ohr;ohr doj;legal jeopardy;doj legal;ohr justice;justice department;christopher steele;jeopardy astonishingthat;jeopardy astonishing;astonishingthat employed;astonishing employed\n",
      "    Sentiment: 0.0272\n",
      "    Strength: 0.1671\n",
      "    Document IDs: 4437198275925723018 5294521276627635033 6817024102264760936 7148720741304645705 14824073781424371042 15178489020767442896\n",
      "    Document Sentiments: 0.0 0.133333 0.0 0.0 0.0 0.090909\n",
      "    Document Exposures: [0.279985, 0.116364, 0.116997, 0.279985, 0.077244, 0.129425]\n",
      "---------------\n",
      "Topic 2 sentiment:\n",
      "    Keywords: america great;statement america;andrew cuomo;great great;worse hightax;hightax andrew;deplorables statement;cuomo statement;clinton deplorables\n",
      "    Sentiment: 0.1737\n",
      "    Strength: 0.1278\n",
      "    Document IDs: 681266336486897660 2473572527244144057 4046653213651213725 4349543959441164718 9687935645391984655\n",
      "    Document Sentiments: 0.133333 0.0 0.181818 0.153846 0.272727\n",
      "    Document Exposures: [0.16757, 0.07648, 0.447117, 0.118358, 0.190475]\n",
      "---------------\n",
      "Topic 3 sentiment:\n",
      "    Keywords: peter strzok;fired fbi;agent peter;fbi agent;strzok fraud;rigged investigation;investigation started;fraud rigged\n",
      "    Sentiment: 0.0\n",
      "    Strength: 0.1237\n",
      "    Document IDs: 2654048984553148490 6817024102264760936 10234332166402996482\n",
      "    Document Sentiments: 0.0 0.0 0.0\n",
      "    Document Exposures: [0.268977, 0.336792, 0.394231]\n",
      "---------------\n",
      "Topic 4 sentiment:\n",
      "    Keywords: trump campaign;lou dobbs;evidence collusion;dobbs special;democrats evidence;conflicts angry;collusion trump;campaign russia;angry democrats\n",
      "    Sentiment: 0.1869\n",
      "    Strength: 0.1255\n",
      "    Document IDs: 5336597423569459183 7290488938710124315 13115764764473553109 15091083406720907936\n",
      "    Document Sentiments: 0.2 0.181818 0.25 0.181818\n",
      "    Document Exposures: [0.04085, 0.44728, 0.06459, 0.44728]\n",
      "---------------\n",
      "Topic 5 sentiment:\n",
      "    Keywords: witch hunt;donald trump;rigged witch;frame donald;unfortunate situation;trump rigged;situation decided;decided frame\n",
      "    Sentiment: 0.1254\n",
      "    Strength: 0.1501\n",
      "    Document IDs: 2654048984553148490 3295270030761385310 5983550775501810643 9220064279401475464 13125097700312206949 13206638373098679225 16578282956147621208 17901578373306652528\n",
      "    Document Sentiments: 0.0 0.0 0.25 0.181818 0.0 0.133333 -0.090909 0.333333\n",
      "    Document Exposures: [0.086977, 0.056143, 0.384203, 0.05864, 0.125594, 0.050216, 0.158829, 0.079399]\n",
      "---------------\n",
      "Topic 6 sentiment:\n",
      "    Keywords: fake news;news media;wacky omarosa;presidential lowlife;overtime wacky;omarosa modern;omarosa legitimate;modern form;media overtime;lowlife omarosa;form communication;communication fake\n",
      "    Sentiment: 0.0\n",
      "    Strength: 0.0901\n",
      "    Document IDs: 1436959758999674726 3946302249027831785 9740888651068443724 12437126850273027242 14171720646613873024 14368578770105844206\n",
      "    Document Sentiments: 0.0 0.0 0.0 0.0 0.0 0.0\n",
      "    Document Exposures: [0.071105, 0.520905, 0.068085, 0.185412, 0.083387, 0.071105]\n",
      "---------------\n",
      "Topic 7 sentiment:\n",
      "    Keywords: john brennan;cia director;tuckercarlson speaking;speaking john;limited intellectually;intellectually cia;director good;brennan limited\n",
      "    Sentiment: 0.0\n",
      "    Strength: 0.0831\n",
      "    Document IDs: 427773076591013776 7508075727870080337 9220064279401475464\n",
      "    Document Sentiments: 0.0 0.0 0.0\n",
      "    Document Exposures: [0.241396, 0.618422, 0.140182]\n",
      "---------------\n"
     ]
    }
   ],
   "source": [
    "dataset = 'trump_tweets' # str | Dataset name\n",
    "#query = '(\"Trump\" OR \"president\")' # str | Fulltext query, using mysql MATCH boolean query format. Example, (\\\"word1\\\" OR \\\"word2\\\") AND (\\\"word3\\\" OR \\\"word4\\\") (optional)\n",
    "query = ''\n",
    "custom_stop_words = [\"real\",\"hillary\"] # str | List of stop words. (optional)\n",
    "num_topics = 8 # int | Number of topics to be extracted from the dataset. (optional) (default to 8)\n",
    "num_keywords = 8 # int | Number of keywords per topic that is extracted from the dataset. (optional) (default to 8)\n",
    "excluded_docs = '' # str | List of document IDs that should be excluded from the analysis. Example, [\"docid1\", \"docid2\", ..., \"docidN\"]  (optional)\n",
    "custom_dict_file = {\"great\": 1.0, \"awful\": -1.0, \"clinton\":-1.0, \"trump\":1.0} # file | Custom sentiment dictionary JSON file. Example, {\"field1\": value1, ..., \"fieldN\": valueN} (optional)\n",
    "remove_redundancies = False # bool | If True, this option removes quasi-duplicates from the analysis. A quasi-duplicate would have the same NLP representation, but not necessarily the exact same text. (optional) (default True)\n",
    "\n",
    "metadata_selection =\"\" # dict | JSON object specifying metadata-based queries on the dataset, of type {\"metadata_field\": \"selected_values\"} (optional)\n",
    "time_period = \"\"     # str | Time period selection. Choices: [\"1M\",\"3M\",\"6M\",\"12M\",\"3Y\",\"5Y\",\"\"] (optional)\n",
    "period_start = \"\" # str | Start date for the period to analyze within the dataset. Format: \"YYYY-MM-DD HH:MM:SS\"\n",
    "period_end = \"\" # str | End date for the period to analyze within the dataset. Format: \"YYYY-MM-DD HH:MM:SS\"\n",
    "\n",
    "try:\n",
    "    payload = nucleus_api.TopicSentimentModel(dataset=dataset, \n",
    "                                              query=query, \n",
    "                                              custom_stop_words=custom_stop_words, \n",
    "                                              num_topics=num_topics, \n",
    "                                              num_keywords=num_keywords,\n",
    "                                              custom_dict_file=custom_dict_file)\n",
    "    api_response = api_instance.post_topic_sentiment_api(payload)\n",
    "    \n",
    "except ApiException as e:\n",
    "    api_error = json.loads(e.body)\n",
    "    print('ERROR:', api_error['message'])\n",
    "\n",
    "for i,res in enumerate(api_response.result):\n",
    "    print('Topic', i, 'sentiment:')\n",
    "    print('    Keywords:', res.keywords)\n",
    "    print('    Sentiment:', res.sentiment)\n",
    "    print('    Strength:', res.strength)\n",
    "    \n",
    "    doc_id_str = ' '.join(str(x) for x in res.doc_ids)\n",
    "    doc_sentiment_str = ' '.join(str(x) for x in res.doc_sentiments)\n",
    "    doc_score_str = ' '.join(str(x) for x in res.doc_topic_exposures)\n",
    "    print('    Document IDs:', doc_id_str)\n",
    "    print('    Document Sentiments:', doc_sentiment_str)\n",
    "    print('    Document Exposures:', doc_score_str)\n",
    "    print('---------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measure the consensus on each topic identified in a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0 consensus:\n",
      "    Keywords: illegal aliens;united permission;permission illegal;enter united;aliens treated;aliens illegal;treated entered;entered legally\n",
      "    Consensus: 1.0\n",
      "    Strength: 0.1322\n",
      "---------------\n",
      "Topic 1 consensus:\n",
      "    Keywords: bruce ohr;ohr doj;legal jeopardy;doj legal;ohr justice;justice department;christopher steele;jeopardy astonishingthat;jeopardy astonishing;astonishingthat employed;astonishing employed\n",
      "    Consensus: 0.7542\n",
      "    Strength: 0.1671\n",
      "---------------\n",
      "Topic 2 consensus:\n",
      "    Keywords: america great;statement america;andrew cuomo;great great;worse hightax;hightax andrew;deplorables statement;cuomo statement;clinton deplorables\n",
      "    Consensus: 0.9235\n",
      "    Strength: 0.1278\n",
      "---------------\n",
      "Topic 3 consensus:\n",
      "    Keywords: peter strzok;fired fbi;agent peter;fbi agent;strzok fraud;rigged investigation;investigation started;fraud rigged\n",
      "    Consensus: 1.0\n",
      "    Strength: 0.1237\n",
      "---------------\n",
      "Topic 4 consensus:\n",
      "    Keywords: trump campaign;lou dobbs;evidence collusion;dobbs special;democrats evidence;conflicts angry;collusion trump;campaign russia;angry democrats\n",
      "    Consensus: 1.0\n",
      "    Strength: 0.1255\n",
      "---------------\n",
      "Topic 5 consensus:\n",
      "    Keywords: witch hunt;donald trump;rigged witch;frame donald;unfortunate situation;trump rigged;situation decided;decided frame\n",
      "    Consensus: 0.5724\n",
      "    Strength: 0.1501\n",
      "---------------\n",
      "Topic 6 consensus:\n",
      "    Keywords: fake news;news media;wacky omarosa;presidential lowlife;overtime wacky;omarosa modern;omarosa legitimate;modern form;media overtime;lowlife omarosa;form communication;communication fake\n",
      "    Consensus: 1.0\n",
      "    Strength: 0.0901\n",
      "---------------\n",
      "Topic 7 consensus:\n",
      "    Keywords: john brennan;cia director;tuckercarlson speaking;speaking john;limited intellectually;intellectually cia;director good;brennan limited\n",
      "    Consensus: 1.0\n",
      "    Strength: 0.0831\n",
      "---------------\n"
     ]
    }
   ],
   "source": [
    "dataset = 'trump_tweets' # str | Dataset name.\n",
    "query = '' # str | Fulltext query, using mysql MATCH boolean query format. Example, (\\\"word1\\\" OR \\\"word2\\\") AND (\\\"word3\\\" OR \\\"word4\\\") (optional)\n",
    "custom_stop_words = [\"real\",\"hillary\"] # str | List of stop words. (optional)\n",
    "num_topics = 8 # int | Number of topics to be extracted from the dataset. (optional) (default to 8)\n",
    "num_keywords = 8 # int | Number of keywords per topic that is extracted from the dataset. (optional) (default to 8)\n",
    "excluded_docs = [''] # str | List of document IDs that should be excluded from the analysis. Example, [\"docid1\", \"docid2\", ..., \"docidN\"]  (optional)\n",
    "custom_dict_file = {\"great\": 1.0, \"awful\": -1.0, \"clinton\":-1.0, \"trump\":1.0} # file | Custom sentiment dictionary JSON file. Example, {\"field1\": value1, ..., \"fieldN\": valueN} (optional)\n",
    "remove_redundancies = False # bool | If True, this option removes quasi-duplicates from the analysis. A quasi-duplicate would have the same NLP representation, but not necessarily the exact same text. (optional) (default True)\n",
    "\n",
    "metadata_selection =\"\" # dict | JSON object specifying metadata-based queries on the dataset, of type {\"metadata_field\": \"selected_values\"} (optional)\n",
    "time_period = \"\"     # str | Time period selection. Choices: [\"1M\",\"3M\",\"6M\",\"12M\",\"3Y\",\"5Y\",\"\"] (optional)\n",
    "period_start = \"\" # str | Start date for the period to analyze within the dataset. Format: \"YYYY-MM-DD HH:MM:SS\"\n",
    "period_end = \"\" # str | End date for the period to analyze within the dataset. Format: \"YYYY-MM-DD HH:MM:SS\"\n",
    "\n",
    "try:\n",
    "    payload = nucleus_api.TopicConsensusModel(dataset=dataset, \n",
    "                                              query=query, \n",
    "                                              custom_stop_words=custom_stop_words, \n",
    "                                              num_topics=num_topics, \n",
    "                                              num_keywords=num_keywords,\n",
    "                                              custom_dict_file=custom_dict_file)\n",
    "    api_response = api_instance.post_topic_consensus_api(payload)\n",
    "except ApiException as e:\n",
    "    api_error = json.loads(e.body)\n",
    "    print('ERROR:', api_error['message'])\n",
    "    \n",
    "for i, res in enumerate(api_response.result):\n",
    "    print('Topic', i, 'consensus:')\n",
    "    print('    Keywords:', res.keywords)\n",
    "    print('    Consensus:', res.consensus)\n",
    "    print('    Strength:', res.strength)\n",
    "    print('---------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform a historical analysis of topics' strength, sentiment, and consensus in a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing historical metrics data...\n",
      "NOTE: historical metrics data can be plotted when running the example in Jupyter Notebook\n",
      "Topic 0 illegal aliens;united permission;permission illegal;enter united;aliens treated;aliens illegal;treated entered;entered legally\n",
      "    Timestamps: ['2018-08-13 22:36:00', '2018-08-14 11:21:00', '2018-08-15 12:44:00', '2018-08-15 13:51:00', '2018-08-16 00:00:00', '2018-08-16 23:37:00', '2018-08-17 11:30:00', '2018-08-17 22:29:00', '2018-08-18 13:12:00', '2018-08-18 13:39:00']\n",
      "    Strengths: ['NaN', 'NaN', '0.2091', '0.3589', '0.5697', 'NaN', 'NaN', 'NaN', 'NaN', 'NaN']\n",
      "    Consensuses: ['NaN', 'NaN', '1.0', '1.0', '1.0', 'NaN', 'NaN', 'NaN', 'NaN', 'NaN']\n",
      "    Sentiments: ['NaN', 'NaN', '-0.5555', '-0.5555', '-0.5555', 'NaN', 'NaN', 'NaN', 'NaN', 'NaN']\n",
      "----------------\n",
      "Topic 1 bruce ohr;ohr doj;legal jeopardy;doj legal;ohr justice;justice department;christopher steele;jeopardy astonishingthat;jeopardy astonishing;astonishingthat employed;astonishing employed\n",
      "    Timestamps: ['2018-08-13 22:36:00', '2018-08-14 11:21:00', '2018-08-15 12:44:00', '2018-08-15 13:51:00', '2018-08-16 00:00:00', '2018-08-16 23:37:00', '2018-08-17 11:30:00', '2018-08-17 22:29:00', '2018-08-18 13:12:00', '2018-08-18 13:39:00']\n",
      "    Strengths: ['NaN', 'NaN', '0.0814', '0.066', 'NaN', '0.1188', '0.1878', '0.2276', '0.3961', '0.7974']\n",
      "    Consensuses: ['NaN', 'NaN', '1.0', '1.0', 'NaN', '1.0', '1.0', '1.0', '1.0', '1.0']\n",
      "    Sentiments: ['NaN', 'NaN', '-0.3333', '-0.3333', 'NaN', '-0.2666', '-0.3764', '-0.3293', '-0.2496', '-0.2496']\n",
      "----------------\n",
      "Topic 2 america great;statement america;andrew cuomo;great great;worse hightax;hightax andrew;deplorables statement;cuomo statement;clinton deplorables\n",
      "    Timestamps: ['2018-08-13 22:36:00', '2018-08-14 11:21:00', '2018-08-15 12:44:00', '2018-08-15 13:51:00', '2018-08-16 00:00:00', '2018-08-16 23:37:00', '2018-08-17 11:30:00', '2018-08-17 22:29:00', '2018-08-18 13:12:00', '2018-08-18 13:39:00']\n",
      "    Strengths: ['NaN', 'NaN', 'NaN', 'NaN', '0.0328', '0.0432', '0.0297', '0.4206', '0.5032', 'NaN']\n",
      "    Consensuses: ['NaN', 'NaN', 'NaN', 'NaN', '1.0', '1.0', '1.0', '0.6903', '0.6903', 'NaN']\n",
      "    Sentiments: ['NaN', 'NaN', 'NaN', 'NaN', '0.2857', '0.2857', '0.2857', '0.1863', '0.1863', 'NaN']\n",
      "----------------\n",
      "Topic 3 peter strzok;fired fbi;agent peter;fbi agent;strzok fraud;rigged investigation;investigation started;fraud rigged\n",
      "    Timestamps: ['2018-08-13 22:36:00', '2018-08-14 11:21:00', '2018-08-15 12:44:00', '2018-08-15 13:51:00', '2018-08-16 00:00:00', '2018-08-16 23:37:00', '2018-08-17 11:30:00', '2018-08-17 22:29:00', '2018-08-18 13:12:00', '2018-08-18 13:39:00']\n",
      "    Strengths: ['0.2385', '0.2978', '0.1871', '0.1517', 'NaN', '0.2141', '0.1473', '0.132', 'NaN', 'NaN']\n",
      "    Consensuses: ['1.0', '1.0', '1.0', '1.0', 'NaN', '1.0', '1.0', '1.0', 'NaN', 'NaN']\n",
      "    Sentiments: ['-0.4999', '-0.4999', '-0.647', '-0.647', 'NaN', '-0.2666', '-0.2666', '-0.2666', 'NaN', 'NaN']\n",
      "----------------\n",
      "Topic 4 trump campaign;lou dobbs;evidence collusion;dobbs special;democrats evidence;conflicts angry;collusion trump;campaign russia;angry democrats\n",
      "    Timestamps: ['2018-08-13 22:36:00', '2018-08-14 11:21:00', '2018-08-15 12:44:00', '2018-08-15 13:51:00', '2018-08-16 00:00:00', '2018-08-16 23:37:00', '2018-08-17 11:30:00', '2018-08-17 22:29:00', '2018-08-18 13:12:00', '2018-08-18 13:39:00']\n",
      "    Strengths: ['NaN', '0.5522', '0.3743', '0.3034', '0.0296', '0.039', '0.0537', '0.024', 'NaN', 'NaN']\n",
      "    Consensuses: ['NaN', '1.0', '1.0', '1.0', '1.0', '1.0', '0.506', '1.0', 'NaN', 'NaN']\n",
      "    Sentiments: ['NaN', '-0.3636', '-0.4545', '-0.4545', '-0.3', '-0.3', '-0.0998', '0.1052', 'NaN', 'NaN']\n",
      "----------------\n",
      "Topic 5 witch hunt;donald trump;rigged witch;frame donald;unfortunate situation;trump rigged;situation decided;decided frame\n",
      "    Timestamps: ['2018-08-13 22:36:00', '2018-08-14 11:21:00', '2018-08-15 12:44:00', '2018-08-15 13:51:00', '2018-08-16 00:00:00', '2018-08-16 23:37:00', '2018-08-17 11:30:00', '2018-08-17 22:29:00', '2018-08-18 13:12:00', '2018-08-18 13:39:00']\n",
      "    Strengths: ['0.2374', '0.0988', '0.1479', '0.1199', '0.327', '0.4305', '0.2571', 'NaN', 'NaN', 'NaN']\n",
      "    Consensuses: ['1.0', '1.0', '1.0', '1.0', '1.0', '1.0', '1.0', 'NaN', 'NaN', 'NaN']\n",
      "    Sentiments: ['-0.3711', '-0.4999', '-0.5042', '-0.5042', '-0.4065', '-0.4065', '-0.3469', 'NaN', 'NaN', 'NaN']\n",
      "----------------\n",
      "Topic 6 fake news;news media;wacky omarosa;presidential lowlife;overtime wacky;omarosa modern;omarosa legitimate;modern form;media overtime;lowlife omarosa;form communication;communication fake\n",
      "    Timestamps: ['2018-08-13 22:36:00', '2018-08-14 11:21:00', '2018-08-15 12:44:00', '2018-08-15 13:51:00', '2018-08-16 00:00:00', '2018-08-16 23:37:00', '2018-08-17 11:30:00', '2018-08-17 22:29:00', '2018-08-18 13:12:00', '2018-08-18 13:39:00']\n",
      "    Strengths: ['0.524', '0.051', 'NaN', 'NaN', 'NaN', '0.1004', '0.0691', 'NaN', '0.026', '0.0524']\n",
      "    Consensuses: ['0.7747', '1.0', 'NaN', 'NaN', 'NaN', '1.0', '1.0', 'NaN', '1.0', '1.0']\n",
      "    Sentiments: ['-0.0649', '-0.5', 'NaN', 'NaN', 'NaN', '-0.4653', '-0.4653', 'NaN', '-0.6666', '-0.6666']\n",
      "----------------\n",
      "Topic 7 john brennan;cia director;tuckercarlson speaking;speaking john;limited intellectually;intellectually cia;director good;brennan limited\n",
      "    Timestamps: ['2018-08-13 22:36:00', '2018-08-14 11:21:00', '2018-08-15 12:44:00', '2018-08-15 13:51:00', '2018-08-16 00:00:00', '2018-08-16 23:37:00', '2018-08-17 11:30:00', '2018-08-17 22:29:00', '2018-08-18 13:12:00', '2018-08-18 13:39:00']\n",
      "    Strengths: ['NaN', 'NaN', 'NaN', 'NaN', '0.0406', '0.0535', '0.2549', '0.1955', '0.0745', '0.15']\n",
      "    Consensuses: ['NaN', 'NaN', 'NaN', 'NaN', '1.0', '1.0', '0.8635', '1.0', '1.0', '1.0']\n",
      "    Sentiments: ['NaN', 'NaN', 'NaN', 'NaN', '-0.125', '-0.125', '-0.017', '0.0', '-0.2', '-0.2']\n",
      "----------------\n",
      "Plotting historical metrics data...\n"
     ]
    }
   ],
   "source": [
    "dataset = 'trump_tweets'   # str | Dataset name.\n",
    "update_period = 'm' # str | Frequency at which the historical anlaysis is performed. choices=[\"d\",\"m\",\"H\",\"M\"] (default to d)\n",
    "query = '' # str | Fulltext query, using mysql MATCH boolean query format. Example, (\\\"word1\\\" OR \\\"word2\\\") AND (\\\"word3\\\" OR \\\"word4\\\") (optional)\n",
    "custom_stop_words = [\"real\",\"hillary\"] # str | List of stop words (optional)\n",
    "num_topics = 8 # int | Number of topics to be extracted from the dataset. (optional) (default to 8)\n",
    "num_keywords = 8 # int | Number of keywords per topic that is extracted from the dataset. (optional) (default to 8)\n",
    "inc_step = 1 # int | Number of increments of the udpate period in between two historical computations. (optional) (default to 1)\n",
    "excluded_docs = [''] # str | List of document IDs that should be excluded from the analysis. Example, [\"docid1\", \"docid2\", ..., \"docidN\"]  (optional)\n",
    "custom_dict_file = {} # file | Custom sentiment dictionary JSON file. Example, {\"field1\": value1, ..., \"fieldN\": valueN} (optional)\n",
    "remove_redundancies = False # bool | If True, this option removes quasi-duplicates from the analysis. A quasi-duplicate would have the same NLP representation, but not necessarily the exact same text. (optional) (default True)\n",
    "\n",
    "metadata_selection =\"\" # dict | JSON object specifying metadata-based queries on the dataset, of type {\"metadata_field\": \"selected_values\"} (optional)\n",
    "time_period = \"12M\"     # str | Time period selection. Choices: [\"1M\",\"3M\",\"6M\",\"12M\",\"3Y\",\"5Y\",\"\"] (optional)\n",
    "period_start = \"\" # str | Start date for the period to analyze within the dataset. Format: \"YYYY-MM-DD HH:MM:SS\"\n",
    "period_end = \"\" # str | End date for the period to analyze within the dataset. Format: \"YYYY-MM-DD HH:MM:SS\"\n",
    "api_response = None\n",
    "try:\n",
    "    payload = nucleus_api.TopicHistoryModel(dataset=dataset, \n",
    "                                            time_period=time_period, \n",
    "                                            query=query, \n",
    "                                            custom_stop_words=custom_stop_words, \n",
    "                                            num_topics=num_topics, \n",
    "                                            num_keywords=num_keywords, \n",
    "                                            metadata_selection=metadata_selection, \n",
    "                                            excluded_docs=excluded_docs,\n",
    "                                            custom_dict_file=custom_dict_file)\n",
    "    api_response = api_instance.post_topic_historical_analysis_api(payload)\n",
    "except ApiException as e:\n",
    "    api_error = json.loads(e.body)\n",
    "    print('ERROR:', api_error['message'])\n",
    "    print(e)\n",
    "\n",
    "print('Printing historical metrics data...')\n",
    "print('NOTE: historical metrics data can be plotted when running the example in Jupyter Notebook')\n",
    "\n",
    "for i,res in enumerate(api_response.result):\n",
    "    print('Topic', i, res.keywords)\n",
    "    print('    Timestamps:', res.time_stamps)\n",
    "    print('    Strengths:', res.strengths)\n",
    "    print('    Consensuses:', res.consensuses)\n",
    "    print('    Sentiments:', res.sentiments)\n",
    "    print('----------------')\n",
    "            \n",
    "\n",
    "# chart the historical metrics when running in Jupyter Notebook\n",
    "if running_notebook:\n",
    "    print('Plotting historical metrics data...')\n",
    "    historical_metrics = []\n",
    "    for res in api_response.result:\n",
    "        # construct a list of historical metrics dictionaries for charting\n",
    "        historical_metrics.append({\n",
    "            'topic'    : res.keywords,\n",
    "            'time_stamps' : np.array(res.time_stamps),\n",
    "            'strength' : np.array(res.strengths, dtype=np.float32),\n",
    "            'consensus': np.array(res.consensuses, dtype=np.float32), \n",
    "            'sentiment': np.array(res.sentiments, dtype=np.float32)})\n",
    "\n",
    "    selected_topics = range(len(historical_metrics)) \n",
    "    #nucleus_helper.topic_charts_historical(historical_metrics, selected_topics, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determine the network of authors similar to a chosen contributor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mainstream connections:\n",
      "    Keywords: NA\n",
      "    Authors: NA\n",
      "Niche connections:\n",
      "    Keywords: NA\n",
      "    Authors: NA\n"
     ]
    }
   ],
   "source": [
    "dataset = 'trump_tweets' # str | Dataset name.\n",
    "target_author = 'D_Trump16' # str | Name of the author to be analyzed.\n",
    "query = '' # str | Fulltext query, using mysql MATCH boolean query format. Subject covered by the author, on which to focus the analysis of connectivity. Example, (\\\"word1\\\" OR \\\"word2\\\") AND (\\\"word3\\\" OR \\\"word4\\\") (optional)\n",
    "custom_stop_words = [\"real\",\"hillary\"] # str | List of words possibly used by the target author that are considered not information-bearing. (optional)\n",
    "excluded_docs = [''] # str | List of document IDs that should be excluded from the analysis. Example, [\"docid1\", \"docid2\", ..., \"docidN\"]  (optional)\n",
    "\n",
    "metadata_selection =\"\" # dict | JSON object specifying metadata-based queries on the dataset, of type {\"metadata_field\": \"selected_values\"} (optional)\n",
    "time_period = \"12M\"     # str | Time period selection. Choices: [\"1M\",\"3M\",\"6M\",\"12M\",\"3Y\",\"5Y\",\"\"] (optional)\n",
    "period_start = \"\" # str | Start date for the period to analyze within the dataset. Format: \"YYYY-MM-DD HH:MM:SS\"\n",
    "period_end = \"\" # str | End date for the period to analyze within the dataset. Format: \"YYYY-MM-DD HH:MM:SS\"\n",
    "\n",
    "try:\n",
    "    payload = nucleus_api.AuthorConnection(dataset=dataset, \n",
    "                                           target_author=target_author, \n",
    "                                           query=query, \n",
    "                                           custom_stop_words=custom_stop_words, \n",
    "                                           time_period=time_period, \n",
    "                                           metadata_selection=metadata_selection, \n",
    "                                           excluded_docs=excluded_docs)\n",
    "    api_response = api_instance.post_author_connectivity_api(payload)    \n",
    "except ApiException as e:\n",
    "    api_error = json.loads(e.body)\n",
    "    print('ERROR:', api_error['message'])\n",
    "\n",
    "res = api_response.result\n",
    "print('Mainstream connections:')\n",
    "for mc in res.mainstream_connections:\n",
    "    print('    Keywords:', mc.keywords)\n",
    "    print('    Authors:', \" \".join(str(x) for x in mc.authors))\n",
    "    \n",
    "print('Niche connections:')\n",
    "for nc in res.niche_connections:\n",
    "    print('    Keywords:', nc.keywords)\n",
    "    print('    Authors:', \" \".join(str(x) for x in nc.authors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply transfer learning to a list of topics identified in one dataset onto another dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'job_id': '169847',\n",
      " 'result': {'doc_ids_t1': ['46802578687162219',\n",
      "                           '157671510549269746',\n",
      "                           '681266336486897660',\n",
      "                           '2463565422226136907',\n",
      "                           '2473572527244144057',\n",
      "                           '2497347111761266654',\n",
      "                           '2692713684604938265',\n",
      "                           '3295270030761385310',\n",
      "                           '3939433634160758993',\n",
      "                           '3951288681850631775',\n",
      "                           '4046653213651213725',\n",
      "                           '4349543959441164718',\n",
      "                           '4437198275925723018',\n",
      "                           '4584670957973383500',\n",
      "                           '5197981696652006791',\n",
      "                           '5294521276627635033',\n",
      "                           '5336597423569459183',\n",
      "                           '5337115965771373285',\n",
      "                           '5501029233133759464',\n",
      "                           '5734555862573956053',\n",
      "                           '5983550775501810643',\n",
      "                           '6584380602911769482',\n",
      "                           '6624757382845353411',\n",
      "                           '6817024102264760936',\n",
      "                           '7057528309914044758',\n",
      "                           '7148720741304645705',\n",
      "                           '7290488938710124315',\n",
      "                           '7508075727870080337',\n",
      "                           '7552668845228134705',\n",
      "                           '7721974971266362305',\n",
      "                           '8932963129356075453',\n",
      "                           '9220064279401475464',\n",
      "                           '9687935645391984655',\n",
      "                           '9742569426091859106',\n",
      "                           '9918250277785954786',\n",
      "                           '9995371484769371289',\n",
      "                           '10022945975448422485',\n",
      "                           '10066536328057463840',\n",
      "                           '10234332166402996482',\n",
      "                           '11067191221354407080',\n",
      "                           '11599059962808147623',\n",
      "                           '12437126850273027242',\n",
      "                           '12816349349621722463',\n",
      "                           '12996919858778233473',\n",
      "                           '13115764764473553109',\n",
      "                           '13125097700312206949',\n",
      "                           '14286875122783287072',\n",
      "                           '14368578770105844206',\n",
      "                           '14824073781424371042',\n",
      "                           '15063183167414362421',\n",
      "                           '15091083406720907936',\n",
      "                           '15178489020767442896',\n",
      "                           '15284262488783149410',\n",
      "                           '16119072178444509629',\n",
      "                           '16578282956147621208',\n",
      "                           '16832443168363937194',\n",
      "                           '17055305115598973412',\n",
      "                           '18054399480757330387',\n",
      "                           '18298582467582872832'],\n",
      "            'topics': [{'doc_topic_exposures_t1': ['0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.527689',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.472311',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0'],\n",
      "                        'keywords': 'illegal aliens;united '\n",
      "                                    'permission;permission illegal;enter '\n",
      "                                    'united;aliens treated;aliens '\n",
      "                                    'illegal;treated entered;entered legally',\n",
      "                        'keywords_weight': ['0.1241',\n",
      "                                            '0.1241',\n",
      "                                            '0.1241',\n",
      "                                            '0.2483',\n",
      "                                            '0.1241',\n",
      "                                            '0.1241',\n",
      "                                            '0.0655',\n",
      "                                            '0.0655'],\n",
      "                        'strength': '0.1857'},\n",
      "                       {'doc_topic_exposures_t1': ['0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.065664',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.518868',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.065664',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.135306',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.214499',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0'],\n",
      "                        'keywords': 'witch hunt;donald trump;rigged '\n",
      "                                    'witch;frame donald;unfortunate '\n",
      "                                    'situation;trump rigged;situation '\n",
      "                                    'decided;decided frame',\n",
      "                        'keywords_weight': ['0.179',\n",
      "                                            '0.0971',\n",
      "                                            '0.0971',\n",
      "                                            '0.0971',\n",
      "                                            '0.1268',\n",
      "                                            '0.179',\n",
      "                                            '0.0971',\n",
      "                                            '0.1268'],\n",
      "                        'strength': '0.1687'},\n",
      "                       {'doc_topic_exposures_t1': ['0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.039911',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.46057',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.038949',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.46057',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0'],\n",
      "                        'keywords': 'trump campaign;lou dobbs;evidence '\n",
      "                                    'collusion;dobbs special;democrats '\n",
      "                                    'evidence;conflicts angry;collusion '\n",
      "                                    'trump;campaign russia;angry democrats',\n",
      "                        'keywords_weight': ['0.1168',\n",
      "                                            '0.1104',\n",
      "                                            '0.1104',\n",
      "                                            '0.1104',\n",
      "                                            '0.1104',\n",
      "                                            '0.1104',\n",
      "                                            '0.1104',\n",
      "                                            '0.1104',\n",
      "                                            '0.1104'],\n",
      "                        'strength': '0.1754'},\n",
      "                       {'doc_topic_exposures_t1': ['0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.392774',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.607226',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0'],\n",
      "                        'keywords': 'peter strzok;fired fbi;agent peter;strzok '\n",
      "                                    'fraud;rigged investigation;investigation '\n",
      "                                    'started;fraud rigged;fbi agent',\n",
      "                        'keywords_weight': ['0.1561',\n",
      "                                            '0.1561',\n",
      "                                            '0.1561',\n",
      "                                            '0.1063',\n",
      "                                            '0.1063',\n",
      "                                            '0.1063',\n",
      "                                            '0.1063',\n",
      "                                            '0.1063'],\n",
      "                        'strength': '0.1236'},\n",
      "                       {'doc_topic_exposures_t1': ['0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.182459',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.270641',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.157476',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.206965',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.182459',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0'],\n",
      "                        'keywords': 'crooked hillary;strzok fbi;hillary '\n",
      "                                    'clinton;sham investigation;fired '\n",
      "                                    'agent;fbi charge;clinton sham;charge '\n",
      "                                    'crooked;agent strzok',\n",
      "                        'keywords_weight': ['0.1052',\n",
      "                                            '0.1052',\n",
      "                                            '0.1176',\n",
      "                                            '0.1052',\n",
      "                                            '0.1052',\n",
      "                                            '0.1334',\n",
      "                                            '0.1176',\n",
      "                                            '0.1052',\n",
      "                                            '0.1052'],\n",
      "                        'strength': '0.0486'},\n",
      "                       {'doc_topic_exposures_t1': ['0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '1.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0'],\n",
      "                        'keywords': 'trade deals;tariffs tariffs;tariffs '\n",
      "                                    'leading;leading great;great trade;deals '\n",
      "                                    'opposed;country built;built tariffs',\n",
      "                        'keywords_weight': ['0.1111',\n",
      "                                            '0.1111',\n",
      "                                            '0.1111',\n",
      "                                            '0.1111',\n",
      "                                            '0.1111',\n",
      "                                            '0.1111',\n",
      "                                            '0.2222',\n",
      "                                            '0.1111'],\n",
      "                        'strength': '0.0959'},\n",
      "                       {'doc_topic_exposures_t1': ['0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.219266',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.660058',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.120676',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0'],\n",
      "                        'keywords': 'great win;win night;wisconsin '\n",
      "                                    'great;vukmir wisconsin;leah '\n",
      "                                    'vukmir;congratulations leah;walker '\n",
      "                                    'special;special great',\n",
      "                        'keywords_weight': ['0.0408',\n",
      "                                            '0.0408',\n",
      "                                            '0.1827',\n",
      "                                            '0.142',\n",
      "                                            '0.142',\n",
      "                                            '0.142',\n",
      "                                            '0.142',\n",
      "                                            '0.1679'],\n",
      "                        'strength': '0.106'},\n",
      "                       {'doc_topic_exposures_t1': ['0.079029',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.775248',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0',\n",
      "                                                   '0.145723',\n",
      "                                                   '0.0',\n",
      "                                                   '0.0'],\n",
      "                        'keywords': 'strzok firing;wrongs committed;step '\n",
      "                                    'direction;skinny circle;firing '\n",
      "                                    'decisive;direction correcting;decisive '\n",
      "                                    'step;correcting wrongs;committed '\n",
      "                                    'comey;comey skinny;action strzok',\n",
      "                        'keywords_weight': ['0.1085',\n",
      "                                            '0.0891',\n",
      "                                            '0.0891',\n",
      "                                            '0.0891',\n",
      "                                            '0.0891',\n",
      "                                            '0.0891',\n",
      "                                            '0.0891',\n",
      "                                            '0.0891',\n",
      "                                            '0.0891',\n",
      "                                            '0.0891',\n",
      "                                            '0.0891'],\n",
      "                        'strength': '0.0955'}]}}\n",
      "Topic 0 exposure within validation dataset:\n",
      "    Keywords: illegal aliens;united permission;permission illegal;enter united;aliens treated;aliens illegal;treated entered;entered legally\n",
      "    Strength: 0.1857\n",
      "    Document IDs: ['46802578687162219', '157671510549269746', '681266336486897660', '2463565422226136907', '2473572527244144057', '2497347111761266654', '2692713684604938265', '3295270030761385310', '3939433634160758993', '3951288681850631775', '4046653213651213725', '4349543959441164718', '4437198275925723018', '4584670957973383500', '5197981696652006791', '5294521276627635033', '5336597423569459183', '5337115965771373285', '5501029233133759464', '5734555862573956053', '5983550775501810643', '6584380602911769482', '6624757382845353411', '6817024102264760936', '7057528309914044758', '7148720741304645705', '7290488938710124315', '7508075727870080337', '7552668845228134705', '7721974971266362305', '8932963129356075453', '9220064279401475464', '9687935645391984655', '9742569426091859106', '9918250277785954786', '9995371484769371289', '10022945975448422485', '10066536328057463840', '10234332166402996482', '11067191221354407080', '11599059962808147623', '12437126850273027242', '12816349349621722463', '12996919858778233473', '13115764764473553109', '13125097700312206949', '14286875122783287072', '14368578770105844206', '14824073781424371042', '15063183167414362421', '15091083406720907936', '15178489020767442896', '15284262488783149410', '16119072178444509629', '16578282956147621208', '16832443168363937194', '17055305115598973412', '18054399480757330387', '18298582467582872832']\n",
      "    Exposure per Doc in Validation Dataset: ['0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.527689', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.472311', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0']\n",
      "---------------\n",
      "Topic 1 exposure within validation dataset:\n",
      "    Keywords: witch hunt;donald trump;rigged witch;frame donald;unfortunate situation;trump rigged;situation decided;decided frame\n",
      "    Strength: 0.1687\n",
      "    Document IDs: ['46802578687162219', '157671510549269746', '681266336486897660', '2463565422226136907', '2473572527244144057', '2497347111761266654', '2692713684604938265', '3295270030761385310', '3939433634160758993', '3951288681850631775', '4046653213651213725', '4349543959441164718', '4437198275925723018', '4584670957973383500', '5197981696652006791', '5294521276627635033', '5336597423569459183', '5337115965771373285', '5501029233133759464', '5734555862573956053', '5983550775501810643', '6584380602911769482', '6624757382845353411', '6817024102264760936', '7057528309914044758', '7148720741304645705', '7290488938710124315', '7508075727870080337', '7552668845228134705', '7721974971266362305', '8932963129356075453', '9220064279401475464', '9687935645391984655', '9742569426091859106', '9918250277785954786', '9995371484769371289', '10022945975448422485', '10066536328057463840', '10234332166402996482', '11067191221354407080', '11599059962808147623', '12437126850273027242', '12816349349621722463', '12996919858778233473', '13115764764473553109', '13125097700312206949', '14286875122783287072', '14368578770105844206', '14824073781424371042', '15063183167414362421', '15091083406720907936', '15178489020767442896', '15284262488783149410', '16119072178444509629', '16578282956147621208', '16832443168363937194', '17055305115598973412', '18054399480757330387', '18298582467582872832']\n",
      "    Exposure per Doc in Validation Dataset: ['0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.065664', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.518868', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.065664', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.135306', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.214499', '0.0', '0.0', '0.0', '0.0']\n",
      "---------------\n",
      "Topic 2 exposure within validation dataset:\n",
      "    Keywords: trump campaign;lou dobbs;evidence collusion;dobbs special;democrats evidence;conflicts angry;collusion trump;campaign russia;angry democrats\n",
      "    Strength: 0.1754\n",
      "    Document IDs: ['46802578687162219', '157671510549269746', '681266336486897660', '2463565422226136907', '2473572527244144057', '2497347111761266654', '2692713684604938265', '3295270030761385310', '3939433634160758993', '3951288681850631775', '4046653213651213725', '4349543959441164718', '4437198275925723018', '4584670957973383500', '5197981696652006791', '5294521276627635033', '5336597423569459183', '5337115965771373285', '5501029233133759464', '5734555862573956053', '5983550775501810643', '6584380602911769482', '6624757382845353411', '6817024102264760936', '7057528309914044758', '7148720741304645705', '7290488938710124315', '7508075727870080337', '7552668845228134705', '7721974971266362305', '8932963129356075453', '9220064279401475464', '9687935645391984655', '9742569426091859106', '9918250277785954786', '9995371484769371289', '10022945975448422485', '10066536328057463840', '10234332166402996482', '11067191221354407080', '11599059962808147623', '12437126850273027242', '12816349349621722463', '12996919858778233473', '13115764764473553109', '13125097700312206949', '14286875122783287072', '14368578770105844206', '14824073781424371042', '15063183167414362421', '15091083406720907936', '15178489020767442896', '15284262488783149410', '16119072178444509629', '16578282956147621208', '16832443168363937194', '17055305115598973412', '18054399480757330387', '18298582467582872832']\n",
      "    Exposure per Doc in Validation Dataset: ['0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.039911', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.46057', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.038949', '0.0', '0.0', '0.0', '0.0', '0.0', '0.46057', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0']\n",
      "---------------\n",
      "Topic 3 exposure within validation dataset:\n",
      "    Keywords: peter strzok;fired fbi;agent peter;strzok fraud;rigged investigation;investigation started;fraud rigged;fbi agent\n",
      "    Strength: 0.1236\n",
      "    Document IDs: ['46802578687162219', '157671510549269746', '681266336486897660', '2463565422226136907', '2473572527244144057', '2497347111761266654', '2692713684604938265', '3295270030761385310', '3939433634160758993', '3951288681850631775', '4046653213651213725', '4349543959441164718', '4437198275925723018', '4584670957973383500', '5197981696652006791', '5294521276627635033', '5336597423569459183', '5337115965771373285', '5501029233133759464', '5734555862573956053', '5983550775501810643', '6584380602911769482', '6624757382845353411', '6817024102264760936', '7057528309914044758', '7148720741304645705', '7290488938710124315', '7508075727870080337', '7552668845228134705', '7721974971266362305', '8932963129356075453', '9220064279401475464', '9687935645391984655', '9742569426091859106', '9918250277785954786', '9995371484769371289', '10022945975448422485', '10066536328057463840', '10234332166402996482', '11067191221354407080', '11599059962808147623', '12437126850273027242', '12816349349621722463', '12996919858778233473', '13115764764473553109', '13125097700312206949', '14286875122783287072', '14368578770105844206', '14824073781424371042', '15063183167414362421', '15091083406720907936', '15178489020767442896', '15284262488783149410', '16119072178444509629', '16578282956147621208', '16832443168363937194', '17055305115598973412', '18054399480757330387', '18298582467582872832']\n",
      "    Exposure per Doc in Validation Dataset: ['0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.392774', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.607226', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0']\n",
      "---------------\n",
      "Topic 4 exposure within validation dataset:\n",
      "    Keywords: crooked hillary;strzok fbi;hillary clinton;sham investigation;fired agent;fbi charge;clinton sham;charge crooked;agent strzok\n",
      "    Strength: 0.0486\n",
      "    Document IDs: ['46802578687162219', '157671510549269746', '681266336486897660', '2463565422226136907', '2473572527244144057', '2497347111761266654', '2692713684604938265', '3295270030761385310', '3939433634160758993', '3951288681850631775', '4046653213651213725', '4349543959441164718', '4437198275925723018', '4584670957973383500', '5197981696652006791', '5294521276627635033', '5336597423569459183', '5337115965771373285', '5501029233133759464', '5734555862573956053', '5983550775501810643', '6584380602911769482', '6624757382845353411', '6817024102264760936', '7057528309914044758', '7148720741304645705', '7290488938710124315', '7508075727870080337', '7552668845228134705', '7721974971266362305', '8932963129356075453', '9220064279401475464', '9687935645391984655', '9742569426091859106', '9918250277785954786', '9995371484769371289', '10022945975448422485', '10066536328057463840', '10234332166402996482', '11067191221354407080', '11599059962808147623', '12437126850273027242', '12816349349621722463', '12996919858778233473', '13115764764473553109', '13125097700312206949', '14286875122783287072', '14368578770105844206', '14824073781424371042', '15063183167414362421', '15091083406720907936', '15178489020767442896', '15284262488783149410', '16119072178444509629', '16578282956147621208', '16832443168363937194', '17055305115598973412', '18054399480757330387', '18298582467582872832']\n",
      "    Exposure per Doc in Validation Dataset: ['0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.182459', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.270641', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.157476', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.206965', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.182459', '0.0', '0.0', '0.0', '0.0']\n",
      "---------------\n",
      "Topic 5 exposure within validation dataset:\n",
      "    Keywords: trade deals;tariffs tariffs;tariffs leading;leading great;great trade;deals opposed;country built;built tariffs\n",
      "    Strength: 0.0959\n",
      "    Document IDs: ['46802578687162219', '157671510549269746', '681266336486897660', '2463565422226136907', '2473572527244144057', '2497347111761266654', '2692713684604938265', '3295270030761385310', '3939433634160758993', '3951288681850631775', '4046653213651213725', '4349543959441164718', '4437198275925723018', '4584670957973383500', '5197981696652006791', '5294521276627635033', '5336597423569459183', '5337115965771373285', '5501029233133759464', '5734555862573956053', '5983550775501810643', '6584380602911769482', '6624757382845353411', '6817024102264760936', '7057528309914044758', '7148720741304645705', '7290488938710124315', '7508075727870080337', '7552668845228134705', '7721974971266362305', '8932963129356075453', '9220064279401475464', '9687935645391984655', '9742569426091859106', '9918250277785954786', '9995371484769371289', '10022945975448422485', '10066536328057463840', '10234332166402996482', '11067191221354407080', '11599059962808147623', '12437126850273027242', '12816349349621722463', '12996919858778233473', '13115764764473553109', '13125097700312206949', '14286875122783287072', '14368578770105844206', '14824073781424371042', '15063183167414362421', '15091083406720907936', '15178489020767442896', '15284262488783149410', '16119072178444509629', '16578282956147621208', '16832443168363937194', '17055305115598973412', '18054399480757330387', '18298582467582872832']\n",
      "    Exposure per Doc in Validation Dataset: ['0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '1.0', '0.0', '0.0', '0.0']\n",
      "---------------\n",
      "Topic 6 exposure within validation dataset:\n",
      "    Keywords: great win;win night;wisconsin great;vukmir wisconsin;leah vukmir;congratulations leah;walker special;special great\n",
      "    Strength: 0.106\n",
      "    Document IDs: ['46802578687162219', '157671510549269746', '681266336486897660', '2463565422226136907', '2473572527244144057', '2497347111761266654', '2692713684604938265', '3295270030761385310', '3939433634160758993', '3951288681850631775', '4046653213651213725', '4349543959441164718', '4437198275925723018', '4584670957973383500', '5197981696652006791', '5294521276627635033', '5336597423569459183', '5337115965771373285', '5501029233133759464', '5734555862573956053', '5983550775501810643', '6584380602911769482', '6624757382845353411', '6817024102264760936', '7057528309914044758', '7148720741304645705', '7290488938710124315', '7508075727870080337', '7552668845228134705', '7721974971266362305', '8932963129356075453', '9220064279401475464', '9687935645391984655', '9742569426091859106', '9918250277785954786', '9995371484769371289', '10022945975448422485', '10066536328057463840', '10234332166402996482', '11067191221354407080', '11599059962808147623', '12437126850273027242', '12816349349621722463', '12996919858778233473', '13115764764473553109', '13125097700312206949', '14286875122783287072', '14368578770105844206', '14824073781424371042', '15063183167414362421', '15091083406720907936', '15178489020767442896', '15284262488783149410', '16119072178444509629', '16578282956147621208', '16832443168363937194', '17055305115598973412', '18054399480757330387', '18298582467582872832']\n",
      "    Exposure per Doc in Validation Dataset: ['0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.219266', '0.0', '0.0', '0.660058', '0.0', '0.0', '0.0', '0.0', '0.0', '0.120676', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0']\n",
      "---------------\n",
      "Topic 7 exposure within validation dataset:\n",
      "    Keywords: strzok firing;wrongs committed;step direction;skinny circle;firing decisive;direction correcting;decisive step;correcting wrongs;committed comey;comey skinny;action strzok\n",
      "    Strength: 0.0955\n",
      "    Document IDs: ['46802578687162219', '157671510549269746', '681266336486897660', '2463565422226136907', '2473572527244144057', '2497347111761266654', '2692713684604938265', '3295270030761385310', '3939433634160758993', '3951288681850631775', '4046653213651213725', '4349543959441164718', '4437198275925723018', '4584670957973383500', '5197981696652006791', '5294521276627635033', '5336597423569459183', '5337115965771373285', '5501029233133759464', '5734555862573956053', '5983550775501810643', '6584380602911769482', '6624757382845353411', '6817024102264760936', '7057528309914044758', '7148720741304645705', '7290488938710124315', '7508075727870080337', '7552668845228134705', '7721974971266362305', '8932963129356075453', '9220064279401475464', '9687935645391984655', '9742569426091859106', '9918250277785954786', '9995371484769371289', '10022945975448422485', '10066536328057463840', '10234332166402996482', '11067191221354407080', '11599059962808147623', '12437126850273027242', '12816349349621722463', '12996919858778233473', '13115764764473553109', '13125097700312206949', '14286875122783287072', '14368578770105844206', '14824073781424371042', '15063183167414362421', '15091083406720907936', '15178489020767442896', '15284262488783149410', '16119072178444509629', '16578282956147621208', '16832443168363937194', '17055305115598973412', '18054399480757330387', '18298582467582872832']\n",
      "    Exposure per Doc in Validation Dataset: ['0.079029', '0.0', '0.0', '0.0', '0.0', '0.775248', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.145723', '0.0', '0.0']\n",
      "---------------\n"
     ]
    }
   ],
   "source": [
    "dataset0 = 'trump_tweets'\n",
    "dataset1 = None # str | Validation dataset (optional if period_0 and period_1 dates provided)\n",
    "#query = '(\"Trump\" OR \"president\")' # str | Fulltext query, using mysql MATCH boolean query format. Example, (\\\"word1\\\" OR \\\"word2\\\") AND (\\\"word3\\\" OR \\\"word4\\\") (optional)\n",
    "query = ''\n",
    "custom_stop_words = [\"\"] # str | List of stop words. (optional)\n",
    "num_topics = 8 # int | Number of topics to be extracted from the dataset. (optional) (default to 8)\n",
    "num_keywords = 8 # int | Number of keywords per topic that is extracted from the dataset. (optional) (default to 8)\n",
    "metadata_selection = \"\" # dict | JSON object specifying metadata-based queries on the dataset, of type {\"metadata_field\": \"selected_values\"} (optional)\n",
    "period_0_start = '2018-08-12' # Not needed if you provide a validation dataset in the \"dataset1\" variable \n",
    "period_0_end = '2018-08-16' # Not needed if you provide a validation dataset in the \"dataset1\" variable\n",
    "period_1_start = '2018-08-14' # Not needed if you provide a validation dataset in the \"dataset1\" variable\n",
    "period_1_end = '2018-08-18' # Not needed if you provide a validation dataset in the \"dataset1\" variable\n",
    "excluded_docs = '' # str | List of document IDs that should be excluded from the analysis. Example, [\"docid1\", \"docid2\", ..., \"docidN\"]  (optional)\n",
    "remove_redundancies = False # bool | If True, this option removes quasi-duplicates from the analysis. A quasi-duplicate would have the same NLP representation, but not necessarily the exact same text. (optional) (default True)\n",
    "\n",
    "try:\n",
    "    payload = nucleus_api.TopicTransferModel(dataset0=dataset0,\n",
    "                                             dataset1=dataset1,\n",
    "                                             query=query, \n",
    "                                             custom_stop_words=custom_stop_words, \n",
    "                                             num_topics=num_topics, \n",
    "                                             num_keywords=num_keywords,\n",
    "                                             period_0_start=period_0_start,\n",
    "                                             period_0_end=period_0_end,\n",
    "                                             period_1_start=period_1_start,\n",
    "                                             period_1_end=period_1_end,\n",
    "                                             metadata_selection=metadata_selection)\n",
    "    api_response = api_instance.post_topic_transfer_api(payload)\n",
    "    api_ok = True\n",
    "except ApiException as e:\n",
    "    print(e)\n",
    "    api_error = json.loads(e.body)\n",
    "    print('ERROR:', api_error['message'])\n",
    "    api_ok = False\n",
    "\n",
    "print(api_response)\n",
    "\n",
    "if api_ok:\n",
    "    doc_ids_t1 = api_response.result.doc_ids_t1\n",
    "    topics = api_response.result.topics\n",
    "    for i,res in enumerate(topics):\n",
    "        print('Topic', i, 'exposure within validation dataset:')\n",
    "        print('    Keywords:', res.keywords)\n",
    "        print('    Strength:', res.strength)\n",
    "        print('    Document IDs:', doc_ids_t1)\n",
    "        print('    Exposure per Doc in Validation Dataset:', res.doc_topic_exposures_t1)\n",
    "        print('---------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply transfer learning to a list of topics that are exogenously chosen, onto a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0 exposure within validation dataset:\n",
      "    Keywords: markets jobs;jobs militarysetting;jobs better;business jobs\n",
      "    Strength: 1.0\n",
      "    Document IDs: ['46802578687162219', '157671510549269746', '681266336486897660', '2463565422226136907', '2473572527244144057', '2497347111761266654', '2692713684604938265', '3295270030761385310', '3939433634160758993', '3951288681850631775', '4046653213651213725', '4349543959441164718', '4437198275925723018', '4584670957973383500', '5197981696652006791', '5294521276627635033', '5336597423569459183', '5337115965771373285', '5501029233133759464', '5734555862573956053', '5983550775501810643', '6584380602911769482', '6624757382845353411', '6817024102264760936', '7057528309914044758', '7148720741304645705', '7290488938710124315', '7508075727870080337', '7552668845228134705', '7721974971266362305', '8932963129356075453', '9220064279401475464', '9687935645391984655', '9742569426091859106', '9918250277785954786', '9995371484769371289', '10022945975448422485', '10066536328057463840', '10234332166402996482', '11067191221354407080', '11599059962808147623', '12437126850273027242', '12816349349621722463', '12996919858778233473', '13115764764473553109', '13125097700312206949', '14286875122783287072', '14368578770105844206', '14824073781424371042', '15063183167414362421', '15091083406720907936', '15178489020767442896', '15284262488783149410', '16119072178444509629', '16578282956147621208', '16832443168363937194', '17055305115598973412', '18054399480757330387', '18298582467582872832']\n",
      "    Exposure per Doc in Validation Dataset: ['0.0', '0.0', '0.387426', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.612574', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0']\n",
      "---------------\n"
     ]
    }
   ],
   "source": [
    "dataset0 = 'trump_tweets'\n",
    "dataset1 = None # str | Validation dataset (optional if period_0 and period_1 dates provided)\n",
    "fixed_topics = [{\"keywords\": [\"north korea\", \"nuclear weapons\", \"real estate\"], \"weights\": [0.5, 0.3, 0.2]},\n",
    "               {\"keywords\": [\"America\", \"jobs\", \"stock market\"], \"weights\": [0.3, 0.3, 0.3]}] # The weights are optional\n",
    "query = ''\n",
    "custom_stop_words = [\"\"] # str | List of stop words. (optional)\n",
    "num_topics = 8 # int | Number of topics to be extracted from the dataset. (optional) (default to 8)\n",
    "num_keywords = 8 # int | Number of keywords per topic that is extracted from the dataset. (optional) (default to 8)\n",
    "metadata_selection = \"\" # dict | JSON object specifying metadata-based queries on the dataset, of type {\"metadata_field\": \"selected_values\"} (optional)\n",
    "period_0_start = '2017-01-01' # Not needed if you provide a validation dataset in the \"dataset1\" variable \n",
    "period_0_end = '2017-12-31' # Not needed if you provide a validation dataset in the \"dataset1\" variable\n",
    "period_1_start = '2018-01-01' # Not needed if you provide a validation dataset in the \"dataset1\" variable\n",
    "period_1_end = '2018-08-18' # Not needed if you provide a validation dataset in the \"dataset1\" variable\n",
    "period_0_start = '2018-08-12' # Not needed if you provide a validation dataset in the \"dataset1\" variable \n",
    "period_0_end = '2018-08-16' # Not needed if you provide a validation dataset in the \"dataset1\" variable\n",
    "period_1_start = '2018-08-14' # Not needed if you provide a validation dataset in the \"dataset1\" variable\n",
    "period_1_end = '2018-08-18' # Not needed if you provide a validation dataset in the \"dataset1\" variable\n",
    "\n",
    "excluded_docs = '' # str | List of document IDs that should be excluded from the analysis. Example, [\"docid1\", \"docid2\", ..., \"docidN\"]  (optional)\n",
    "remove_redundancies = False # bool | If True, this option removes quasi-duplicates from the analysis. A quasi-duplicate would have the same NLP representation, but not necessarily the exact same text. (optional) (default True)\n",
    "\n",
    "try:\n",
    "    payload = nucleus_api.TopicTransferModel(dataset0=dataset0,\n",
    "                                             dataset1=dataset1,\n",
    "                                             fixed_topics=fixed_topics,\n",
    "                                             query=query, \n",
    "                                             custom_stop_words=custom_stop_words, \n",
    "                                             num_topics=num_topics, \n",
    "                                             num_keywords=num_keywords,\n",
    "                                             period_0_start=period_0_start,\n",
    "                                             period_0_end=period_0_end,\n",
    "                                             period_1_start=period_1_start,\n",
    "                                             period_1_end=period_1_end,\n",
    "                                             metadata_selection=metadata_selection)\n",
    "    api_response = api_instance.post_topic_transfer_api(payload)\n",
    "    api_ok = True\n",
    "except ApiException as e:\n",
    "    api_error = json.loads(e.body)\n",
    "    print('ERROR:', api_error['message'])\n",
    "    api_ok = False\n",
    "\n",
    "if api_ok:\n",
    "    doc_ids_t1 = api_response.result.doc_ids_t1\n",
    "    topics = api_response.result.topics\n",
    "    for i,res in enumerate(topics):\n",
    "        print('Topic', i, 'exposure within validation dataset:')\n",
    "        print('    Keywords:', res.keywords)\n",
    "        print('    Strength:', res.strength)\n",
    "        print('    Document IDs:', doc_ids_t1)\n",
    "        print('    Exposure per Doc in Validation Dataset:', res.doc_topic_exposures_t1)\n",
    "        print('---------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply transfer learning to a list of topics identified in one dataset onto another dataset for sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0 exposure within validation dataset:\n",
      "    Keywords: illegal aliens;united permission;permission illegal;enter united;aliens treated;aliens illegal;treated entered;entered legally\n",
      "    Strength: 0.1857\n",
      "    Sentiment: 0.0\n",
      "    Document IDs: ['2692713684604938265', '9742569426091859106']\n",
      "    Sentiment per Doc in Validation Dataset: ['0.0', '0.0']\n",
      "---------------\n",
      "Topic 1 exposure within validation dataset:\n",
      "    Keywords: witch hunt;donald trump;rigged witch;frame donald;unfortunate situation;trump rigged;situation decided;decided frame\n",
      "    Strength: 0.1687\n",
      "    Sentiment: 0.1177\n",
      "    Document IDs: ['3295270030761385310', '5983550775501810643', '9220064279401475464', '13125097700312206949', '16578282956147621208']\n",
      "    Sentiment per Doc in Validation Dataset: ['0.0', '0.25', '0.181818', '0.0', '-0.090909']\n",
      "---------------\n",
      "Topic 2 exposure within validation dataset:\n",
      "    Keywords: trump campaign;lou dobbs;evidence collusion;dobbs special;democrats evidence;conflicts angry;collusion trump;campaign russia;angry democrats\n",
      "    Strength: 0.1754\n",
      "    Sentiment: 0.1867\n",
      "    Document IDs: ['5336597423569459183', '7290488938710124315', '13115764764473553109', '15091083406720907936']\n",
      "    Sentiment per Doc in Validation Dataset: ['0.2', '0.181818', '0.25', '0.181818']\n",
      "---------------\n",
      "Topic 3 exposure within validation dataset:\n",
      "    Keywords: peter strzok;fired fbi;agent peter;strzok fraud;rigged investigation;investigation started;fraud rigged;fbi agent\n",
      "    Strength: 0.1236\n",
      "    Sentiment: 0.0\n",
      "    Document IDs: ['6817024102264760936', '10234332166402996482']\n",
      "    Sentiment per Doc in Validation Dataset: ['0.0', '0.0']\n",
      "---------------\n",
      "Topic 4 exposure within validation dataset:\n",
      "    Keywords: crooked hillary;strzok fbi;hillary clinton;sham investigation;fired agent;fbi charge;clinton sham;charge crooked;agent strzok\n",
      "    Strength: 0.0486\n",
      "    Sentiment: -0.0302\n",
      "    Document IDs: ['4046653213651213725', '7057528309914044758', '10234332166402996482', '13125097700312206949', '16578282956147621208']\n",
      "    Sentiment per Doc in Validation Dataset: ['0.181818', '0.0', '0.0', '0.0', '-0.25']\n",
      "---------------\n",
      "Topic 5 exposure within validation dataset:\n",
      "    Keywords: trade deals;tariffs tariffs;tariffs leading;leading great;great trade;deals opposed;country built;built tariffs\n",
      "    Strength: 0.0959\n",
      "    Sentiment: 0.1428\n",
      "    Document IDs: ['16832443168363937194']\n",
      "    Sentiment per Doc in Validation Dataset: ['0.142857']\n",
      "---------------\n",
      "Topic 6 exposure within validation dataset:\n",
      "    Keywords: great win;win night;wisconsin great;vukmir wisconsin;leah vukmir;congratulations leah;walker special;special great\n",
      "    Strength: 0.106\n",
      "    Sentiment: 0.3281\n",
      "    Document IDs: ['12996919858778233473', '14286875122783287072', '15284262488783149410']\n",
      "    Sentiment per Doc in Validation Dataset: ['0.5', '0.333333', '0.0']\n",
      "---------------\n",
      "Topic 7 exposure within validation dataset:\n",
      "    Keywords: strzok firing;wrongs committed;step direction;skinny circle;firing decisive;direction correcting;decisive step;correcting wrongs;committed comey;comey skinny;action strzok\n",
      "    Strength: 0.0955\n",
      "    Sentiment: 0.0\n",
      "    Document IDs: ['46802578687162219', '2497347111761266654', '17055305115598973412']\n",
      "    Sentiment per Doc in Validation Dataset: ['0.0', '0.0', '0.0']\n",
      "---------------\n"
     ]
    }
   ],
   "source": [
    "dataset0 = 'trump_tweets'\n",
    "dataset1 = None\n",
    "#dataset1 = dataset # str | Validation dataset (optional if period_0 and period_1 dates provided)\n",
    "#query = '(\"Trump\" OR \"president\")' # str | Fulltext query, using mysql MATCH boolean query format. Example, (\\\"word1\\\" OR \\\"word2\\\") AND (\\\"word3\\\" OR \\\"word4\\\") (optional)\n",
    "#fixed_topic is also an available input argument\n",
    "query = ''\n",
    "custom_stop_words = [\"\"] # str | List of stop words. (optional)\n",
    "num_topics = 8 # int | Number of topics to be extracted from the dataset. (optional) (default to 8)\n",
    "num_keywords = 8 # int | Number of keywords per topic that is extracted from the dataset. (optional) (default to 8)\n",
    "metadata_selection = \"\" # dict | JSON object specifying metadata-based queries on the dataset, of type {\"metadata_field\": \"selected_values\"} (optional)\n",
    "period_0_start = '2018-08-12' # Not needed if you provide a validation dataset in the \"dataset1\" variable \n",
    "period_0_end = '2018-08-16' # Not needed if you provide a validation dataset in the \"dataset1\" variable\n",
    "period_1_start = '2018-08-14' # Not needed if you provide a validation dataset in the \"dataset1\" variable\n",
    "period_1_end = '2018-08-18' # Not needed if you provide a validation dataset in the \"dataset1\" variable\n",
    "excluded_docs = '' # str | List of document IDs that should be excluded from the analysis. Example, [\"docid1\", \"docid2\", ..., \"docidN\"]  (optional)\n",
    "custom_dict_file = {\"great\": 1.0, \"awful\": -1.0, \"clinton\":-1.0, \"trump\":1.0} # file | Custom sentiment dictionary JSON file. Example, {\"field1\": value1, ..., \"fieldN\": valueN} (optional)\n",
    "remove_redundancies = False # bool | If True, this option removes quasi-duplicates from the analysis. A quasi-duplicate would have the same NLP representation, but not necessarily the exact same text. (optional) (default True)\n",
    "\n",
    "try:\n",
    "    payload = nucleus_api.TopicSentimentTransferModel(dataset0=dataset0,\n",
    "                                                      dataset1=dataset1,\n",
    "                                                      query=query, \n",
    "                                                      custom_stop_words=custom_stop_words, \n",
    "                                                      num_topics=num_topics, \n",
    "                                                      num_keywords=num_keywords,\n",
    "                                                      period_0_start=period_0_start,\n",
    "                                                      period_0_end=period_0_end,\n",
    "                                                      period_1_start=period_1_start,\n",
    "                                                      period_1_end=period_1_end,\n",
    "                                                      metadata_selection=metadata_selection,\n",
    "                                                      custom_dict_file=custom_dict_file)\n",
    "    \n",
    "    api_response = api_instance.post_topic_sentiment_transfer_api(payload)\n",
    "    api_ok = True\n",
    "except ApiException as e:\n",
    "    api_error = json.loads(e.body)\n",
    "    print('ERROR:', api_error['message'])\n",
    "    api_ok = False\n",
    "\n",
    "if api_ok:\n",
    "    topics = api_response.result\n",
    "    for i,res in enumerate(topics):\n",
    "        print('Topic', i, 'exposure within validation dataset:')\n",
    "        print('    Keywords:', res.keywords)\n",
    "        print('    Strength:', res.strength)\n",
    "        print('    Sentiment:', res.sentiment)\n",
    "        print('    Document IDs:', res.doc_ids_t1)\n",
    "        print('    Sentiment per Doc in Validation Dataset:', res.doc_sentiments_t1)\n",
    "        print('---------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply transfer learning to a list of topics identified in one dataset onto another dataset for consensus analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0 exposure within validation dataset:\n",
      "    Keywords: illegal aliens;united permission;permission illegal;enter united;aliens treated;aliens illegal;treated entered;entered legally\n",
      "    Consensus: 1.0\n",
      "---------------\n",
      "Topic 1 exposure within validation dataset:\n",
      "    Keywords: witch hunt;donald trump;rigged witch;frame donald;unfortunate situation;trump rigged;situation decided;decided frame\n",
      "    Consensus: 0.5652\n",
      "---------------\n",
      "Topic 2 exposure within validation dataset:\n",
      "    Keywords: trump campaign;lou dobbs;evidence collusion;dobbs special;democrats evidence;conflicts angry;collusion trump;campaign russia;angry democrats\n",
      "    Consensus: 1.0\n",
      "---------------\n",
      "Topic 3 exposure within validation dataset:\n",
      "    Keywords: peter strzok;fired fbi;agent peter;strzok fraud;rigged investigation;investigation started;fraud rigged;fbi agent\n",
      "    Consensus: 1.0\n",
      "---------------\n",
      "Topic 4 exposure within validation dataset:\n",
      "    Keywords: crooked hillary;strzok fbi;hillary clinton;sham investigation;fired agent;fbi charge;clinton sham;charge crooked;agent strzok\n",
      "    Consensus: 0.6549\n",
      "---------------\n",
      "Topic 5 exposure within validation dataset:\n",
      "    Keywords: trade deals;tariffs tariffs;tariffs leading;leading great;great trade;deals opposed;country built;built tariffs\n",
      "    Consensus: 1.0\n",
      "---------------\n",
      "Topic 6 exposure within validation dataset:\n",
      "    Keywords: great win;win night;wisconsin great;vukmir wisconsin;leah vukmir;congratulations leah;walker special;special great\n",
      "    Consensus: 0.871\n",
      "---------------\n",
      "Topic 7 exposure within validation dataset:\n",
      "    Keywords: strzok firing;wrongs committed;step direction;skinny circle;firing decisive;direction correcting;decisive step;correcting wrongs;committed comey;comey skinny;action strzok\n",
      "    Consensus: 1.0\n",
      "---------------\n"
     ]
    }
   ],
   "source": [
    "dataset0 = 'trump_tweets'\n",
    "dataset1 = None # str | Validation dataset (optional if period_0 and period_1 dates provided)\n",
    "#query = '(\"Trump\" OR \"president\")' # str | Fulltext query, using mysql MATCH boolean query format. Example, (\\\"word1\\\" OR \\\"word2\\\") AND (\\\"word3\\\" OR \\\"word4\\\") (optional)\n",
    "#fixed_topic is also an available input argument\n",
    "query = ''\n",
    "custom_stop_words = [\"\"] # str | List of stop words. (optional)\n",
    "num_topics = 8 # int | Number of topics to be extracted from the dataset. (optional) (default to 8)\n",
    "num_keywords = 8 # int | Number of keywords per topic that is extracted from the dataset. (optional) (default to 8)\n",
    "metadata_selection = \"\" # dict | JSON object specifying metadata-based queries on the dataset, of type {\"metadata_field\": \"selected_values\"} (optional)\n",
    "period_0_start = '2018-08-12' # Not needed if you provide a validation dataset in the \"dataset1\" variable \n",
    "period_0_end = '2018-08-16' # Not needed if you provide a validation dataset in the \"dataset1\" variable\n",
    "period_1_start = '2018-08-14' # Not needed if you provide a validation dataset in the \"dataset1\" variable\n",
    "period_1_end = '2019-08-18' # Not needed if you provide a validation dataset in the \"dataset1\" variable\n",
    "excluded_docs = '' # str | List of document IDs that should be excluded from the analysis. Example, [\"docid1\", \"docid2\", ..., \"docidN\"]  (optional)\n",
    "custom_dict_file = {\"great\": 1.0, \"awful\": -1.0, \"clinton\":-1.0, \"trump\":1.0} # file | Custom sentiment dictionary JSON file. Example, {\"field1\": value1, ..., \"fieldN\": valueN} (optional)\n",
    "remove_redundancies = False # bool | If True, this option removes quasi-duplicates from the analysis. A quasi-duplicate would have the same NLP representation, but not necessarily the exact same text. (optional) (default True)\n",
    "\n",
    "try:\n",
    "    payload = nucleus_api.TopicConsensusTransferModel(dataset0=dataset0,\n",
    "                                                      dataset1=dataset1,\n",
    "                                                      query=query,\n",
    "                                                      custom_stop_words=custom_stop_words, \n",
    "                                                      num_topics=num_topics, \n",
    "                                                      num_keywords=num_keywords,\n",
    "                                                      period_0_start=period_0_start,\n",
    "                                                      period_0_end=period_0_end,\n",
    "                                                      period_1_start=period_1_start,\n",
    "                                                      period_1_end=period_1_end,\n",
    "                                                      metadata_selection=metadata_selection,\n",
    "                                                      custom_dict_file=custom_dict_file)\n",
    "    \n",
    "    api_response = api_instance.post_topic_consensus_transfer_api(payload)\n",
    "    api_ok = True\n",
    "except ApiException as e:\n",
    "    api_error = json.loads(e.body)\n",
    "    print('ERROR:', api_error['message'])\n",
    "    api_ok = False\n",
    "\n",
    "if api_ok:\n",
    "    topics = api_response.result\n",
    "    for i,res in enumerate(topics):\n",
    "        print('Topic', i, 'exposure within validation dataset:')\n",
    "        print('    Keywords:', res.keywords)\n",
    "        print('    Consensus:', res.consensus)\n",
    "        print('---------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract a topic contrasting two subsets of content in a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR: If you are searching for specific terms, there are not enough matches to your filter. Please add additional search terms or check your documents are the correct ones. Otherwise, your documents are likely too short or too few. Please check the length of their content or add more documents.\n"
     ]
    }
   ],
   "source": [
    "dataset = 'trump_tweets' # str | Dataset name.\n",
    "metadata_selection = {\"content\": \"Trump\"} # dict | The metadata selection defining the two categories of documents to contrast and summarize against each other\n",
    "query = '' # str | Dataset-language-specific fulltext query, using mysql MATCH boolean query format (optional)\n",
    "custom_stop_words = [\"real\",\"hillary\"] # List of stop words. (optional)\n",
    "time_period = \"1M\" # str | Alternative 1: time period counting back from today over which the analysis is conducted (optional)\n",
    "period_start = '2018-08-12' # str | Alternative 2: start of period over which the analysis is conducted (optional)\n",
    "period_end = '2018-08-15' # str | Alternative 2: start of period over which the analysis is conducted (optional)\n",
    "excluded_docs = '' # str | List of document IDs that should be excluded from the analysis. Example, [\"docid1\", \"docid2\", ..., \"docidN\"]  (optional)\n",
    "syntax_variables = True # bool | Specifies whether to take into account syntax aspects of each category of documents to help with contrasting them (optional) (default to False)\n",
    "compression = 0.002 # float | Parameter controlling the breadth of the contrasted topic. Contained between 0 and 1, the smaller it is, the more contrasting terms will be captured, with decreasing weight. (optional) (default to 0.000002)\n",
    "remove_redundancies = False # bool | If True, this option removes quasi-duplicates from the analysis and retain only one copy of it. A quasi-duplicate would have the same NLP representation, but not necessarily the exact same text. (optional) (default False)\n",
    "metadata_selection_contrast = {\"content\": \"Trump\"}\n",
    "\n",
    "try:\n",
    "    payload = nucleus_api.TopicContrastModel(\n",
    "        dataset=dataset, \n",
    "        metadata_selection=metadata_selection,\n",
    "        metadata_selection_contrast=metadata_selection_contrast\n",
    "    )\n",
    "    api_response = api_instance.post_topic_contrast_api(payload)\n",
    "    \n",
    "    print('Contrasted Topic')\n",
    "    print('    Keywords:', api_response.result.keywords)\n",
    "    print('    Keywords Weight:', api_response.result.keywords_weight)\n",
    "    \n",
    "except ApiException as e:\n",
    "    api_error = json.loads(e.body)\n",
    "    print('ERROR:', api_error['message'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform documents-centered analysis\n",
    "\n",
    "This section goes over all APIs that enable users to analyze documents in a dataset on a standalone basis.\n",
    "- Sentiment analysis\n",
    "- Document summarization and contrasted summarization\n",
    "- Document classification\n",
    "- Named Entity recognition (strict match off pre-determined list)\n",
    "- Content recommendation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve the summary information of a list of documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document ID: 1562745438871302628\n",
      "    title: D_Trump2018_8_18_13_39\n",
      "    time : 2018-08-18 06:39:00\n",
      "    source : Twitter for iPhone\n",
      "    author : D_Trump2\n",
      "---------------\n",
      "Document ID: 12117038462682248474\n",
      "    title: D_Trump2018_8_18_13_34\n",
      "    time : 2018-08-18 06:34:00\n",
      "    source : Twitter for iPhone\n",
      "    author : D_Trump3\n",
      "---------------\n",
      "Document ID: 4065242729730211509\n",
      "    title: D_Trump2018_8_18_13_6\n",
      "    time : 2018-08-18 06:06:00\n",
      "    source : Twitter for iPhone\n",
      "    author : D_Trump5\n",
      "---------------\n",
      "Document ID: 4437198275925723018\n",
      "    title: D_Trump2018_8_18_1_46\n",
      "    time : 2018-08-17 17:00:00\n",
      "    source : Twitter for iPhone\n",
      "    author : D_Trump11\n",
      "---------------\n",
      "Document ID: 7148720741304645705\n",
      "    title: D_Trump2018_8_18_1_37\n",
      "    time : 2018-08-17 17:00:00\n",
      "    source : Twitter for iPhone\n",
      "    author : D_Trump12\n",
      "---------------\n",
      "Document ID: 14824073781424371042\n",
      "    title: D_Trump2018_8_17_22_29\n",
      "    time : 2018-08-17 15:29:00\n",
      "    source : Twitter for iPhone\n",
      "    author : D_Trump13\n",
      "---------------\n",
      "Document ID: 5337115965771373285\n",
      "    title: D_Trump2018_8_17_2_4\n",
      "    time : 2018-08-16 17:00:00\n",
      "    source : Twitter for iPhone\n",
      "    author : D_Trump26\n",
      "---------------\n",
      "Document ID: 13115764764473553109\n",
      "    title: D_Trump2018_8_17_1_54\n",
      "    time : 2018-08-16 17:00:00\n",
      "    source : Twitter for iPhone\n",
      "    author : D_Trump27\n",
      "---------------\n",
      "Document ID: 6624757382845353411\n",
      "    title: D_Trump2018_8_16_23_45\n",
      "    time : 2018-08-16 16:45:00\n",
      "    source : Twitter for iPhone\n",
      "    author : D_Trump32\n",
      "---------------\n",
      "Document ID: 6817024102264760936\n",
      "    title: D_Trump2018_8_16_23_37\n",
      "    time : 2018-08-16 16:37:00\n",
      "    source : Twitter for iPhone\n",
      "    author : D_Trump33\n",
      "---------------\n",
      "Document ID: 14885319269940915989\n",
      "    title: D_Trump2018_8_16_18_43\n",
      "    time : 2018-08-16 11:43:00\n",
      "    source : Twitter for iPhone\n",
      "    author : D_Trump36\n",
      "---------------\n",
      "Document ID: 10066536328057463840\n",
      "    title: D_Trump2018_8_16_15_36\n",
      "    time : 2018-08-16 08:36:00\n",
      "    source : Twitter for iPhone\n",
      "    author : D_Trump37\n",
      "---------------\n",
      "Document ID: 12437126850273027242\n",
      "    title: D_Trump2018_8_16_12_50\n",
      "    time : 2018-08-16 05:50:00\n",
      "    source : Twitter for iPhone\n",
      "    author : D_Trump40\n",
      "---------------\n",
      "Document ID: 13372108096006302125\n",
      "    title: D_Trump2018_8_18_19_39\n",
      "    time : 2018-08-18 12:39:00\n",
      "    source : Twitter for iPhone\n",
      "    author : D_Trump1\n",
      "---------------\n",
      "Document ID: 5734555862573956053\n",
      "    title: D_Trump2018_8_15_14_57\n",
      "    time : 2018-08-15 07:57:00\n",
      "    source : Twitter for iPhone\n",
      "    author : D_Trump51\n",
      "---------------\n",
      "Document ID: 5197981696652006791\n",
      "    title: D_Trump2018_8_15_13_31\n",
      "    time : 2018-08-15 06:31:00\n",
      "    source : Twitter for iPhone\n",
      "    author : D_Trump56\n",
      "---------------\n",
      "Document ID: 427773076591013776\n",
      "    title: D_Trump2018_8_18_13_12\n",
      "    time : 2018-08-18 06:12:00\n",
      "    source : Twitter for iPhone\n",
      "    author : D_Trump4\n",
      "---------------\n",
      "Document ID: 5968111755819154687\n",
      "    title: D_Trump2018_8_18_11_46\n",
      "    time : 2018-08-18 04:46:00\n",
      "    source : Twitter for iPhone\n",
      "    author : D_Trump6\n",
      "---------------\n",
      "Document ID: 2485430564355641796\n",
      "    title: D_Trump2018_8_18_11_40\n",
      "    time : 2018-08-18 04:40:00\n",
      "    source : Twitter for iPhone\n",
      "    author : D_Trump7\n",
      "---------------\n",
      "Document ID: 1436959758999674726\n",
      "    title: D_Trump2018_8_18_11_32\n",
      "    time : 2018-08-18 04:32:00\n",
      "    source : Twitter for iPhone\n",
      "    author : D_Trump8\n",
      "---------------\n",
      "Document ID: 10182645611171771227\n",
      "    title: D_Trump2018_8_18_11_23\n",
      "    time : 2018-08-18 04:23:00\n",
      "    source : Twitter for iPhone\n",
      "    author : D_Trump9\n",
      "---------------\n",
      "Document ID: 4584670957973383500\n",
      "    title: D_Trump2018_8_17_1_49\n",
      "    time : 2018-08-16 17:00:00\n",
      "    source : Twitter for iPhone\n",
      "    author : D_Trump28\n",
      "---------------\n",
      "Document ID: 2463565422226136907\n",
      "    title: D_Trump2018_8_17_0_56\n",
      "    time : 2018-08-16 17:00:00\n",
      "    source : Twitter for iPhone\n",
      "    author : D_Trump29\n",
      "---------------\n",
      "Document ID: 7508075727870080337\n",
      "    title: D_Trump2018_8_17_0_45\n",
      "    time : 2018-08-16 17:00:00\n",
      "    source : Twitter for iPhone\n",
      "    author : D_Trump30\n",
      "---------------\n",
      "Document ID: 5294521276627635033\n",
      "    title: D_Trump2018_8_16_23_53\n",
      "    time : 2018-08-16 16:53:00\n",
      "    source : Twitter for iPhone\n",
      "    author : D_Trump31\n",
      "---------------\n",
      "Document ID: 9918250277785954786\n",
      "    title: D_Trump2018_8_16_12_43\n",
      "    time : 2018-08-16 05:43:00\n",
      "    source : Twitter for iPhone\n",
      "    author : D_Trump41\n",
      "---------------\n",
      "Document ID: 9210234284146319434\n",
      "    title: D_Trump2018_8_16_2_2\n",
      "    time : 2018-08-15 17:00:00\n",
      "    source : Twitter for iPhone\n",
      "    author : D_Trump44\n",
      "---------------\n",
      "Document ID: 2497347111761266654\n",
      "    title: D_Trump2018_8_15_14_15\n",
      "    time : 2018-08-15 07:15:00\n",
      "    source : Twitter for iPhone\n",
      "    author : D_Trump52\n",
      "---------------\n",
      "Document ID: 3295270030761385310\n",
      "    title: D_Trump2018_8_15_14_8\n",
      "    time : 2018-08-15 07:08:00\n",
      "    source : Twitter for iPhone\n",
      "    author : D_Trump53\n",
      "---------------\n",
      "Document ID: 10022945975448422485\n",
      "    title: D_Trump2018_8_15_13_51\n",
      "    time : 2018-08-15 06:51:00\n",
      "    source : Twitter for iPhone\n",
      "    author : D_Trump54\n",
      "---------------\n",
      "Document ID: 5501029233133759464\n",
      "    title: D_Trump2018_8_15_13_39\n",
      "    time : 2018-08-15 06:39:00\n",
      "    source : Twitter for iPhone\n",
      "    author : D_Trump55\n",
      "---------------\n",
      "Document ID: 15284262488783149410\n",
      "    title: D_Trump2018_8_15_13_14\n",
      "    time : 2018-08-15 06:14:00\n",
      "    source : Twitter for iPhone\n",
      "    author : D_Trump58\n",
      "---------------\n",
      "Document ID: 16020903065363723393\n",
      "    title: D_Trump2018_8_13_20_42\n",
      "    time : 2018-08-13 13:42:00\n",
      "    source : Twitter for iPhone\n",
      "    author : D_Trump83\n",
      "---------------\n",
      "Document ID: 1323691717445942922\n",
      "    title: D_Trump2018_8_13_19_37\n",
      "    time : 2018-08-13 12:37:00\n",
      "    source : Twitter for iPhone\n",
      "    author : D_Trump84\n",
      "---------------\n",
      "Document ID: 6827313427448806555\n",
      "    title: D_Trump2018_8_13_18_19\n",
      "    time : 2018-08-13 11:19:00\n",
      "    source : Twitter for iPhone\n",
      "    author : D_Trump85\n",
      "---------------\n",
      "Document ID: 2123066111701114595\n",
      "    title: D_Trump2018_8_13_17_14\n",
      "    time : 2018-08-13 10:14:00\n",
      "    source : Twitter for iPad\n",
      "    author : D_Trump86\n",
      "---------------\n",
      "Document ID: 16227381323440672186\n",
      "    title: D_Trump2018_8_13_17_14\n",
      "    time : 2018-08-13 10:14:00\n",
      "    source : Twitter for iPad\n",
      "    author : D_Trump87\n",
      "---------------\n",
      "Document ID: 13751117731643751516\n",
      "    title: D_Trump2018_8_13_17_11\n",
      "    time : 2018-08-13 10:11:00\n",
      "    source : Twitter for iPad\n",
      "    author : D_Trump88\n",
      "---------------\n",
      "Document ID: 14171720646613873024\n",
      "    title: D_Trump2018_8_13_16_13\n",
      "    time : 2018-08-13 09:13:00\n",
      "    source : Twitter for iPhone\n",
      "    author : D_Trump89\n",
      "---------------\n",
      "Document ID: 1359168824680488548\n",
      "    title: D_Trump2018_8_13_16_9\n",
      "    time : 2018-08-13 09:09:00\n",
      "    source : Twitter for iPhone\n",
      "    author : D_Trump90\n",
      "---------------\n",
      "Document ID: 2654048984553148490\n",
      "    title: D_Trump2018_8_13_16_4\n",
      "    time : 2018-08-13 09:04:00\n",
      "    source : Twitter for iPhone\n",
      "    author : D_Trump91\n",
      "---------------\n",
      "Document ID: 2616181455934535230\n",
      "    title: D_Trump2018_8_16_2_40\n",
      "    time : 2018-08-15 17:00:00\n",
      "    source : Twitter for iPhone\n",
      "    author : D_Trump42\n",
      "---------------\n",
      "Document ID: 5336597423569459183\n",
      "    title: D_Trump2018_8_16_2_31\n",
      "    time : 2018-08-15 17:00:00\n",
      "    source : Twitter for iPhone\n",
      "    author : D_Trump43\n",
      "---------------\n",
      "Document ID: 2692713684604938265\n",
      "    title: D_Trump2018_8_15_13_18\n",
      "    time : 2018-08-15 06:18:00\n",
      "    source : Twitter for iPhone\n",
      "    author : D_Trump57\n",
      "---------------\n",
      "Document ID: 9995371484769371289\n",
      "    title: D_Trump2018_8_15_13_12\n",
      "    time : 2018-08-15 06:12:00\n",
      "    source : Twitter for iPhone\n",
      "    author : D_Trump59\n",
      "---------------\n",
      "Document ID: 16578282956147621208\n",
      "    title: D_Trump2018_8_14_12_6\n",
      "    time : 2018-08-14 05:06:00\n",
      "    source : Twitter for iPhone\n",
      "    author : D_Trump69\n",
      "---------------\n",
      "Document ID: 15178489020767442896\n",
      "    title: D_Trump2018_8_14_11_55\n",
      "    time : 2018-08-14 04:55:00\n",
      "    source : Twitter for iPhone\n",
      "    author : D_Trump70\n",
      "---------------\n",
      "Document ID: 18054399480757330387\n",
      "    title: D_Trump2018_8_14_11_42\n",
      "    time : 2018-08-14 04:42:00\n",
      "    source : Twitter for iPhone\n",
      "    author : D_Trump71\n",
      "---------------\n",
      "Document ID: 7552668845228134705\n",
      "    title: D_Trump2018_8_14_11_31\n",
      "    time : 2018-08-14 04:31:00\n",
      "    source : Twitter for iPhone\n",
      "    author : D_Trump72\n",
      "---------------\n",
      "Document ID: 15091083406720907936\n",
      "    title: D_Trump2018_8_14_11_21\n",
      "    time : 2018-08-14 04:21:00\n",
      "    source : Twitter for iPhone\n",
      "    author : D_Trump73\n",
      "---------------\n",
      "Document ID: 2473572527244144057\n",
      "    title: D_Trump2018_8_16_1_53\n",
      "    time : 2018-08-15 17:00:00\n",
      "    source : Twitter for iPhone\n",
      "    author : D_Trump45\n",
      "---------------\n",
      "Document ID: 5983550775501810643\n",
      "    title: D_Trump2018_8_16_1_14\n",
      "    time : 2018-08-15 17:00:00\n",
      "    source : Twitter for iPhone\n",
      "    author : D_Trump46\n",
      "---------------\n",
      "Document ID: 9220064279401475464\n",
      "    title: D_Trump2018_8_16_1_0\n",
      "    time : 2018-08-15 17:00:00\n",
      "    source : Twitter for iPhone\n",
      "    author : D_Trump47\n",
      "---------------\n",
      "Document ID: 11067191221354407080\n",
      "    title: D_Trump2018_8_15_19_52\n",
      "    time : 2018-08-15 12:52:00\n",
      "    source : Twitter for iPhone\n",
      "    author : D_Trump48\n",
      "---------------\n",
      "Document ID: 7650310364039694459\n",
      "    title: D_Trump2018_8_15_18_34\n",
      "    time : 2018-08-15 11:34:00\n",
      "    source : Twitter for iPhone\n",
      "    author : D_Trump49\n",
      "---------------\n",
      "Document ID: 16832443168363937194\n",
      "    title: D_Trump2018_8_15_15_4\n",
      "    time : 2018-08-15 08:04:00\n",
      "    source : Twitter for iPhone\n",
      "    author : D_Trump50\n",
      "---------------\n",
      "Document ID: 12996919858778233473\n",
      "    title: D_Trump2018_8_15_13_7\n",
      "    time : 2018-08-15 06:07:00\n",
      "    source : Twitter for iPhone\n",
      "    author : D_Trump60\n",
      "---------------\n",
      "Document ID: 17583854768923684322\n",
      "    title: D_Trump2018_8_15_13_2\n",
      "    time : 2018-08-15 06:02:00\n",
      "    source : Twitter for iPhone\n",
      "    author : D_Trump61\n",
      "---------------\n",
      "Document ID: 14286875122783287072\n",
      "    title: D_Trump2018_8_15_12_54\n",
      "    time : 2018-08-15 05:54:00\n",
      "    source : Twitter for iPhone\n",
      "    author : D_Trump62\n",
      "---------------\n",
      "Document ID: 9742569426091859106\n",
      "    title: D_Trump2018_8_15_12_44\n",
      "    time : 2018-08-15 05:44:00\n",
      "    source : Twitter for iPhone\n",
      "    author : D_Trump63\n",
      "---------------\n",
      "Document ID: 10221017677007391285\n",
      "    title: D_Trump2018_8_15_12_30\n",
      "    time : 2018-08-15 05:30:00\n",
      "    source : Twitter for iPhone\n",
      "    author : D_Trump64\n",
      "---------------\n",
      "Document ID: 10786162767269824082\n",
      "    title: D_Trump2018_8_14_18_21\n",
      "    time : 2018-08-14 11:21:00\n",
      "    source : Twitter for iPhone\n",
      "    author : D_Trump65\n",
      "---------------\n",
      "Document ID: 7290488938710124315\n",
      "    title: D_Trump2018_8_14_13_15\n",
      "    time : 2018-08-14 06:15:00\n",
      "    source : Twitter for iPhone\n",
      "    author : D_Trump66\n",
      "---------------\n",
      "Document ID: 7721974971266362305\n",
      "    title: D_Trump2018_8_18_1_47\n",
      "    time : 2018-08-17 17:00:00\n",
      "    source : Twitter for iPhone\n",
      "    author : D_Trump10\n",
      "---------------\n",
      "Document ID: 17055305115598973412\n",
      "    title: D_Trump2018_8_14_11_13\n",
      "    time : 2018-08-14 04:13:00\n",
      "    source : Twitter for iPhone\n",
      "    author : D_Trump74\n",
      "---------------\n",
      "Document ID: 10908843577559309852\n",
      "    title: D_Trump2018_8_17_14_6\n",
      "    time : 2018-08-17 07:06:00\n",
      "    source : Twitter for iPhone\n",
      "    author : D_Trump19\n",
      "---------------\n",
      "Document ID: 17061176723846963603\n",
      "    title: D_Trump2018_8_17_12_29\n",
      "    time : 2018-08-17 05:29:00\n",
      "    source : Twitter for iPhone\n",
      "    author : D_Trump21\n",
      "---------------\n",
      "Document ID: 16119072178444509629\n",
      "    title: D_Trump2018_8_17_12_10\n",
      "    time : 2018-08-17 05:10:00\n",
      "    source : Twitter for iPhone\n",
      "    author : D_Trump22\n",
      "---------------\n",
      "Document ID: 12816349349621722463\n",
      "    title: D_Trump2018_8_17_11_57\n",
      "    time : 2018-08-17 04:57:00\n",
      "    source : Twitter for iPhone\n",
      "    author : D_Trump23\n",
      "---------------\n",
      "Document ID: 9687935645391984655\n",
      "    title: D_Trump2018_8_17_11_44\n",
      "    time : 2018-08-17 04:44:00\n",
      "    source : Twitter for iPhone\n",
      "    author : D_Trump24\n",
      "---------------\n",
      "Document ID: 3939433634160758993\n",
      "    title: D_Trump2018_8_16_23_30\n",
      "    time : 2018-08-16 16:30:00\n",
      "    source : Twitter for iPhone\n",
      "    author : D_Trump34\n",
      "---------------\n",
      "Document ID: 1756534205982635438\n",
      "    title: D_Trump2018_8_16_18_55\n",
      "    time : 2018-08-16 11:55:00\n",
      "    source : Twitter for iPhone\n",
      "    author : D_Trump35\n",
      "---------------\n",
      "Document ID: 14368578770105844206\n",
      "    title: D_Trump2018_8_16_14_10\n",
      "    time : 2018-08-16 07:10:00\n",
      "    source : Twitter for iPhone\n",
      "    author : D_Trump38\n",
      "---------------\n",
      "Document ID: 157671510549269746\n",
      "    title: D_Trump2018_8_16_14_0\n",
      "    time : 2018-08-16 07:00:00\n",
      "    source : Twitter for iPhone\n",
      "    author : D_Trump39\n",
      "---------------\n",
      "Document ID: 7057528309914044758\n",
      "    title: D_Trump2018_8_14_11_7\n",
      "    time : 2018-08-14 04:07:00\n",
      "    source : Twitter for iPhone\n",
      "    author : D_Trump75\n",
      "---------------\n",
      "Document ID: 18298582467582872832\n",
      "    title: D_Trump2018_8_17_19_25\n",
      "    time : 2018-08-17 12:25:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    source : Twitter for iPhone\n",
      "    author : D_Trump15\n",
      "---------------\n",
      "Document ID: 4046653213651213725\n",
      "    title: D_Trump2018_8_17_19_25\n",
      "    time : 2018-08-17 12:25:00\n",
      "    source : Twitter for iPhone\n",
      "    author : D_Trump16\n",
      "---------------\n",
      "Document ID: 4349543959441164718\n",
      "    title: D_Trump2018_8_17_14_17\n",
      "    time : 2018-08-17 07:17:00\n",
      "    source : Twitter for iPhone\n",
      "    author : D_Trump17\n",
      "---------------\n",
      "Document ID: 681266336486897660\n",
      "    title: D_Trump2018_8_17_14_10\n",
      "    time : 2018-08-17 07:10:00\n",
      "    source : Twitter for iPhone\n",
      "    author : D_Trump18\n",
      "---------------\n",
      "Document ID: 10570509452130354407\n",
      "    title: D_Trump2018_8_17_12_38\n",
      "    time : 2018-08-17 05:38:00\n",
      "    source : Twitter for iPhone\n",
      "    author : D_Trump20\n",
      "---------------\n",
      "Document ID: 13125097700312206949\n",
      "    title: D_Trump2018_8_14_13_10\n",
      "    time : 2018-08-14 06:10:00\n",
      "    source : Twitter for iPhone\n",
      "    author : D_Trump67\n",
      "---------------\n",
      "Document ID: 10234332166402996482\n",
      "    title: D_Trump2018_8_14_13_1\n",
      "    time : 2018-08-14 06:01:00\n",
      "    source : Twitter for iPhone\n",
      "    author : D_Trump68\n",
      "---------------\n",
      "Document ID: 16402127651644187533\n",
      "    title: D_Trump2018_8_13_15_12\n",
      "    time : 2018-08-13 08:12:00\n",
      "    source : Twitter for iPhone\n",
      "    author : D_Trump92\n",
      "---------------\n",
      "Document ID: 3946302249027831785\n",
      "    title: D_Trump2018_8_13_14_21\n",
      "    time : 2018-08-13 07:21:00\n",
      "    source : Twitter for iPhone\n",
      "    author : D_Trump93\n",
      "---------------\n",
      "Document ID: 13387225095901658901\n",
      "    title: D_Trump2018_8_12_1_51\n",
      "    time : 2018-08-11 17:00:00\n",
      "    source : Twitter for iPhone\n",
      "    author : D_Trump100\n",
      "---------------\n",
      "Document ID: 3951288681850631775\n",
      "    title: D_Trump2018_8_17_11_30\n",
      "    time : 2018-08-17 04:30:00\n",
      "    source : Twitter for iPhone\n",
      "    author : D_Trump25\n",
      "---------------\n",
      "Document ID: 17249561681240116508\n",
      "    title: D_Trump2018_8_13_13_50\n",
      "    time : 2018-08-13 06:50:00\n",
      "    source : Twitter for iPhone\n",
      "    author : D_Trump94\n",
      "---------------\n",
      "Document ID: 9740888651068443724\n",
      "    title: D_Trump2018_8_13_13_27\n",
      "    time : 2018-08-13 06:27:00\n",
      "    source : Twitter for iPhone\n",
      "    author : D_Trump95\n",
      "---------------\n",
      "Document ID: 17901578373306652528\n",
      "    title: D_Trump2018_8_12_13_25\n",
      "    time : 2018-08-12 06:25:00\n",
      "    source : Twitter for iPhone\n",
      "    author : D_Trump96\n",
      "---------------\n",
      "Document ID: 11502473416451875111\n",
      "    title: D_Trump2018_8_12_12_57\n",
      "    time : 2018-08-12 05:57:00\n",
      "    source : Twitter for iPhone\n",
      "    author : D_Trump97\n",
      "---------------\n",
      "Document ID: 8324397691013960207\n",
      "    title: D_Trump2018_8_12_12_34\n",
      "    time : 2018-08-12 05:34:00\n",
      "    source : Twitter for iPhone\n",
      "    author : D_Trump98\n",
      "---------------\n",
      "Document ID: 13206638373098679225\n",
      "    title: D_Trump2018_8_12_12_24\n",
      "    time : 2018-08-12 05:24:00\n",
      "    source : Twitter for iPhone\n",
      "    author : D_Trump99\n",
      "---------------\n",
      "Document ID: 5436841123252652095\n",
      "    title: D_Trump2018_8_17_21_25\n",
      "    time : 2018-08-17 14:25:00\n",
      "    source : Twitter for iPhone\n",
      "    author : D_Trump14\n",
      "---------------\n",
      "Document ID: 46802578687162219\n",
      "    title: D_Trump2018_8_14_10_59\n",
      "    time : 2018-08-14 03:59:00\n",
      "    source : Twitter for iPhone\n",
      "    author : D_Trump76\n",
      "---------------\n",
      "Document ID: 11599059962808147623\n",
      "    title: D_Trump2018_8_14_1_57\n",
      "    time : 2018-08-13 17:00:00\n",
      "    source : Twitter for iPhone\n",
      "    author : D_Trump77\n",
      "---------------\n",
      "Document ID: 6584380602911769482\n",
      "    title: D_Trump2018_8_14_1_50\n",
      "    time : 2018-08-13 17:00:00\n",
      "    source : Twitter for iPhone\n",
      "    author : D_Trump78\n",
      "---------------\n",
      "Document ID: 3324138947484704515\n",
      "    title: D_Trump2018_8_14_1_37\n",
      "    time : 2018-08-13 17:00:00\n",
      "    source : Twitter for iPhone\n",
      "    author : D_Trump79\n",
      "---------------\n",
      "Document ID: 8932963129356075453\n",
      "    title: D_Trump2018_8_14_1_37\n",
      "    time : 2018-08-13 17:00:00\n",
      "    source : Twitter for iPhone\n",
      "    author : D_Trump80\n",
      "---------------\n",
      "Document ID: 15063183167414362421\n",
      "    title: D_Trump2018_8_14_0_11\n",
      "    time : 2018-08-13 17:00:00\n",
      "    source : Twitter for iPhone\n",
      "    author : D_Trump81\n",
      "---------------\n",
      "Document ID: 6772114335584387757\n",
      "    title: D_Trump2018_8_13_22_36\n",
      "    time : 2018-08-13 15:36:00\n",
      "    source : Twitter for iPhone\n",
      "    author : D_Trump82\n",
      "---------------\n"
     ]
    }
   ],
   "source": [
    "dataset = 'trump_tweets'\n",
    "# doc_titles, doc_ids, and metadata_selection below are filters to narrow down \n",
    "# documents to be retrieved.\n",
    "# The information of all documents will be retrived when no filters are provided.\n",
    "\n",
    "# doc_titles: list of strings\n",
    "# The titles of the documents to retrieve. Example: [\"title1\", \"title2\", ..., \"titleN\"]  (optional)\n",
    "# doc_titles = ['D_Trump2018_8_18_1_47']   \n",
    "doc_titles = []\n",
    "# doc_ids: list of strings\n",
    "# The docid of the documents to retrieve. Example: [\"docid1\", \"docid2\", ..., \"docidN\"]  (optional)\n",
    "# doc_ids = ['3397215194896514820', '776902852041351634']\n",
    "doc_ids = []\n",
    "\n",
    "# metadata_selection = {\"author\": \"D_Trump16\"} # dict | A selector off metadata. Example: {\"field\": \"value\"}  (optional)\n",
    "metadata_selection = ''\n",
    "\n",
    "try:\n",
    "    payload = nucleus_api.DocInfo(dataset=dataset, \n",
    "                                doc_titles=doc_titles, \n",
    "                                doc_ids=doc_ids,\n",
    "                                metadata_selection=metadata_selection)\n",
    "    api_response = api_instance.post_doc_info(payload)\n",
    "except ApiException as e:\n",
    "    api_error = json.loads(e.body)\n",
    "    print('ERROR:', api_error['message'])\n",
    "\n",
    "for res in api_response.result:\n",
    "    print('Document ID:', res.doc_id)\n",
    "    print('    title:', res.title)\n",
    "    for attr in res.attribute.keys():\n",
    "        if attr == 'time':\n",
    "            print('   ', attr, ':', datetime.datetime.fromtimestamp(float(res.attribute[attr])))\n",
    "        else:\n",
    "            print('   ', attr, ':', res.attribute[attr])\n",
    "    print('---------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve the summary information of a list of documents with a metadata selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document ID: 4046653213651213725\n",
      "    title: D_Trump2018_8_17_19_25\n",
      "    time : 2018-08-17 12:25:00\n",
      "    source : Twitter for iPhone\n",
      "    author : D_Trump16\n",
      "---------------\n"
     ]
    }
   ],
   "source": [
    "dataset = 'trump_tweets' # str | Dataset name.\n",
    "metadata_selection = {\"author\": \"D_Trump16\"}      # dict | A selector off metadata. Example: {\"field\": \"value\"}  (optional)\n",
    "\n",
    "try:\n",
    "    payload = nucleus_api.DocInfo(dataset=dataset, metadata_selection=metadata_selection)\n",
    "    api_response = api_instance.post_doc_info(payload)\n",
    "    \n",
    "except ApiException as e:\n",
    "    api_error = json.loads(e.body)\n",
    "    print('ERROR:', api_error['message'])\n",
    "\n",
    "for res in api_response.result:\n",
    "    print('Document ID:', res.doc_id)\n",
    "    print('    title:', res.title)\n",
    "    for attr in res.attribute.keys():\n",
    "        if attr == 'time':\n",
    "            print('   ', attr, ':', datetime.datetime.fromtimestamp(float(res.attribute[attr])))\n",
    "        else:\n",
    "            print('   ', attr, ':', res.attribute[attr])\n",
    "    print('---------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display chosen documents, content and metadata included"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document ID: 4046653213651213725\n",
      "    Title: D_Trump2018_8_17_19_25\n",
      "    Author: D_Trump16\n",
      "    Time: 2018-08-17 12:25:00\n",
      "    Content Which is worse Hightax Andrew Cuomo's statement WERE NOT GOING TO MAKE AMERICA GREAT AGAIN IT WAS NEVER THAT GREAT or Hillary Clintons DEPLORABLES statement...\n",
      "---------------\n"
     ]
    }
   ],
   "source": [
    "dataset = 'trump_tweets' # str | Dataset name.\n",
    "#doc_titles = ['D_Trump2018_8_18_1_47']   # str | The title of the documents to retrieve. Example: [\"title1\", \"title2\", ..., \"titleN\"]  (optional)\n",
    "doc_ids = ['4046653213651213725']      # str | The docid of the documents to retrieve. Example: [\"docid1\", \"docid2\", ..., \"docidN\"]  (optional)\n",
    "\n",
    "try:\n",
    "    payload = nucleus_api.DocDisplay(dataset, doc_ids=doc_ids)\n",
    "    api_response = api_instance.post_doc_display(payload)\n",
    "    \n",
    "except ApiException as e:\n",
    "    api_error = json.loads(e.body)\n",
    "    print('ERROR:', api_error['message'])\n",
    "\n",
    "for res in api_response.result:\n",
    "    print('Document ID:', res.doc_id)\n",
    "    print('    Title:', res.title)\n",
    "    print('    Author:', res.attribute['author'])\n",
    "    print('    Time:', datetime.datetime.fromtimestamp(float(res.attribute['time'])))\n",
    "    print('    Content', res.attribute['content'])\n",
    "    print('---------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display chosen documents, content and metadata included, with a metadata selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document ID: 4046653213651213725\n",
      "    Title: D_Trump2018_8_17_19_25\n",
      "    Author: D_Trump16\n",
      "    Time: 2018-08-17 12:25:00\n",
      "    Content Which is worse Hightax Andrew Cuomo's statement WERE NOT GOING TO MAKE AMERICA GREAT AGAIN IT WAS NEVER THAT GREAT or Hillary Clintons DEPLORABLES statement...\n",
      "---------------\n"
     ]
    }
   ],
   "source": [
    "dataset = 'trump_tweets' # str | Dataset name.\n",
    "metadata_selection = {\"author\": \"D_Trump16\"}      # dict | A selector off metadata. Example: {\"field\": \"value\"}  (optional)\n",
    "\n",
    "try:\n",
    "    payload = nucleus_api.DocDisplay(dataset=dataset, metadata_selection=metadata_selection)\n",
    "    api_response = api_instance.post_doc_display(payload)\n",
    "    \n",
    "except ApiException as e:\n",
    "    api_error = json.loads(e.body)\n",
    "    print('ERROR:', api_error['message'])\n",
    "\n",
    "for res in api_response.result:\n",
    "    print('Document ID:', res.doc_id)\n",
    "    print('    Title:', res.title)\n",
    "    print('    Author:', res.attribute['author'])\n",
    "    print('    Time:', datetime.datetime.fromtimestamp(float(res.attribute['time'])))\n",
    "    print('    Content', res.attribute['content'])\n",
    "    print('---------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate document recommendations on a list of topics in a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document recommendations for topic 0 :\n",
      "    Keywords: illegal aliens;united permission;permission illegal;enter united;aliens treated;aliens illegal;treated entered;entered legally\n",
      "    Recommendation 0 :\n",
      "        Document ID: 2692713684604938265\n",
      "        Title: D_Trump2018_8_15_13_18\n",
      "        Attribute: {'docid': 2692713684604938265, 'author': 'D_Trump57', 'time': 1534339080, 'source': 'Twitter for iPhone'}\n",
      "        Author: D_Trump57\n",
      "        Time: 2018-08-15 06:18:00\n",
      "    Recommendation 1 :\n",
      "        Document ID: 9742569426091859106\n",
      "        Title: D_Trump2018_8_15_12_44\n",
      "        Attribute: {'docid': 9742569426091859106, 'author': 'D_Trump63', 'time': 1534337040, 'source': 'Twitter for iPhone'}\n",
      "        Author: D_Trump63\n",
      "        Time: 2018-08-15 05:44:00\n",
      "---------------\n",
      "Document recommendations for topic 1 :\n",
      "    Keywords: bruce ohr;ohr doj;legal jeopardy;doj legal;ohr justice;justice department;christopher steele;jeopardy astonishingthat;jeopardy astonishing;astonishingthat employed;astonishing employed\n",
      "    Recommendation 0 :\n",
      "        Document ID: 6817024102264760936\n",
      "        Title: D_Trump2018_8_16_23_37\n",
      "        Attribute: {'docid': 6817024102264760936, 'author': 'D_Trump33', 'time': 1534462620, 'source': 'Twitter for iPhone'}\n",
      "        Author: D_Trump33\n",
      "        Time: 2018-08-16 16:37:00\n",
      "    Recommendation 1 :\n",
      "        Document ID: 15178489020767442896\n",
      "        Title: D_Trump2018_8_14_11_55\n",
      "        Attribute: {'docid': 15178489020767442896, 'author': 'D_Trump70', 'time': 1534247700, 'source': 'Twitter for iPhone'}\n",
      "        Author: D_Trump70\n",
      "        Time: 2018-08-14 04:55:00\n",
      "    Recommendation 2 :\n",
      "        Document ID: 7148720741304645705\n",
      "        Title: D_Trump2018_8_18_1_37\n",
      "        Attribute: {'docid': 7148720741304645705, 'author': 'D_Trump12', 'time': 1534550400, 'source': 'Twitter for iPhone'}\n",
      "        Author: D_Trump12\n",
      "        Time: 2018-08-17 17:00:00\n",
      "    Recommendation 3 :\n",
      "        Document ID: 4437198275925723018\n",
      "        Title: D_Trump2018_8_18_1_46\n",
      "        Attribute: {'docid': 4437198275925723018, 'author': 'D_Trump11', 'time': 1534550400, 'source': 'Twitter for iPhone'}\n",
      "        Author: D_Trump11\n",
      "        Time: 2018-08-17 17:00:00\n",
      "---------------\n",
      "Document recommendations for topic 2 :\n",
      "    Keywords: america great;statement america;andrew cuomo;great great;worse hightax;hightax andrew;deplorables statement;cuomo statement;clinton deplorables\n",
      "    Recommendation 0 :\n",
      "        Document ID: 4046653213651213725\n",
      "        Title: D_Trump2018_8_17_19_25\n",
      "        Attribute: {'docid': 4046653213651213725, 'author': 'D_Trump16', 'time': 1534533900, 'source': 'Twitter for iPhone'}\n",
      "        Author: D_Trump16\n",
      "        Time: 2018-08-17 12:25:00\n",
      "    Recommendation 1 :\n",
      "        Document ID: 681266336486897660\n",
      "        Title: D_Trump2018_8_17_14_10\n",
      "        Attribute: {'docid': 681266336486897660, 'author': 'D_Trump18', 'time': 1534515000, 'source': 'Twitter for iPhone'}\n",
      "        Author: D_Trump18\n",
      "        Time: 2018-08-17 07:10:00\n",
      "    Recommendation 2 :\n",
      "        Document ID: 9687935645391984655\n",
      "        Title: D_Trump2018_8_17_11_44\n",
      "        Attribute: {'docid': 9687935645391984655, 'author': 'D_Trump24', 'time': 1534506240, 'source': 'Twitter for iPhone'}\n",
      "        Author: D_Trump24\n",
      "        Time: 2018-08-17 04:44:00\n",
      "    Recommendation 3 :\n",
      "        Document ID: 4349543959441164718\n",
      "        Title: D_Trump2018_8_17_14_17\n",
      "        Attribute: {'docid': 4349543959441164718, 'author': 'D_Trump17', 'time': 1534515420, 'source': 'Twitter for iPhone'}\n",
      "        Author: D_Trump17\n",
      "        Time: 2018-08-17 07:17:00\n",
      "---------------\n",
      "Document recommendations for topic 3 :\n",
      "    Keywords: peter strzok;fired fbi;agent peter;fbi agent;strzok fraud;rigged investigation;investigation started;fraud rigged\n",
      "    Recommendation 0 :\n",
      "        Document ID: 10234332166402996482\n",
      "        Title: D_Trump2018_8_14_13_1\n",
      "        Attribute: {'docid': 10234332166402996482, 'author': 'D_Trump68', 'time': 1534251660, 'source': 'Twitter for iPhone'}\n",
      "        Author: D_Trump68\n",
      "        Time: 2018-08-14 06:01:00\n",
      "    Recommendation 1 :\n",
      "        Document ID: 2654048984553148490\n",
      "        Title: D_Trump2018_8_13_16_4\n",
      "        Attribute: {'docid': 2654048984553148490, 'author': 'D_Trump91', 'time': 1534176240, 'source': 'Twitter for iPhone'}\n",
      "        Author: D_Trump91\n",
      "        Time: 2018-08-13 09:04:00\n",
      "    Recommendation 2 :\n",
      "        Document ID: 6817024102264760936\n",
      "        Title: D_Trump2018_8_16_23_37\n",
      "        Attribute: {'docid': 6817024102264760936, 'author': 'D_Trump33', 'time': 1534462620, 'source': 'Twitter for iPhone'}\n",
      "        Author: D_Trump33\n",
      "        Time: 2018-08-16 16:37:00\n",
      "---------------\n",
      "Document recommendations for topic 4 :\n",
      "    Keywords: trump campaign;lou dobbs;evidence collusion;dobbs special;democrats evidence;conflicts angry;collusion trump;campaign russia;angry democrats\n",
      "    Recommendation 0 :\n",
      "        Document ID: 7290488938710124315\n",
      "        Title: D_Trump2018_8_14_13_15\n",
      "        Attribute: {'docid': 7290488938710124315, 'author': 'D_Trump66', 'time': 1534252500, 'source': 'Twitter for iPhone'}\n",
      "        Author: D_Trump66\n",
      "        Time: 2018-08-14 06:15:00\n",
      "    Recommendation 1 :\n",
      "        Document ID: 15091083406720907936\n",
      "        Title: D_Trump2018_8_14_11_21\n",
      "        Attribute: {'docid': 15091083406720907936, 'author': 'D_Trump73', 'time': 1534245660, 'source': 'Twitter for iPhone'}\n",
      "        Author: D_Trump73\n",
      "        Time: 2018-08-14 04:21:00\n",
      "---------------\n",
      "Document recommendations for topic 5 :\n",
      "    Keywords: witch hunt;donald trump;rigged witch;frame donald;unfortunate situation;trump rigged;situation decided;decided frame\n",
      "    Recommendation 0 :\n",
      "        Document ID: 16578282956147621208\n",
      "        Title: D_Trump2018_8_14_12_6\n",
      "        Attribute: {'docid': 16578282956147621208, 'author': 'D_Trump69', 'time': 1534248360, 'source': 'Twitter for iPhone'}\n",
      "        Author: D_Trump69\n",
      "        Time: 2018-08-14 05:06:00\n",
      "    Recommendation 1 :\n",
      "        Document ID: 5983550775501810643\n",
      "        Title: D_Trump2018_8_16_1_14\n",
      "        Attribute: {'docid': 5983550775501810643, 'author': 'D_Trump46', 'time': 1534377600, 'source': 'Twitter for iPhone'}\n",
      "        Author: D_Trump46\n",
      "        Time: 2018-08-15 17:00:00\n",
      "    Recommendation 2 :\n",
      "        Document ID: 13125097700312206949\n",
      "        Title: D_Trump2018_8_14_13_10\n",
      "        Attribute: {'docid': 13125097700312206949, 'author': 'D_Trump67', 'time': 1534252200, 'source': 'Twitter for iPhone'}\n",
      "        Author: D_Trump67\n",
      "        Time: 2018-08-14 06:10:00\n",
      "---------------\n",
      "Document recommendations for topic 6 :\n",
      "    Keywords: fake news;news media;wacky omarosa;presidential lowlife;overtime wacky;omarosa modern;omarosa legitimate;modern form;media overtime;lowlife omarosa;form communication;communication fake\n",
      "    Recommendation 0 :\n",
      "        Document ID: 3946302249027831785\n",
      "        Title: D_Trump2018_8_13_14_21\n",
      "        Attribute: {'docid': 3946302249027831785, 'author': 'D_Trump93', 'time': 1534170060, 'source': 'Twitter for iPhone'}\n",
      "        Author: D_Trump93\n",
      "        Time: 2018-08-13 07:21:00\n",
      "    Recommendation 1 :\n",
      "        Document ID: 12437126850273027242\n",
      "        Title: D_Trump2018_8_16_12_50\n",
      "        Attribute: {'docid': 12437126850273027242, 'author': 'D_Trump40', 'time': 1534423800, 'source': 'Twitter for iPhone'}\n",
      "        Author: D_Trump40\n",
      "        Time: 2018-08-16 05:50:00\n",
      "---------------\n",
      "Document recommendations for topic 7 :\n",
      "    Keywords: john brennan;cia director;tuckercarlson speaking;speaking john;limited intellectually;intellectually cia;director good;brennan limited\n",
      "    Recommendation 0 :\n",
      "        Document ID: 7508075727870080337\n",
      "        Title: D_Trump2018_8_17_0_45\n",
      "        Attribute: {'docid': 7508075727870080337, 'author': 'D_Trump30', 'time': 1534464000, 'source': 'Twitter for iPhone'}\n",
      "        Author: D_Trump30\n",
      "        Time: 2018-08-16 17:00:00\n",
      "    Recommendation 1 :\n",
      "        Document ID: 427773076591013776\n",
      "        Title: D_Trump2018_8_18_13_12\n",
      "        Attribute: {'docid': 427773076591013776, 'author': 'D_Trump4', 'time': 1534597920, 'source': 'Twitter for iPhone'}\n",
      "        Author: D_Trump4\n",
      "        Time: 2018-08-18 06:12:00\n",
      "---------------\n"
     ]
    }
   ],
   "source": [
    "dataset = 'trump_tweets' # str | Dataset name.\n",
    "#query = '(\"Trump\" OR \"president\")' # str | Fulltext query, using mysql MATCH boolean query format. Example, (\\\"word1\\\" OR \\\"word2\\\") AND (\\\"word3\\\" OR \\\"word4\\\") (optional)\n",
    "query = ''\n",
    "custom_stop_words = [\"real\",\"hillary\"] # str | List of stop words. (optional)\n",
    "num_topics = 8 # int | Number of topics to be extracted from the dataset. (optional) (default to 8)\n",
    "num_keywords = 8 # int | Number of keywords per topic that is extracted from the dataset. (optional) (default to 8)\n",
    "excluded_docs = '' # str | List of document IDs that should be excluded from the analysis. Example, [\"docid1\", \"docid2\", ..., \"docidN\"]  (optional)\n",
    "remove_redundancies = False # bool | If True, this option removes quasi-duplicates from the analysis. A quasi-duplicate would have the same NLP representation, but not necessarily the exact same text. (optional) (default True)\n",
    "\n",
    "try:\n",
    "    payload = nucleus_api.DocumentRecommendModel(dataset=dataset, \n",
    "                                                 query=query, \n",
    "                                                 custom_stop_words=custom_stop_words, \n",
    "                                                 num_topics=num_topics, \n",
    "                                                 num_keywords=num_keywords)\n",
    "    api_response = api_instance.post_doc_recommend_api(payload)\n",
    "except ApiException as e:\n",
    "    api_error = json.loads(e.body)\n",
    "    print('ERROR:', api_error['message'])\n",
    "    \n",
    "for i, res in enumerate(api_response.result):\n",
    "    print('Document recommendations for topic', i, ':')\n",
    "    print('    Keywords:', res.keywords)\n",
    "\n",
    "    for j, doc in enumerate(res.recommendations):\n",
    "        print('    Recommendation', j, ':')\n",
    "        print('        Document ID:', doc.doc_id)\n",
    "        print('        Title:', doc.title)\n",
    "        print('        Attribute:', doc.attribute)\n",
    "        print('        Author:', doc.attribute['author'])\n",
    "        print('        Time:', datetime.datetime.fromtimestamp(float(doc.attribute['time'])))\n",
    "    print('---------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarize a document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary for D_Trump2018_8_17_14_10\n",
      "    * Big pushback on Governor Andrew Cuomo of New York for his really dumb statement about Americas lack of greatness.\n"
     ]
    }
   ],
   "source": [
    "dataset = 'trump_tweets' # str | Dataset name.\n",
    "doc_title = 'D_Trump2018_8_17_14_10' # str | The title of the document to be summarized.\n",
    "custom_stop_words = [\"real\",\"hillary\"] # List of stop words. (optional)\n",
    "summary_length = 6 # int | The maximum number of bullet points a user wants to see in the document summary. (optional) (default to 6)\n",
    "context_amount = 0 # int | The number of sentences surrounding key summary sentences in the documents that they come from. (optional) (default to 0)\n",
    "short_sentence_length = 0 # int | The sentence length below which a sentence is excluded from summarization (optional) (default to 4)\n",
    "long_sentence_length = 40 # int | The sentence length beyond which a sentence is excluded from summarization (optional) (default to 40)\n",
    "\n",
    "try:\n",
    "    payload = nucleus_api.DocumentSummaryModel(dataset=dataset, \n",
    "                                               doc_title=doc_title, \n",
    "                                               custom_stop_words=custom_stop_words, \n",
    "                                                summary_length=summary_length, \n",
    "                                                context_amount=context_amount,\n",
    "                                                short_sentence_length=short_sentence_length,\n",
    "                                                long_sentence_length=long_sentence_length)\n",
    "    api_response = api_instance.post_doc_summary_api(payload)\n",
    "    \n",
    "    print('Summary for', api_response.result.doc_title)\n",
    "    for sent in api_response.result.summary.sentences:\n",
    "        print('    *', sent)\n",
    "    \n",
    "except ApiException as e:\n",
    "    api_error = json.loads(e.body)\n",
    "    print('ERROR:', api_error['message'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarize what makes a document stand-out from the background in a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR: If you are searching for specific terms, there are not enough matches to your filter. Please add additional search terms or check your documents are the correct ones. Otherwise, your documents are likely too short or too few. Please check the length of their content or add more documents.\n"
     ]
    }
   ],
   "source": [
    "dataset = 'trump_tweets' # str | Dataset name.\n",
    "metadata_selection = {\"content\": \"Trump\"} # dict | The metadata selection defining the two categories of documents to contrast and summarize against each other\n",
    "query = '' # str | Dataset-language-specific fulltext query, using mysql MATCH boolean query format (optional)\n",
    "custom_stop_words = [\"real\",\"hillary\"] # List of stop words. (optional)\n",
    "summary_length = 6 # int | The maximum number of bullet points a user wants to see in the contrasted summary. (optional) (default to 6)\n",
    "context_amount = 0 # int | The number of sentences surrounding key summary sentences in the documents that they come from. (optional) (default to 0)\n",
    "short_sentence_length = 0 # int | The sentence length below which a sentence is excluded from summarization (optional) (default to 4)\n",
    "long_sentence_length = 40 # int | The sentence length beyond which a sentence is excluded from summarization (optional) (default to 40)\n",
    "time_period = \"1M\" # str | Alternative 1: time period counting back from today over which the analysis is conducted (optional)\n",
    "period_start = '2018-08-12' # str | Alternative 2: start of period over which the analysis is conducted (optional)\n",
    "period_end = '2018-08-15' # str | Alternative 2: start of period over which the analysis is conducted (optional)\n",
    "excluded_docs = '' # str | List of document IDs that should be excluded from the analysis. Example, [\"docid1\", \"docid2\", ..., \"docidN\"]  (optional)\n",
    "syntax_variables = True # bool | Specifies whether to take into account syntax aspects of each category of documents to help with contrasting them (optional) (default to False)\n",
    "compression = 0.002 # float | Parameter controlling the breadth of the contrasted summary. Contained between 0 and 1, the smaller it is, the more contrasting terms will be captured, with decreasing weight. (optional) (default to 0.000002)\n",
    "remove_redundancies = False # bool | If True, this option removes quasi-duplicates from the analysis and retain only one copy of it. A quasi-duplicate would have the same NLP representation, but not necessarily the exact same text. (optional) (default False)\n",
    "metadata_selection_contrast = {\"content\": \"Trump\"}\n",
    "\n",
    "try:\n",
    "    payload = nucleus_api.DocumentContrastSummaryModel(\n",
    "        dataset=dataset, \n",
    "        metadata_selection=metadata_selection,\n",
    "        metadata_selection_contrast=metadata_selection_contrast\n",
    "        )\n",
    "    api_response = api_instance.post_document_contrast_summary_api(payload)\n",
    "    \n",
    "    print('Summary for', [x for x in  metadata_selection.values()])\n",
    "    for sent in api_response.result.class_1_content.sentences:\n",
    "        print('    *', sent)\n",
    "    print('======')\n",
    "    for sent in api_response.result.class_2_content.sentences:\n",
    "        print('    *', sent)    \n",
    "except ApiException as e:\n",
    "    api_error = json.loads(e.body)\n",
    "    print('ERROR:', api_error['message'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measure the sentiment of a document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment for D_Trump2018_8_17_14_10\n",
      "-0.3333\n"
     ]
    }
   ],
   "source": [
    "dataset = 'trump_tweets' # str | Dataset name.\n",
    "doc_title = 'D_Trump2018_8_17_14_10' # str | The title of the document to be analyzed.\n",
    "custom_stop_words = [\"real\",\"hillary\"] # List of stop words. (optional)\n",
    "num_topics = 8 # int | Number of topics to be extracted from the document. (optional) (default to 8)\n",
    "num_keywords = 8 # int | Number of keywords per topic that is extracted from the document. (optional) (default to 8)\n",
    "\n",
    "try:\n",
    "    payload = nucleus_api.DocumentSentimentModel(dataset=dataset, \n",
    "                                                 doc_title=doc_title, \n",
    "                                                 custom_stop_words=custom_stop_words, \n",
    "                                                 num_topics=num_topics, \n",
    "                                                 num_keywords=num_keywords)\n",
    "    api_response = api_instance.post_doc_sentiment_api(payload)\n",
    "    \n",
    "    print('Sentiment for', api_response.result.doc_title)\n",
    "    print(api_response.result.sentiment)\n",
    "\n",
    "except ValueError as e:\n",
    "    print('ERROR:', e)\n",
    "except ApiException as e:\n",
    "    api_error = json.loads(e.body)\n",
    "    print('ERROR:', api_error['message'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classify documents based on a topic contrasting two categories of content in a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR: If you are searching for specific terms, there are not enough matches to your filter. Please add additional search terms or check your documents are the correct ones. Otherwise, your documents are likely too short or too few. Please check the length of their content or add more documents.\n"
     ]
    }
   ],
   "source": [
    "dataset = 'trump_tweets' # str | Dataset name.\n",
    "fixed_topics = {\"keywords\": [\"america\", \"jobs\", \"economy\"], \"weights\": [0.5, 0.25, 0.25]} # dict | The contrasting topic used to separate the two categories of documents\n",
    "\n",
    "# Here we want to classify documents that talk about Trump vs documents that don't talk about Trump based on their exposure to the topic [america, jobs, economy]\n",
    "# A more natural classification task for the algo is to define metadata-based categories such as metadata_selection = {\"document_category\": [\"speech\", \"press release\"]}\n",
    "metadata_selection = {\"content\": \"Trump\"} # dict | The metadata selection defining the two categories of documents that a document can be classified into\n",
    "query = '' # str | Dataset-language-specific fulltext query, using mysql MATCH boolean query format (optional)\n",
    "custom_stop_words = [\"real\",\"hillary\"] # List of stop words. (optional)\n",
    "time_period = \"1M\" # str | Alternative 1: time period counting back from today over which the analysis is conducted (optional)\n",
    "period_start = '2018-08-12' # str | Alternative 2: start of period over which the analysis is conducted (optional)\n",
    "period_end = '2018-08-15' # str | Alternative 2: start of period over which the analysis is conducted (optional)\n",
    "excluded_docs = '' # str | List of document IDs that should be excluded from the analysis. Example, [\"docid1\", \"docid2\", ..., \"docidN\"]  (optional)\n",
    "syntax_variables = True # bool | If True, the classifier will include syntax-related variables on top of content variables (optional) (default to False)\n",
    "validation_phase = False # bool | If True, the classifier assumes that the dataset provided is labeled with the 2 classes and will use that to compute accuracy/precision/recall (optional) (default to False)\n",
    "threshold = 0 # float | Threshold value for a document exposure to the contrastic topic, above which the document is assigned to class 1 specified through metadata_selection. (optional) (default to 0)\n",
    "remove_redundancies = False # bool | If True, this option removes quasi-duplicates from the analysis and retain only one copy of it. A quasi-duplicate would have the same NLP representation, but not necessarily the exact same text. (optional) (default False)\n",
    "classifier_config = {}\n",
    "metadata_selection_contrast = {\"content\": \"Hillary\"} \n",
    "\n",
    "try:\n",
    "    payload = nucleus_api.DocClassifyModel(\n",
    "        dataset=dataset,\n",
    "        fixed_topics=fixed_topics,\n",
    "        metadata_selection=metadata_selection,\n",
    "        classifier_config=classifier_config,\n",
    "        metadata_selection_contrast=metadata_selection_contrast\n",
    "    )\n",
    "    \n",
    "    api_response = api_instance.post_doc_classify_api(payload)\n",
    "    \n",
    "    print('Detailed Results')\n",
    "    print('    Docids:', api_response.result.detailed_results.docids)\n",
    "    print('    Exposure:', api_response.result.detailed_results.exposures)\n",
    "    print('    Estimated Category:', api_response.result.detailed_results.estimated_class)\n",
    "    print('    Actual Category:', api_response.result.detailed_results.true_class)\n",
    "    print('\\n')\n",
    "    if validation_phase:\n",
    "        print('Perf Metrics')\n",
    "        print('    Accuracy:', api_response.result.perf_metrics.hit_rate)\n",
    "        print('    Recall:', api_response.result.perf_metrics.recall)\n",
    "        print('    Precision:', api_response.result.perf_metrics.precision)\n",
    "            \n",
    "except ApiException as e:\n",
    "    api_error = json.loads(e.body)\n",
    "    print('ERROR:', api_error['message'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tag documents based on pre-determined named-entity recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'job_id': '169861',\n",
      " 'result': {'dataset': 'trump_tweets',\n",
      "            'doc_ids': ['157671510549269746',\n",
      "                        '681266336486897660',\n",
      "                        '1323691717445942922',\n",
      "                        '6827313427448806555',\n",
      "                        '10908843577559309852'],\n",
      "            'entities_count': ['6', '6', '6', '6', '6'],\n",
      "            'entities_tagged': ['new york city',\n",
      "                                'big apple',\n",
      "                                'NYC',\n",
      "                                'New York']}}\n",
      "    Entities tagged: ['new york city', 'big apple', 'NYC', 'New York']\n",
      "    Docids tagged with the entities: ['157671510549269746', '681266336486897660', '1323691717445942922', '6827313427448806555', '10908843577559309852']\n",
      "    Entities count: ['6', '6', '6', '6', '6']\n"
     ]
    }
   ],
   "source": [
    "dataset = 'trump_tweets' # str | Dataset name.\n",
    "try:\n",
    "    payload = nucleus_api.DatasetTagging(dataset=dataset, \n",
    "                                         query='new york city OR big apple OR NYC OR New York', \n",
    "                                         metadata_selection='', \n",
    "                                         time_period='',\n",
    "                                         period_start='2010-01-01',\n",
    "                                         period_end='2019-04-30')\n",
    "    \n",
    "    api_response = api_instance.post_dataset_tagging(payload)\n",
    "    print(api_response)\n",
    "    print('    Entities tagged:', api_response.result.entities_tagged)\n",
    "    print('    Docids tagged with the entities:', api_response.result.doc_ids)\n",
    "    print('    Entities count:', api_response.result.entities_count)\n",
    "except ApiException as e:\n",
    "    api_error = json.loads(e.body)\n",
    "    print('ERROR:', api_error['message'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarize files from a URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary for quarles20181109a-newname.pdf :\n",
      "    * Enhanced transparency goes to the very core of democratic accountability and the rights of all US citizens--including the management and shareholders of the institutions that are subject to the stress tests--to understand the requirements to which they are subject.\n",
      "    * A healthy US economy relies on a strong, well-capitalized banking system that can weather stressful events and continue lending to households and businesses.\n",
      "    * Many have noted that a single market shock does not adequately capture risks in firms trading book, and we agree with those comments.\n",
      "    * If it guesses wrong, it could be publicly shamed for failing the stress test (if its dividends are too high relative to capital), or penalized in the markets for inadequate distribution of income (if its dividends are too low relative to capital).\n",
      "    * We are currently considering options to provide additional transparency regarding scenarios and scenario design and I expect that the Board will seek comment on the advisability of, and possible approaches to, gathering the publics input on scenarios and salient risks facing the banking system each year.\n",
      "    * In such an environment, firms would remain subject to the same supervisory expectations, and examiners would continue to conduct rigorous horizontal and firm-specific assessments of a firms capital positions and capital planning, tailored to the risk profile of the firm.\n"
     ]
    }
   ],
   "source": [
    "######################################################################################\n",
    "# file_params fields descriptions:  \n",
    "#   file_url              : string, the URL at which the file is stored (could be a S3 bucket address for instance)\n",
    "#   filename              : OPTIONAL string, filename saved on the server. also serves as the doc_title for summarization\n",
    "#   custom_stop_words     : OPTIONAL a string list, user-provided list of stopwords to be excluded from the content analysis leading to document summarization\n",
    "#                            [\"word1\", \"word2\", ...]. DEFAULT: empty\n",
    "#   summary_length        : OPTIONAL an integer, the maximum number of bullet points a user wants to see in the document summary. DEFAULT: 6\n",
    "#   context_amount        : OPTIONAL an integer, the number of sentences surrounding key summary sentences in the original document that a user wants to see in the document summary. DEFAULT: 0\n",
    "#   short_sentence_length : OPTIONAL an integer, the sentence length below which a sentence is excluded from summarization. DEFAULT: 4 words\n",
    "#   long_sentence_length  : OPTIONAL an integer, the sentence length beyond which a sentence is excluded from summarization. DEFAULT: 40 words\n",
    "\n",
    "file_params = {\n",
    "    'file_url': 'https://s3-us-west-2.amazonaws.com/sumup-public/nucleus-sdk/quarles20181109a.docx',\n",
    "    'filename': 'quarles20181109a-newname.pdf',   \n",
    "    'custom_stop_words': [\"document\", \"sometimes\"], \n",
    "    'summary_length': 6,\n",
    "    'context_amount': 0, \n",
    "    'short_sentence_length': 4, \n",
    "    'long_sentence_length': 40}\n",
    "\n",
    "result = nucleus_helper.summarize_file_url(api_instance, file_params)\n",
    "  \n",
    "print('Summary for', result.doc_title, ':')\n",
    "for sent in result.summary.sentences:\n",
    "    print('    *', sent)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "265px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
