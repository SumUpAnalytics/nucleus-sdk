{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) 2018-2019 SumUp Analytics, Inc. All Rights Reserved.\n",
    "\n",
    "NOTICE: All information contained herein is, and remains the property of SumUp Analytics Inc. and its suppliers, if any. The intellectual and technical concepts contained herein are proprietary to SumUp Analytics Inc. and its suppliers and may be covered by U.S. and Foreign Patents, patents in process, and are protected by trade secret or copyright law.\n",
    "\n",
    "Dissemination of this information or reproduction of this material is strictly forbidden unless prior written permission is obtained from SumUp Analytics Inc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization, configure API host and key, and create new API instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running example in Jupyter Notebook\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import json\n",
    "import datetime\n",
    "import time\n",
    "import nucleus_api\n",
    "from nucleus_api.rest import ApiException\n",
    "import nucleus_api.api.nucleus_api as nucleus_helper\n",
    "from pprint import pprint\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# Determine if in Jupyter notebook or not\n",
    "try:\n",
    "    ip = get_ipython()\n",
    "    running_notebook = True\n",
    "except NameError:\n",
    "    running_notebook = False\n",
    "\n",
    "if running_notebook:\n",
    "    print('Running example in Jupyter Notebook')\n",
    "else:\n",
    "    print('Running example in script mode')\n",
    "    \n",
    "configuration = nucleus_api.Configuration()\n",
    "configuration.host = 'UPDATE-WITH-API-SERVER-HOSTNAME'\n",
    "configuration.api_key['x-api-key'] = 'UPDATE-WITH-API-KEY'\n",
    "\n",
    "# Create API instance\n",
    "api_instance = nucleus_api.NucleusApi(nucleus_api.ApiClient(configuration))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Append file from local drive to dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- Append file from local drive to dataset -----------\n",
      "quarles20181109a.pdf has been added to dataset dataset_test\n",
      "-------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print('--------- Append file from local drive to dataset -----------')\n",
    "file = 'quarles20181109a.pdf'         # file | \n",
    "dataset = 'dataset_test'              # str | Destination dataset where the file will be inserted.\n",
    "metadata = {\"time\": \"1/2/2018\", \n",
    "            \"author\": \"Test Author\"}  # Optional json containing additional document metadata\n",
    "\n",
    "try:\n",
    "    api_response = api_instance.post_upload_file(file, dataset)\n",
    "    print(api_response.result, 'has been added to dataset', dataset)\n",
    "    #print('api_response=', api_response)   # raw API response    \n",
    "except ApiException as e:\n",
    "    print(\"Exception when calling DatasetsApi->post_upload_file: %s\\n\" % e)\n",
    "\n",
    "print('-------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Append all PDFs from a folder to dataset in parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- Append all files from local folder to dataset -----------\n",
      "INFO: fomcminutes20181108.pdf imported.\n",
      "INFO: fomcminutes20181219.pdf imported.\n",
      "-------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print('--------- Append all files from local folder to dataset -----------')\n",
    "folder = 'fomc-minutes'         \n",
    "\n",
    "dataset = 'dataset_test'              # str | Destination dataset where the file will be inserted.\n",
    "\n",
    "for root, dirs, files in os.walk(folder):\n",
    "    file_iters = []\n",
    "    for file in files:\n",
    "        if Path(file).suffix == '.pdf':\n",
    "            file_iters.append(os.path.join(root, file))\n",
    "        \n",
    "    nucleus_helper.import_files(api_instance, dataset, file_iters, processes=4)\n",
    "\n",
    "print('-------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Append file from URL to dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------ Append file from URL to dataset ---------------\n",
      "https://s3-us-west-2.amazonaws.com/sumup-public/nucleus-sdk/quarles20181109a.docx has been added to dataset dataset_test\n",
      "-------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print('------------ Append file from URL to dataset ---------------')\n",
    "\n",
    "dataset = 'dataset_test'\n",
    "file_url = 'https://s3-us-west-2.amazonaws.com/sumup-public/nucleus-sdk/quarles20181109a.docx'\n",
    "# Optional filename saved on the server for the URL. If not specified, Nucleus will make\n",
    "# an intelligent guess from the file URL\n",
    "filename = 'quarles20181109a-newname.pdf'  \n",
    "payload = nucleus_api.UploadURLModel(\n",
    "                dataset=dataset,\n",
    "                file_url=file_url,\n",
    "                filename=filename  \n",
    "            ) # UploadURLModel | \n",
    "\n",
    "try:\n",
    "    api_response = api_instance.post_upload_url(payload)\n",
    "    #print('api_response=', api_response)   # raw API response\n",
    "except ApiException as e:\n",
    "    print(\"Exception when calling DatasetsApi->post_upload_url: %s\\n\" % e)\n",
    "    \n",
    "print(api_response.result, 'has been added to dataset', dataset)\n",
    "print('-------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Append files from URLs to dataset in parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------ Append file from URL to dataset ---------------\n",
      "INFO: https://s3-us-west-2.amazonaws.com/sumup-public/nucleus-sdk/quarles20181109d.docx imported\n",
      "INFO: https://s3-us-west-2.amazonaws.com/sumup-public/nucleus-sdk/quarles20181109c.docx imported\n",
      "INFO: https://s3-us-west-2.amazonaws.com/sumup-public/nucleus-sdk/quarles20181109b.docx imported\n",
      "INFO: https://s3-us-west-2.amazonaws.com/sumup-public/nucleus-sdk/quarles20181109a.docx imported\n",
      "-------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print('------------ Append file from URL to dataset ---------------')\n",
    "\n",
    "dataset = 'dataset_test'\n",
    "file_urls = [\n",
    "    'https://s3-us-west-2.amazonaws.com/sumup-public/nucleus-sdk/quarles20181109a.docx',\n",
    "    'https://s3-us-west-2.amazonaws.com/sumup-public/nucleus-sdk/quarles20181109b.docx',\n",
    "    'https://s3-us-west-2.amazonaws.com/sumup-public/nucleus-sdk/quarles20181109c.docx',\n",
    "    'https://s3-us-west-2.amazonaws.com/sumup-public/nucleus-sdk/quarles20181109d.docx'\n",
    "]\n",
    "\n",
    "nucleus_helper.import_urls(api_instance, dataset, file_urls, processes=4)\n",
    "    \n",
    "\n",
    "print('-------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Append jsons from csv to dataset in parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- Append json from CSV to dataset -----------------\n",
      "-------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# This dataset will be used to test all topics and documents APIs\n",
    "print('----------- Append json from CSV to dataset -----------------')\n",
    "csv_file = 'trump-tweets-100.csv'\n",
    "dataset = 'trump_tweets'\n",
    "\n",
    "with open(csv_file, encoding='utf-8-sig') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    #print(list(reader))\n",
    "    nucleus_helper.import_jsons(api_instance, dataset, reader, processes=1)\n",
    "\n",
    "print('-------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List available datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------- List available datasets ---------------------\n",
      "5 datasets in the database:\n",
      "     dataset_test\n",
      "     test\n",
      "     test_8k\n",
      "     trump_tweets\n",
      "     trump_tweets_full\n",
      "-------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print('---------------- List available datasets ---------------------')\n",
    "try:\n",
    "    api_response = api_instance.get_list_datasets()\n",
    "except ApiException as e:\n",
    "    print(\"Exception when calling DatasetsApi->get_list_datasets: %s\\n\" % e)\n",
    "\n",
    "list_datasets = api_response.result\n",
    "\n",
    "print(len(list_datasets), 'datasets in the database:')\n",
    "for ds in list_datasets:\n",
    "    print('    ', ds)\n",
    "\n",
    "    \n",
    "print('-------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get dataset information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------- Get dataset information -------------------\n",
      "Information about dataset dataset_test\n",
      "    Language: en\n",
      "    Number of documents: 8\n",
      "    Time range: 2019-03-16 06:24:08 to 2019-03-16 06:24:11\n",
      "-------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print('--------------- Get dataset information -------------------')\n",
    "dataset = 'dataset_test' # str | Dataset name.\n",
    "query = '' # str | Fulltext query, using mysql MATCH boolean query format. (optional)\n",
    "metadata_selection = '' # str | json object of {\\\"metadata_field\\\":[\\\"selected_values\\\"]} (optional)\n",
    "time_period = '' # str | Time period selection (optional)\n",
    "period_start = \"\" # str | Start date for the period to analyze within the dataset. Format: \"YYYY-MM-DD HH:MM:SS\"\n",
    "period_end = \"\" # str | End date for the period to analyze within the dataset. Format: \"YYYY-MM-DD HH:MM:SS\"\n",
    "\n",
    "try:\n",
    "    payload = nucleus_api.DatasetInfo(dataset=dataset, \n",
    "                                    query=query, \n",
    "                                    metadata_selection=metadata_selection, \n",
    "                                    time_period=time_period)\n",
    "    api_response = api_instance.post_dataset_info(payload)\n",
    "    #print('api_response=', api_response) # raw API response\n",
    "except ApiException as e:\n",
    "    print(\"Exception when calling DatasetsApi->post_dataset_info: %s\\n\" % e)\n",
    "\n",
    "print('Information about dataset', dataset)\n",
    "print('    Language:', api_response.result.detected_language)\n",
    "print('    Number of documents:', api_response.result.num_documents)\n",
    "print('    Time range:', datetime.datetime.fromtimestamp(float(api_response.result.time_range[0])),\n",
    "             'to', datetime.datetime.fromtimestamp(float(api_response.result.time_range[1])))\n",
    "\n",
    "\n",
    "print('-------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------- Delete document -----------------------\n",
      "Exception when calling DatasetsApi->post_delete_document: (500)\n",
      "Reason: INTERNAL SERVER ERROR\n",
      "HTTP response headers: HTTPHeaderDict({'Server': 'gunicorn/19.9.0', 'Date': 'Sat, 16 Mar 2019 06:24:40 GMT', 'Connection': 'close', 'Content-Type': 'application/json', 'Content-Length': '52'})\n",
      "HTTP response body: {\"message\": \"IndexError : list index out of range\"}\n",
      "\n",
      "\n",
      "\n",
      "Document 1 from dataset dataset_test has been deleted.\n",
      "-------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print('--------------------- Delete document -----------------------')\n",
    "dataset = 'dataset_test'\n",
    "\n",
    "docid = '1'\n",
    "payload = nucleus_api.Deletedocumentmodel(dataset=dataset,\n",
    "                                             docid=docid) # Deletedocumentmodel | \n",
    "\n",
    "try:\n",
    "    api_response = api_instance.post_delete_document(payload)\n",
    "except ApiException as e:\n",
    "    print(\"Exception when calling DatasetsApi->post_delete_document: %s\\n\" % e)\n",
    "\n",
    "\n",
    "print('Document', docid, 'from dataset', dataset, 'has been deleted.')\n",
    "# print(api_response)     # raw API response\n",
    "print('-------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------- Delete dataset ------------------------\n",
      "api_response= {'job_id': None,\n",
      " 'result': ['test', 'test_8k', 'trump_tweets', 'trump_tweets_full']}\n",
      "-------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print('--------------------- Delete dataset ------------------------')\n",
    "\n",
    "dataset = 'dataset_test'  \n",
    "payload = nucleus_api.Deletedatasetmodel(dataset=dataset) # Deletedatasetmodel | \n",
    "\n",
    "\n",
    "try:\n",
    "    api_response = api_instance.post_delete_dataset(payload)\n",
    "    #print(api_response)\n",
    "except ApiException as e:\n",
    "    print(\"Exception when calling DatasetsApi->post_delete_dataset: %s\\n\" % e)\n",
    "    \n",
    "# List datasets again to check if the specified dataset has been deleted\n",
    "try:\n",
    "    api_response = api_instance.get_list_datasets()\n",
    "    print('api_response=', api_response)\n",
    "except ApiException as e:\n",
    "    print(\"Exception when calling DatasetsApi->get_list_datasets: %s\\n\" % e)\n",
    "    \n",
    "print('-------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get list of topics from dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------- Get list of topics from dataset --------------\n",
      "Topic 1 keywords:\n",
      "    Keywords: illegal aliens;united permission;permission illegal;enter united;aliens treated;aliens illegal;treated entered;entered legally\n",
      "    Keyword weights: 0.12414163288726007;0.12414163288726007;0.12414163288726007;0.24828326577452015;0.12414163288726007;0.12414163288726007;0.06550428489458975;0.06550428489458975\n",
      "    Strength: 0.12742550951026896\n",
      "    Document IDs: 3397215194896514820 4825367511331474696\n",
      "    Document exposures: 0.5276908390973678 0.47230916090263214\n",
      "---------------\n",
      "Topic 2 keywords:\n",
      "    Keywords: bruce ohr;ohr justice;justice department;christopher steele;helping disgraced;disgraced christopher;believe accused;accused helping\n",
      "    Keyword weights: 0.25874319579286653;0.13422945467910785;0.13422945467910785;0.08787360454164554;0.08787360454164554;0.08787360454164554;0.08787360454164554;0.12130347668233549\n",
      "    Strength: 0.15848864730324944\n",
      "    Document IDs: 6303783743713708484 7290029718334628379 12936417737022695482 13072902166717108911 13460407141547160473 16828254283062838304\n",
      "    Document exposures: 0.3060093585097111 0.12739789901399845 0.16294664910541848 0.08977923098690818 0.13714004061081056 0.17672682177315324\n",
      "---------------\n",
      "Topic 3 keywords:\n",
      "    Keywords: america great;statement america;andrew cuomo;great great;hillary clinton;worse hightax;hightax andrew;cuomo statement\n",
      "    Keyword weights: 0.11287035796679323;0.16506248820300184;0.14791232053833467;0.163312830086746;0.13467452407628971;0.09205582637627821;0.09205582637627821;0.09205582637627821\n",
      "    Strength: 0.1414528728878926\n",
      "    Document IDs: 776902852041351634 2205902445999073018 3545423942726121399 5864841412738683134 8047817457772465264 10010199882756041615 14722230792170818214\n",
      "    Document exposures: 0.03912514602916692 0.11258429003912225 0.18108041662950897 0.3910603137326332 0.15950113130227564 0.07250958466968419 0.044139117597608774\n",
      "---------------\n",
      "Topic 4 keywords:\n",
      "    Keywords: witch hunt;donald trump;rigged witch;frame donald;unfortunate situation;trump rigged;situation decided;decided frame\n",
      "    Keyword weights: 0.1789709833283964;0.09710956100104969;0.09710956100104969;0.09710956100104969;0.12680989466950424;0.09710956100104969;0.12680989466950424;0.1789709833283964\n",
      "    Strength: 0.14465377548369468\n",
      "    Document IDs: 2365960778917245307 5217366909427623007 5821020073909755150 9035906359710233744 14722230792170818214 14988735547592816033 15429790537822270921 18423518517048905190\n",
      "    Document exposures: 0.05613012034502662 0.4294513626831707 0.05810019433776669 0.05434775532922656 0.17753423838710491 0.05810019433776669 0.05434775532922656 0.11198837925071124\n",
      "---------------\n",
      "Topic 5 keywords:\n",
      "    Keywords: peter strzok;fired fbi;agent peter;fbi agent;strzok fraud;rigged investigation;investigation started;fraud rigged\n",
      "    Keyword weights: 0.17978654560850793;0.1359931431406415;0.17978654560850793;0.17978654560850793;0.08116180500845867;0.08116180500845867;0.08116180500845867;0.08116180500845867\n",
      "    Strength: 0.11929991261700007\n",
      "    Document IDs: 14241248046650668697 14988735547592816033 16828254283062838304\n",
      "    Document exposures: 0.41868692909536664 0.2630756355948166 0.3182374353098168\n",
      "---------------\n",
      "Topic 6 keywords:\n",
      "    Keywords: trump campaign;lou dobbs;evidence collusion;dobbs special;democrats evidence;conflicts angry;collusion trump;angry democrats\n",
      "    Keyword weights: 0.14006843551915632;0.12284736635440624;0.12284736635440624;0.12284736635440624;0.12284736635440624;0.12284736635440624;0.12284736635440624;0.12284736635440624\n",
      "    Strength: 0.12282413166027603\n",
      "    Document IDs: 3325720912382988533 4746121785136787662 11357478787751126246 16856013646116686449\n",
      "    Document exposures: 0.045969398666378135 0.45346299242305366 0.047104616487514545 0.45346299242305366\n",
      "---------------\n",
      "Topic 7 keywords:\n",
      "    Keywords: fake news;news media;wacky omarosa;presidential lowlife;omarosa modern;modern form;lowlife omarosa;communication fake\n",
      "    Keyword weights: 0.18179971748505058;0.14262298315553928;0.10767401297975154;0.10767401297975154;0.10767401297975154;0.10767401297975154;0.10767401297975154;0.13720723446065247\n",
      "    Strength: 0.1057105715676719\n",
      "    Document IDs: 950604085993420810 4767189974744133712 6173618630202756293 7014079786619530089 14463102537742211332 16442619160892914817\n",
      "    Document exposures: 0.23432085542146297 0.4170029060457151 0.09910069944350508 0.09284914631659266 0.0809153822631681 0.07581101050955609\n",
      "---------------\n",
      "Topic 8 keywords:\n",
      "    Keywords: john brennan;cia director;tuckercarlson speaking;speaking john;limited intellectually;intellectually cia;director good;brennan limited\n",
      "    Keyword weights: 0.11353238966286361;0.11353238966286361;0.16901767572863952;0.11353238966286361;0.11353238966286361;0.11353238966286361;0.14978798629417872;0.11353238966286361\n",
      "    Strength: 0.08014457896994633\n",
      "    Document IDs: 7967605045913198983 9035906359710233744 14355443980237980691\n",
      "    Document exposures: 0.6820830675989886 0.10783839616484461 0.21007853623616685\n",
      "---------------\n",
      "-------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print('------------- Get list of topics from dataset --------------')\n",
    "dataset = 'trump_tweets'\n",
    "#query = '(\"Trump\" OR \"president\")' # str | Fulltext query, using mysql MATCH boolean query format. Example, (\\\"word1\\\" OR \\\"word2\\\") AND (\\\"word3\\\" OR \\\"word4\\\") (optional)\n",
    "query = ''\n",
    "custom_stop_words = [\"real\",\"hillary\"] # str | List of stop words. (optional)\n",
    "num_topics = 8 # int | Number of topics to be extracted from the dataset. (optional) (default to 8)\n",
    "metadata_selection = \"\" # dict | JSON object specifying metadata-based queries on the dataset, of type {\"metadata_field\": \"selected_values\"} (optional)\n",
    "time_period = \"\"     # str | Time period selection. Choices: [\"1M\",\"3M\",\"6M\",\"12M\",\"3Y\",\"5Y\",\"\"] (optional)\n",
    "\n",
    "try:\n",
    "    payload = nucleus_api.Topics(dataset=dataset,                                \n",
    "                                query=query,                   \n",
    "                                custom_stop_words=custom_stop_words,     \n",
    "                                num_topics=num_topics,\n",
    "                                metadata_selection=metadata_selection,\n",
    "                                time_period=time_period)\n",
    "    api_response = api_instance.post_topic_api(payload)        \n",
    "except ApiException as e:\n",
    "    print(\"Exception when calling TopicsApi->post_topic_api: %s\\n\" % e)\n",
    "    \n",
    "#print(api_response)\n",
    "i = 1\n",
    "for res in api_response.result:\n",
    "    print('Topic', i, 'keywords:')\n",
    "    print('    Keywords:', res.topic)\n",
    "    keywords_weight_str = \";\".join(str(x) for x in res.keywords_weight)\n",
    "    print('    Keyword weights:', keywords_weight_str)\n",
    "    print('    Strength:', res.strength)\n",
    "    doc_topic_exposure_sel = []  # list of non-zero doc_topic_exposure\n",
    "    doc_id_sel = []        # list of doc ids matching doc_topic_exposure_sel\n",
    "    for j in range(len(res.doc_topic_exposure)):\n",
    "        doc_topic_exp = float(res.doc_topic_exposure[j])\n",
    "        if doc_topic_exp != 0:\n",
    "            doc_topic_exposure_sel.append(doc_topic_exp)\n",
    "            doc_id_sel.append(res.doc_id[j])\n",
    "    \n",
    "    doc_id_sel_str = ' '.join(str(x) for x in doc_id_sel)\n",
    "    doc_topic_exposure_sel_str = ' '.join(str(x) for x in doc_topic_exposure_sel)\n",
    "    print('    Document IDs:', doc_id_sel_str)\n",
    "    print('    Document exposures:', doc_topic_exposure_sel_str)\n",
    "\n",
    "    print('---------------')\n",
    "    i = i + 1\n",
    "    \n",
    "print('-------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get list of topics from dataset with a time range selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------- Get list of topics from dataset --------------\n",
      "Topic 1 keywords:\n",
      "    Keywords: total endorsement;complete total;pete complete;bob total;america great;witch hunt;great great;rigged witch\n",
      "    Keyword weights: 0.417496613427435;0.4830693470853973;0.0007742492506180052;0.0005365550931677482;0.0004222046146991653;0.0006092414566723346;0.033897709097540474;0.06319407997447\n",
      "    Strength: 0.29043876890402737\n",
      "    Document IDs: 1292265014981711161 2205902445999073018 2373450842905457495 3545423942726121399 5217366909427623007 5864841412738683134 8047817457772465264 8073561612845847316 8991483632660067955 9785400758777816854 10010199882756041615 10785765889843731879 11760987759040078706 12936417737022695482 12945860735388879748 14722230792170818214 14988735547592816033 15429790537822270921 16259624839192846495 16485451943305749356 16828254283062838304 17770243791305455215 18423518517048905190\n",
      "    Document exposures: 0.11938167545812424 0.00010651119996977998 0.10833710517819305 0.00018768666130894416 0.00014189301271935766 0.0001960323243288718 9.051711451034855e-05 0.13468069379517436 0.06186571134582509 0.11938167545812424 0.00021672188886004206 0.11938167545812424 0.11533365571033308 0.00010907402652275058 9.051711451034855e-05 9.11127731573233e-05 7.329108434527129e-05 7.554672054290163e-05 0.06043837457412784 0.15934330502674743 0.00011736967055587216 0.00020560043628348157 0.00015425396761111656\n",
      "---------------\n",
      "Topic 2 keywords:\n",
      "    Keywords: andrew cuomo;statement america;york dumb;pushback governor;governor andrew;dumb statement;cuomo york;america lack\n",
      "    Keyword weights: 0.16188811365208738;0.1419096938412974;0.11603369875110255;0.11603369875110255;0.11603369875110255;0.11603369875110255;0.11603369875110255;0.11603369875110255\n",
      "    Strength: 0.11535256570325149\n",
      "    Document IDs: 2205902445999073018 3545423942726121399 5864841412738683134 8047817457772465264 10010199882756041615 12945860735388879748 17770243791305455215\n",
      "    Document exposures: 0.05954670121862939 0.054329519109147954 0.12147942106559041 0.31259222252763136 0.07156622817616744 0.31259222252763136 0.06789368537520213\n",
      "---------------\n",
      "Topic 3 keywords:\n",
      "    Keywords: illegal aliens;united permission;permission illegal;enter united;aliens treated;aliens illegal;treated entered;entered legally\n",
      "    Keyword weights: 0.12414163288726007;0.12414163288726007;0.12414163288726007;0.24828326577452015;0.12414163288726007;0.12414163288726007;0.06550428489458975;0.06550428489458975\n",
      "    Strength: 0.09308719428630949\n",
      "    Document IDs: 3397215194896514820 4825367511331474696\n",
      "    Document exposures: 0.5276908390973678 0.4723091609026322\n",
      "---------------\n",
      "Topic 4 keywords:\n",
      "    Keywords: bruce ohr;ohr doj;legal jeopardy;doj legal;ohr justice;justice department;christopher steele;disgraced christopher\n",
      "    Keyword weights: 0.30422238084098074;0.10303477164140232;0.10303477164140232;0.0581727034141404;0.0930904841956935;0.11281496275546023;0.11281496275546023;0.11281496275546023\n",
      "    Strength: 0.13627030045202118\n",
      "    Document IDs: 6303783743713708484 7290029718334628379 12936417737022695482 13072902166717108911 13460407141547160473 16828254283062838304\n",
      "    Document exposures: 0.19568894748302926 0.11439112936608456 0.16279697712036525 0.19010189664145746 0.19010189664145746 0.14691915274760603\n",
      "---------------\n",
      "Topic 5 keywords:\n",
      "    Keywords: donald trump;frame donald;hillary clinton;unfortunate situation;trump rigged;situation decided;decided frame;clinton frame\n",
      "    Keyword weights: 0.1126907216252126;0.2613507602199643;0.09611405672034704;0.09611405672034704;0.09611405672034704;0.16880817399689102;0.09611405672034704;0.07269411727654397\n",
      "    Strength: 0.10043503591346258\n",
      "    Document IDs: 776902852041351634 2365960778917245307 5217366909427623007 5821020073909755150 5864841412738683134 9035906359710233744 14722230792170818214\n",
      "    Document exposures: 0.04313118443873197 0.08821750234423009 0.36831430030164325 0.10002924535118157 0.048658538440882096 0.08586461159332427 0.2657846175300067\n",
      "---------------\n",
      "Topic 6 keywords:\n",
      "    Keywords: fake news;news media;wacky omarosa;presidential lowlife;omarosa modern;modern form;lowlife omarosa;communication fake\n",
      "    Keyword weights: 0.20492968408238166;0.14061509943930425;0.10448091160921456;0.10448091160921456;0.10448091160921456;0.10448091160921456;0.10448091160921456;0.1320506584322411\n",
      "    Strength: 0.08753386329051362\n",
      "    Document IDs: 372746459070796601 950604085993420810 4767189974744133712 6173618630202756293 7014079786619530089 14463102537742211332 16442619160892914817\n",
      "    Document exposures: 0.08104549340194335 0.20115201716371967 0.41162813669531895 0.09414694922745542 0.08435482401239114 0.052223331210476204 0.07544924828869526\n",
      "---------------\n",
      "Topic 7 keywords:\n",
      "    Keywords: peter strzok;fired fbi;agent peter;fbi agent;strzok fraud;rigged investigation;investigation started;fraud rigged\n",
      "    Keyword weights: 0.1798025289191013;0.13599444251629045;0.1798025289191013;0.1798025289191013;0.08114949268160139;0.08114949268160139;0.08114949268160139;0.08114949268160139\n",
      "    Strength: 0.08715508829945298\n",
      "    Document IDs: 14241248046650668697 14988735547592816033 16828254283062838304\n",
      "    Document exposures: 0.4453038991783845 0.2539369436730905 0.30075915714852497\n",
      "---------------\n",
      "Topic 8 keywords:\n",
      "    Keywords: trump campaign;lou dobbs;evidence collusion;dobbs special;democrats evidence;conflicts angry;collusion trump;angry democrats\n",
      "    Keyword weights: 0.1400862068965517;0.12284482758620689;0.12284482758620689;0.12284482758620689;0.12284482758620689;0.12284482758620689;0.12284482758620689;0.12284482758620689\n",
      "    Strength: 0.08972718315096134\n",
      "    Document IDs: 3325720912382988533 4746121785136787662 11357478787751126246 16856013646116686449\n",
      "    Document exposures: 0.049577727591878194 0.44981010951795436 0.05080205337221325 0.44981010951795436\n",
      "---------------\n",
      "-------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print('------------- Get list of topics from dataset --------------')\n",
    "dataset = 'trump_tweets'\n",
    "#query = '(\"Trump\" OR \"president\")' # str | Fulltext query, using mysql MATCH boolean query format. Example, (\\\"word1\\\" OR \\\"word2\\\") AND (\\\"word3\\\" OR \\\"word4\\\") (optional)\n",
    "query = ''\n",
    "custom_stop_words = [\"real\",\"hillary\"] # str | List of stop words. (optional)\n",
    "num_topics = 8 # int | Number of topics to be extracted from the dataset. (optional) (default to 8)\n",
    "metadata_selection = \"\" # dict | JSON object specifying metadata-based queries on the dataset, of type {\"metadata_field\": \"selected_values\"} (optional)\n",
    "period_start = \"2016-10-15 04:30:00\" # str | Start date for the period to analyze within the dataset. Format: \"YYYY-MM-DD HH:MM:SS\"\n",
    "period_end = \"2019-01-01 12:00:05\" # str | End date for the period to analyze within the dataset. Format: \"YYYY-MM-DD HH:MM:SS\"\n",
    "\n",
    "try:\n",
    "    payload = nucleus_api.Topics(dataset=dataset,                                \n",
    "                                query=query,                   \n",
    "                                custom_stop_words=custom_stop_words,     \n",
    "                                num_topics=num_topics,\n",
    "                                metadata_selection=metadata_selection,\n",
    "                                period_start=period_start,\n",
    "                                period_end=period_end)\n",
    "    api_response = api_instance.post_topic_api(payload)        \n",
    "except ApiException as e:\n",
    "    print(\"Exception when calling TopicsApi->post_topic_api: %s\\n\" % e)\n",
    "    \n",
    "#print(api_response)\n",
    "i = 1\n",
    "for res in api_response.result:\n",
    "    print('Topic', i, 'keywords:')\n",
    "    print('    Keywords:', res.topic)\n",
    "    keywords_weight_str = \";\".join(str(x) for x in res.keywords_weight)\n",
    "    print('    Keyword weights:', keywords_weight_str)\n",
    "    print('    Strength:', res.strength)\n",
    "    doc_topic_exposure_sel = []  # list of non-zero doc_topic_exposure\n",
    "    doc_id_sel = []        # list of doc ids matching doc_topic_exposure_sel\n",
    "    for j in range(len(res.doc_topic_exposure)):\n",
    "        doc_topic_exp = float(res.doc_topic_exposure[j])\n",
    "        if doc_topic_exp != 0:\n",
    "            doc_topic_exposure_sel.append(doc_topic_exp)\n",
    "            doc_id_sel.append(res.doc_id[j])\n",
    "    \n",
    "    doc_id_sel_str = ' '.join(str(x) for x in doc_id_sel)\n",
    "    doc_topic_exposure_sel_str = ' '.join(str(x) for x in doc_topic_exposure_sel)\n",
    "    print('    Document IDs:', doc_id_sel_str)\n",
    "    print('    Document exposures:', doc_topic_exposure_sel_str)\n",
    "\n",
    "    print('---------------')\n",
    "    i = i + 1\n",
    "    \n",
    "print('-------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get list of topics from dataset with a metadata selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------- Get list of topics from dataset --------------\n",
      "Topic 1 keywords:\n",
      "    Keywords: worse hightax;hightax andrew;andrew cuomo\n",
      "    Keyword weights: 0.3333333333333333;0.3333333333333333;0.3333333333333333\n",
      "    Strength: 0.3333333333333333\n",
      "    Document IDs: 5864841412738683134\n",
      "    Document exposures: 1.0\n",
      "---------------\n",
      "Topic 2 keywords:\n",
      "    Keywords: statement america;cuomo statement;america great\n",
      "    Keyword weights: 0.3333333333333333;0.3333333333333333;0.3333333333333333\n",
      "    Strength: 0.3333333333333333\n",
      "    Document IDs: 5864841412738683134\n",
      "    Document exposures: 1.0\n",
      "---------------\n",
      "Topic 3 keywords:\n",
      "    Keywords: hillary clinton;great hillary;great great\n",
      "    Keyword weights: 0.3333333333333333;0.3333333333333333;0.3333333333333333\n",
      "    Strength: 0.3333333333333333\n",
      "    Document IDs: 5864841412738683134\n",
      "    Document exposures: 1.0\n",
      "---------------\n",
      "-------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print('------------- Get list of topics from dataset --------------')\n",
    "dataset = 'trump_tweets'\n",
    "#query = '(\"Trump\" OR \"president\")' # str | Fulltext query, using mysql MATCH boolean query format. Example, (\\\"word1\\\" OR \\\"word2\\\") AND (\\\"word3\\\" OR \\\"word4\\\") (optional)\n",
    "query = ''\n",
    "custom_stop_words = [\"real\",\"hillary\"] # str | List of stop words. (optional)\n",
    "num_topics = 8 # int | Number of topics to be extracted from the dataset. (optional) (default to 8)\n",
    "metadata_selection = {\"author\": \"D_Trump16\"} # dict | JSON object specifying metadata-based queries on the dataset, of type {\"metadata_field\": \"selected_values\"} (optional)\n",
    "\n",
    "try:\n",
    "    payload = nucleus_api.Topics(dataset=dataset,                                \n",
    "                                query=query,                   \n",
    "                                custom_stop_words=custom_stop_words,     \n",
    "                                num_topics=num_topics,\n",
    "                                metadata_selection=metadata_selection)\n",
    "    api_response = api_instance.post_topic_api(payload)        \n",
    "except ApiException as e:\n",
    "    print(\"Exception when calling TopicsApi->post_topic_api: %s\\n\" % e)\n",
    "    \n",
    "#print(api_response)\n",
    "i = 1\n",
    "for res in api_response.result:\n",
    "    print('Topic', i, 'keywords:')\n",
    "    print('    Keywords:', res.topic)\n",
    "    keywords_weight_str = \";\".join(str(x) for x in res.keywords_weight)\n",
    "    print('    Keyword weights:', keywords_weight_str)\n",
    "    print('    Strength:', res.strength)\n",
    "    doc_topic_exposure_sel = []  # list of non-zero doc_topic_exposure\n",
    "    doc_id_sel = []        # list of doc ids matching doc_topic_exposure_sel\n",
    "    for j in range(len(res.doc_topic_exposure)):\n",
    "        doc_topic_exp = float(res.doc_topic_exposure[j])\n",
    "        if doc_topic_exp != 0:\n",
    "            doc_topic_exposure_sel.append(doc_topic_exp)\n",
    "            doc_id_sel.append(res.doc_id[j])\n",
    "    \n",
    "    doc_id_sel_str = ' '.join(str(x) for x in doc_id_sel)\n",
    "    doc_topic_exposure_sel_str = ' '.join(str(x) for x in doc_topic_exposure_sel)\n",
    "    print('    Document IDs:', doc_id_sel_str)\n",
    "    print('    Document exposures:', doc_topic_exposure_sel_str)\n",
    "\n",
    "    print('---------------')\n",
    "    i = i + 1\n",
    "    \n",
    "print('-------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get topic summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------- Get topic summary -----------------------\n",
      "Topic 1 summary:\n",
      "    Keywords: illegal aliens;united permission;permission illegal;enter united;aliens treated;aliens illegal;treated entered;entered legally\n",
      "{'attribute': {'author': 'D_Trump57', 'counts': None, 'time': 1534339080},\n",
      " 'sentences': \"['“People who enter the United States without our permission \"\n",
      "              'are illegal aliens and illegal aliens should not be treated the '\n",
      "              \"same as people who entered the US legally”.']\",\n",
      " 'sourceid': '3397215194896514820',\n",
      " 'title': 'D_Trump2018_8_15_13_18'}\n",
      "    Document ID: 3397215194896514820\n",
      "        Title: D_Trump2018_8_15_13_18\n",
      "        Sentences: ['“People who enter the United States without our permission are illegal aliens and illegal aliens should not be treated the same as people who entered the US legally”.']\n",
      "        Author: D_Trump57\n",
      "        Time: 2018-08-15 06:18:00\n",
      "{'attribute': {'author': 'D_Trump63', 'counts': None, 'time': 1534337040},\n",
      " 'sentences': \"['“People who enter the United States without our permission \"\n",
      "              'are illegal aliens and illegal aliens should not be treated the '\n",
      "              \"same as people who enters the US legally”.']\",\n",
      " 'sourceid': '4825367511331474696',\n",
      " 'title': 'D_Trump2018_8_15_12_44'}\n",
      "    Document ID: 4825367511331474696\n",
      "        Title: D_Trump2018_8_15_12_44\n",
      "        Sentences: ['“People who enter the United States without our permission are illegal aliens and illegal aliens should not be treated the same as people who enters the US legally”.']\n",
      "        Author: D_Trump63\n",
      "        Time: 2018-08-15 05:44:00\n",
      "---------------\n",
      "Topic 2 summary:\n",
      "    Keywords: bruce ohr;ohr justice;justice department;christopher steele;helping disgraced;disgraced christopher;believe accused;accused helping\n",
      "{'attribute': {'author': 'D_Trump13', 'counts': None, 'time': 1534544940},\n",
      " 'sentences': \"['“Fox News has learned that Bruce Ohr wrote Christopher Steele \"\n",
      "              'following the firing of James Comey saying that he was afraid '\n",
      "              \"the anti-Trump Russia probe will be exposed”.']\",\n",
      " 'sourceid': '7290029718334628379',\n",
      " 'title': 'D_Trump2018_8_17_22_29'}\n",
      "    Document ID: 7290029718334628379\n",
      "        Title: D_Trump2018_8_17_22_29\n",
      "        Sentences: ['“Fox News has learned that Bruce Ohr wrote Christopher Steele following the firing of James Comey saying that he was afraid the anti-Trump Russia probe will be exposed”.']\n",
      "        Author: D_Trump13\n",
      "        Time: 2018-08-17 15:29:00\n",
      "{'attribute': {'author': 'D_Trump31', 'counts': None, 'time': 1534463580},\n",
      " 'sentences': \"['“Very concerned about Comey’s firing afraid they will be \"\n",
      "              \"exposed” said Bruce Ohr.']\",\n",
      " 'sourceid': '12936417737022695482',\n",
      " 'title': 'D_Trump2018_8_16_23_53'}\n",
      "    Document ID: 12936417737022695482\n",
      "        Title: D_Trump2018_8_16_23_53\n",
      "        Sentences: ['“Very concerned about Comey’s firing afraid they will be exposed” said Bruce Ohr.']\n",
      "        Author: D_Trump31\n",
      "        Time: 2018-08-16 16:53:00\n",
      "{'attribute': {'author': 'D_Trump33', 'counts': None, 'time': 1534462620},\n",
      " 'sentences': \"['“The FBI received documents from Bruce Ohr (of the Justice \"\n",
      "              \"Department  whose wife Nelly worked for Fusion GPS)”.']\",\n",
      " 'sourceid': '16828254283062838304',\n",
      " 'title': 'D_Trump2018_8_16_23_37'}\n",
      "    Document ID: 16828254283062838304\n",
      "        Title: D_Trump2018_8_16_23_37\n",
      "        Sentences: ['“The FBI received documents from Bruce Ohr (of the Justice Department  whose wife Nelly worked for Fusion GPS)”.']\n",
      "        Author: D_Trump33\n",
      "        Time: 2018-08-16 16:37:00\n",
      "{'attribute': {'author': 'D_Trump70', 'counts': None, 'time': 1534247700},\n",
      " 'sentences': \"['Bruce Ohr of the “Justice” Department (can you believe he is \"\n",
      "              'still there) is accused of helping disgraced Christopher Steele '\n",
      "              \"“find dirt on Trump”.']\",\n",
      " 'sourceid': '6303783743713708484',\n",
      " 'title': 'D_Trump2018_8_14_11_55'}\n",
      "    Document ID: 6303783743713708484\n",
      "        Title: D_Trump2018_8_14_11_55\n",
      "        Sentences: ['Bruce Ohr of the “Justice” Department (can you believe he is still there) is accused of helping disgraced Christopher Steele “find dirt on Trump”.']\n",
      "        Author: D_Trump70\n",
      "        Time: 2018-08-14 04:55:00\n",
      "---------------\n",
      "Topic 3 summary:\n",
      "    Keywords: america great;statement america;andrew cuomo;great great;hillary clinton;worse hightax;hightax andrew;cuomo statement\n",
      "{'attribute': {'author': 'D_Trump16', 'counts': None, 'time': 1534533900},\n",
      " 'sentences': '[\"Which is worse Hightax Andrew Cuomo\\'s statement “WE’RE NOT '\n",
      "              'GOING TO MAKE AMERICA GREAT AGAIN IT WAS NEVER THAT GREAT” or '\n",
      "              'Hillary Clinton’s “DEPLORABLES” statement...\"]',\n",
      " 'sourceid': '5864841412738683134',\n",
      " 'title': 'D_Trump2018_8_17_19_25'}\n",
      "    Document ID: 5864841412738683134\n",
      "        Title: D_Trump2018_8_17_19_25\n",
      "        Sentences: [\"Which is worse Hightax Andrew Cuomo's statement “WE’RE NOT GOING TO MAKE AMERICA GREAT AGAIN IT WAS NEVER THAT GREAT” or Hillary Clinton’s “DEPLORABLES” statement...\"]\n",
      "        Author: D_Trump16\n",
      "        Time: 2018-08-17 12:25:00\n",
      "{'attribute': {'author': 'D_Trump17', 'counts': None, 'time': 1534515420},\n",
      " 'sentences': \"['When a politician admits that “We’re not going to make \"\n",
      "              'America great again” there doesn’t seem to be much reason to '\n",
      "              \"ever vote for him.']\",\n",
      " 'sourceid': '2205902445999073018',\n",
      " 'title': 'D_Trump2018_8_17_14_17'}\n",
      "    Document ID: 2205902445999073018\n",
      "        Title: D_Trump2018_8_17_14_17\n",
      "        Sentences: ['When a politician admits that “We’re not going to make America great again” there doesn’t seem to be much reason to ever vote for him.']\n",
      "        Author: D_Trump17\n",
      "        Time: 2018-08-17 07:17:00\n",
      "{'attribute': {'author': 'D_Trump19', 'counts': None, 'time': 1534514760},\n",
      " 'sentences': \"['Big pushback on Governor Andrew Cuomo of New York for his \"\n",
      "              \"really dumb statement about America’s lack of greatness.']\",\n",
      " 'sourceid': '8047817457772465264',\n",
      " 'title': 'D_Trump2018_8_17_14_6'}\n",
      "    Document ID: 8047817457772465264\n",
      "        Title: D_Trump2018_8_17_14_6\n",
      "        Sentences: ['Big pushback on Governor Andrew Cuomo of New York for his really dumb statement about America’s lack of greatness.']\n",
      "        Author: D_Trump19\n",
      "        Time: 2018-08-17 07:06:00\n",
      "{'attribute': {'author': 'D_Trump24', 'counts': None, 'time': 1534506240},\n",
      " 'sentences': \"['How does a politician Cuomo known for pushing people and \"\n",
      "              'businesses out of his state not to mention having the highest '\n",
      "              'taxes in the US survive making the statement WE’RE NOT GOING TO '\n",
      "              \"MAKE AMERICA GREAT AGAIN IT WAS NEVER THAT GREAT?']\",\n",
      " 'sourceid': '3545423942726121399',\n",
      " 'title': 'D_Trump2018_8_17_11_44'}\n",
      "    Document ID: 3545423942726121399\n",
      "        Title: D_Trump2018_8_17_11_44\n",
      "        Sentences: ['How does a politician Cuomo known for pushing people and businesses out of his state not to mention having the highest taxes in the US survive making the statement WE’RE NOT GOING TO MAKE AMERICA GREAT AGAIN IT WAS NEVER THAT GREAT?']\n",
      "        Author: D_Trump24\n",
      "        Time: 2018-08-17 04:44:00\n",
      "---------------\n",
      "Topic 4 summary:\n",
      "    Keywords: witch hunt;donald trump;rigged witch;frame donald;unfortunate situation;trump rigged;situation decided;decided frame\n",
      "{'attribute': {'author': 'D_Trump46', 'counts': None, 'time': 1534382040},\n",
      " 'sentences': \"['We have the unfortunate situation where they then decided \"\n",
      "              'they were going to frame Donald Trump” concerning the Rigged '\n",
      "              \"Witch Hunt.']\",\n",
      " 'sourceid': '5217366909427623007',\n",
      " 'title': 'D_Trump2018_8_16_1_14'}\n",
      "    Document ID: 5217366909427623007\n",
      "        Title: D_Trump2018_8_16_1_14\n",
      "        Sentences: ['We have the unfortunate situation where they then decided they were going to frame Donald Trump” concerning the Rigged Witch Hunt.']\n",
      "        Author: D_Trump46\n",
      "        Time: 2018-08-15 18:14:00\n",
      "{'attribute': {'author': 'D_Trump47', 'counts': None, 'time': 1534381200},\n",
      " 'sentences': \"['Former Secret Service Agent and author of new book “Spygate \"\n",
      "              \"the Attempted Sabotage of Donald J Trump” Dan Bongino.']\",\n",
      " 'sourceid': '9035906359710233744',\n",
      " 'title': 'D_Trump2018_8_16_1_0'}\n",
      "    Document ID: 9035906359710233744\n",
      "        Title: D_Trump2018_8_16_1_0\n",
      "        Sentences: ['Former Secret Service Agent and author of new book “Spygate the Attempted Sabotage of Donald J Trump” Dan Bongino.']\n",
      "        Author: D_Trump47\n",
      "        Time: 2018-08-15 18:00:00\n",
      "{'attribute': {'author': 'D_Trump67', 'counts': None, 'time': 1534252200},\n",
      " 'sentences': \"['Strzok started the illegal Rigged Witch Hunt - why isn’t this \"\n",
      "              \"so-called “probe” ended immediately?']\",\n",
      " 'sourceid': '18423518517048905190',\n",
      " 'title': 'D_Trump2018_8_14_13_10'}\n",
      "    Document ID: 18423518517048905190\n",
      "        Title: D_Trump2018_8_14_13_10\n",
      "        Sentences: ['Strzok started the illegal Rigged Witch Hunt - why isn’t this so-called “probe” ended immediately?']\n",
      "        Author: D_Trump67\n",
      "        Time: 2018-08-14 06:10:00\n",
      "{'attribute': {'author': 'D_Trump69', 'counts': None, 'time': 1534248360},\n",
      " 'sentences': \"['Gregg Jarrett on @foxandfriends  If we had a real Attorney \"\n",
      "              \"General this Witch Hunt would never have been started!']\",\n",
      " 'sourceid': '14722230792170818214',\n",
      " 'title': 'D_Trump2018_8_14_12_6'}\n",
      "    Document ID: 14722230792170818214\n",
      "        Title: D_Trump2018_8_14_12_6\n",
      "        Sentences: ['Gregg Jarrett on @foxandfriends  If we had a real Attorney General this Witch Hunt would never have been started!']\n",
      "        Author: D_Trump69\n",
      "        Time: 2018-08-14 05:06:00\n",
      "{'attribute': {'author': 'D_Trump91', 'counts': None, 'time': 1534176240},\n",
      " 'sentences': \"['Based on the fact that Strzok was in charge of the Witch Hunt \"\n",
      "              \"will it be dropped?']\",\n",
      " 'sourceid': '14988735547592816033',\n",
      " 'title': 'D_Trump2018_8_13_16_4'}\n",
      "    Document ID: 14988735547592816033\n",
      "        Title: D_Trump2018_8_13_16_4\n",
      "        Sentences: ['Based on the fact that Strzok was in charge of the Witch Hunt will it be dropped?']\n",
      "        Author: D_Trump91\n",
      "        Time: 2018-08-13 09:04:00\n",
      "{'attribute': {'author': 'D_Trump96', 'counts': None, 'time': 1534080300},\n",
      " 'sentences': \"['“Seems like the Department of Justice (and FBI) had a program \"\n",
      "              \"to keep Donald Trump from becoming President”.']\",\n",
      " 'sourceid': '5821020073909755150',\n",
      " 'title': 'D_Trump2018_8_12_13_25'}\n",
      "    Document ID: 5821020073909755150\n",
      "        Title: D_Trump2018_8_12_13_25\n",
      "        Sentences: ['“Seems like the Department of Justice (and FBI) had a program to keep Donald Trump from becoming President”.']\n",
      "        Author: D_Trump96\n",
      "        Time: 2018-08-12 06:25:00\n",
      "---------------\n",
      "Topic 5 summary:\n",
      "    Keywords: peter strzok;fired fbi;agent peter;fbi agent;strzok fraud;rigged investigation;investigation started;fraud rigged\n",
      "{'attribute': {'author': 'D_Trump33', 'counts': None, 'time': 1534462620},\n",
      " 'sentences': \"['Disgraced and fired FBI Agent Peter Strzok.']\",\n",
      " 'sourceid': '16828254283062838304',\n",
      " 'title': 'D_Trump2018_8_16_23_37'}\n",
      "    Document ID: 16828254283062838304\n",
      "        Title: D_Trump2018_8_16_23_37\n",
      "        Sentences: ['Disgraced and fired FBI Agent Peter Strzok.']\n",
      "        Author: D_Trump33\n",
      "        Time: 2018-08-16 16:37:00\n",
      "{'attribute': {'author': 'D_Trump68', 'counts': None, 'time': 1534251660},\n",
      " 'sentences': \"['Fired FBI Agent Peter Strzok is a fraud as is the rigged \"\n",
      "              \"investigation he started.']\",\n",
      " 'sourceid': '14241248046650668697',\n",
      " 'title': 'D_Trump2018_8_14_13_1'}\n",
      "    Document ID: 14241248046650668697\n",
      "        Title: D_Trump2018_8_14_13_1\n",
      "        Sentences: ['Fired FBI Agent Peter Strzok is a fraud as is the rigged investigation he started.']\n",
      "        Author: D_Trump68\n",
      "        Time: 2018-08-14 06:01:00\n",
      "{'attribute': {'author': 'D_Trump91', 'counts': None, 'time': 1534176240},\n",
      " 'sentences': \"['Agent Peter Strzok was just fired from the FBI - finally.']\",\n",
      " 'sourceid': '14988735547592816033',\n",
      " 'title': 'D_Trump2018_8_13_16_4'}\n",
      "    Document ID: 14988735547592816033\n",
      "        Title: D_Trump2018_8_13_16_4\n",
      "        Sentences: ['Agent Peter Strzok was just fired from the FBI - finally.']\n",
      "        Author: D_Trump91\n",
      "        Time: 2018-08-13 09:04:00\n",
      "---------------\n",
      "Topic 6 summary:\n",
      "    Keywords: trump campaign;lou dobbs;evidence collusion;dobbs special;democrats evidence;conflicts angry;collusion trump;angry democrats\n",
      "{'attribute': {'author': 'D_Trump27', 'counts': None, 'time': 1534470840},\n",
      " 'sentences': \"['“Director Brennan’s recent statements purport to know as fact \"\n",
      "              \"that the Trump campaign colluded with a foreign power.']\",\n",
      " 'sourceid': '3325720912382988533',\n",
      " 'title': 'D_Trump2018_8_17_1_54'}\n",
      "    Document ID: 3325720912382988533\n",
      "        Title: D_Trump2018_8_17_1_54\n",
      "        Sentences: ['“Director Brennan’s recent statements purport to know as fact that the Trump campaign colluded with a foreign power.']\n",
      "        Author: D_Trump27\n",
      "        Time: 2018-08-16 18:54:00\n",
      "{'attribute': {'author': 'D_Trump43', 'counts': None, 'time': 1534386660},\n",
      " 'sentences': \"['Mark Levin “When they had power they didn’t stop the Russians \"\n",
      "              'the Chinese the North Koreans they funded the Iranians  are '\n",
      "              'responsible for the greatest scandal in American history by '\n",
      "              'interfering with our election  trying to undermine the Trump '\n",
      "              \"Campaign and Trump Presidency”.']\",\n",
      " 'sourceid': '11357478787751126246',\n",
      " 'title': 'D_Trump2018_8_16_2_31'}\n",
      "    Document ID: 11357478787751126246\n",
      "        Title: D_Trump2018_8_16_2_31\n",
      "        Sentences: ['Mark Levin “When they had power they didn’t stop the Russians the Chinese the North Koreans they funded the Iranians  are responsible for the greatest scandal in American history by interfering with our election  trying to undermine the Trump Campaign and Trump Presidency”.']\n",
      "        Author: D_Trump43\n",
      "        Time: 2018-08-15 19:31:00\n",
      "{'attribute': {'author': 'D_Trump66', 'counts': None, 'time': 1534252500},\n",
      " 'sentences': \"['Lou Dobbs: “This cannot go forward...this Special Counsel \"\n",
      "              'with all of his conflicts with his 17 Angry Democrats without '\n",
      "              \"any evidence of collusion by the Trump Campaign and Russia.']\",\n",
      " 'sourceid': '16856013646116686449',\n",
      " 'title': 'D_Trump2018_8_14_13_15'}\n",
      "    Document ID: 16856013646116686449\n",
      "        Title: D_Trump2018_8_14_13_15\n",
      "        Sentences: ['Lou Dobbs: “This cannot go forward...this Special Counsel with all of his conflicts with his 17 Angry Democrats without any evidence of collusion by the Trump Campaign and Russia.']\n",
      "        Author: D_Trump66\n",
      "        Time: 2018-08-14 06:15:00\n",
      "{'attribute': {'author': 'D_Trump73', 'counts': None, 'time': 1534245660},\n",
      " 'sentences': \"['Lou Dobbs: “This cannot go forward...this Special Councel \"\n",
      "              'with all of his conflicts with his 17 Angry Democrats without '\n",
      "              \"any evidence of collusion by the Trump Campaign and Russia.']\",\n",
      " 'sourceid': '4746121785136787662',\n",
      " 'title': 'D_Trump2018_8_14_11_21'}\n",
      "    Document ID: 4746121785136787662\n",
      "        Title: D_Trump2018_8_14_11_21\n",
      "        Sentences: ['Lou Dobbs: “This cannot go forward...this Special Councel with all of his conflicts with his 17 Angry Democrats without any evidence of collusion by the Trump Campaign and Russia.']\n",
      "        Author: D_Trump73\n",
      "        Time: 2018-08-14 04:21:00\n",
      "---------------\n",
      "Topic 7 summary:\n",
      "    Keywords: fake news;news media;wacky omarosa;presidential lowlife;omarosa modern;modern form;lowlife omarosa;communication fake\n",
      "{'attribute': {'author': 'D_Trump8', 'counts': None, 'time': 1534591920},\n",
      " 'sentences': \"['If you are weeding out Fake News there is nothing so Fake as \"\n",
      "              'CNN  MSNBC  yet I do not ask that their sick behavior be '\n",
      "              \"removed.']\",\n",
      " 'sourceid': '16442619160892914817',\n",
      " 'title': 'D_Trump2018_8_18_11_32'}\n",
      "    Document ID: 16442619160892914817\n",
      "        Title: D_Trump2018_8_18_11_32\n",
      "        Sentences: ['If you are weeding out Fake News there is nothing so Fake as CNN  MSNBC  yet I do not ask that their sick behavior be removed.']\n",
      "        Author: D_Trump8\n",
      "        Time: 2018-08-18 04:32:00\n",
      "{'attribute': {'author': 'D_Trump38', 'counts': None, 'time': 1534428600},\n",
      " 'sentences': \"['The fact is that the Press is FREE to write and say anything \"\n",
      "              'it wants but much of what it says is FAKE NEWS pushing a '\n",
      "              \"political agenda or just plain trying to hurt people.']\",\n",
      " 'sourceid': '7014079786619530089',\n",
      " 'title': 'D_Trump2018_8_16_14_10'}\n",
      "    Document ID: 7014079786619530089\n",
      "        Title: D_Trump2018_8_16_14_10\n",
      "        Sentences: ['The fact is that the Press is FREE to write and say anything it wants but much of what it says is FAKE NEWS pushing a political agenda or just plain trying to hurt people.']\n",
      "        Author: D_Trump38\n",
      "        Time: 2018-08-16 07:10:00\n",
      "{'attribute': {'author': 'D_Trump40', 'counts': None, 'time': 1534423800},\n",
      " 'sentences': \"['THE FAKE NEWS MEDIA IS THE OPPOSITION PARTY.']\",\n",
      " 'sourceid': '950604085993420810',\n",
      " 'title': 'D_Trump2018_8_16_12_50'}\n",
      "    Document ID: 950604085993420810\n",
      "        Title: D_Trump2018_8_16_12_50\n",
      "        Sentences: ['THE FAKE NEWS MEDIA IS THE OPPOSITION PARTY.']\n",
      "        Author: D_Trump40\n",
      "        Time: 2018-08-16 05:50:00\n",
      "{'attribute': {'author': 'D_Trump89', 'counts': None, 'time': 1534176780},\n",
      " 'sentences': \"['Wacky Omarosa already has a fully signed Non-Disclosure \"\n",
      "              \"Agreement!']\",\n",
      " 'sourceid': '6173618630202756293',\n",
      " 'title': 'D_Trump2018_8_13_16_13'}\n",
      "    Document ID: 6173618630202756293\n",
      "        Title: D_Trump2018_8_13_16_13\n",
      "        Sentences: ['Wacky Omarosa already has a fully signed Non-Disclosure Agreement!']\n",
      "        Author: D_Trump89\n",
      "        Time: 2018-08-13 09:13:00\n",
      "{'attribute': {'author': 'D_Trump93', 'counts': None, 'time': 1534170060},\n",
      " 'sentences': \"['While I know it’s “not presidential” to take on a lowlife \"\n",
      "              'like Omarosa and while I would rather not be doing so this is a '\n",
      "              'modern day form of communication and I know the Fake News Media '\n",
      "              'will be working overtime to make even Wacky Omarosa look '\n",
      "              \"legitimate as possible.']\",\n",
      " 'sourceid': '4767189974744133712',\n",
      " 'title': 'D_Trump2018_8_13_14_21'}\n",
      "    Document ID: 4767189974744133712\n",
      "        Title: D_Trump2018_8_13_14_21\n",
      "        Sentences: ['While I know it’s “not presidential” to take on a lowlife like Omarosa and while I would rather not be doing so this is a modern day form of communication and I know the Fake News Media will be working overtime to make even Wacky Omarosa look legitimate as possible.']\n",
      "        Author: D_Trump93\n",
      "        Time: 2018-08-13 07:21:00\n",
      "{'attribute': {'author': 'D_Trump95', 'counts': None, 'time': 1534166820},\n",
      " 'sentences': \"['Wacky Omarosa who got fired 3 times on the Apprentice now got \"\n",
      "              \"fired for the last time.']\",\n",
      " 'sourceid': '14463102537742211332',\n",
      " 'title': 'D_Trump2018_8_13_13_27'}\n",
      "    Document ID: 14463102537742211332\n",
      "        Title: D_Trump2018_8_13_13_27\n",
      "        Sentences: ['Wacky Omarosa who got fired 3 times on the Apprentice now got fired for the last time.']\n",
      "        Author: D_Trump95\n",
      "        Time: 2018-08-13 06:27:00\n",
      "---------------\n",
      "Topic 8 summary:\n",
      "    Keywords: john brennan;cia director;tuckercarlson speaking;speaking john;limited intellectually;intellectually cia;director good;brennan limited\n",
      "{'attribute': {'author': 'D_Trump4', 'counts': None, 'time': 1534597920},\n",
      " 'sentences': \"['Has anyone looked at the mistakes that John Brennan made \"\n",
      "              \"while serving as CIA Director?']\",\n",
      " 'sourceid': '14355443980237980691',\n",
      " 'title': 'D_Trump2018_8_18_13_12'}\n",
      "    Document ID: 14355443980237980691\n",
      "        Title: D_Trump2018_8_18_13_12\n",
      "        Sentences: ['Has anyone looked at the mistakes that John Brennan made while serving as CIA Director?']\n",
      "        Author: D_Trump4\n",
      "        Time: 2018-08-18 06:12:00\n",
      "{'attribute': {'author': 'D_Trump30', 'counts': None, 'time': 1534466700},\n",
      " 'sentences': \"['@TuckerCarlson speaking of John Brennan: “How did somebody so \"\n",
      "              'obviously limited intellectually get to be CIA Director in the '\n",
      "              \"first place?” Now that is a really good question!']\",\n",
      " 'sourceid': '7967605045913198983',\n",
      " 'title': 'D_Trump2018_8_17_0_45'}\n",
      "    Document ID: 7967605045913198983\n",
      "        Title: D_Trump2018_8_17_0_45\n",
      "        Sentences: ['@TuckerCarlson speaking of John Brennan: “How did somebody so obviously limited intellectually get to be CIA Director in the first place?” Now that is a really good question!']\n",
      "        Author: D_Trump30\n",
      "        Time: 2018-08-16 17:45:00\n",
      "{'attribute': {'author': 'D_Trump47', 'counts': None, 'time': 1534381200},\n",
      " 'sentences': \"['“John Brennan is a stain on the Country we deserve better \"\n",
      "              \"than this”.']\",\n",
      " 'sourceid': '9035906359710233744',\n",
      " 'title': 'D_Trump2018_8_16_1_0'}\n",
      "    Document ID: 9035906359710233744\n",
      "        Title: D_Trump2018_8_16_1_0\n",
      "        Sentences: ['“John Brennan is a stain on the Country we deserve better than this”.']\n",
      "        Author: D_Trump47\n",
      "        Time: 2018-08-15 18:00:00\n",
      "---------------\n",
      "-------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print('------------------- Get topic summary -----------------------')\n",
    "dataset = 'trump_tweets' # str | Dataset name.\n",
    "#query = '(\"Trump\" OR \"president\")' # str | Fulltext query, using mysql MATCH boolean query format. Example, (\\\"word1\\\" OR \\\"word2\\\") AND (\\\"word3\\\" OR \\\"word4\\\") (optional)\n",
    "query = ''\n",
    "custom_stop_words = [\"real\",\"hillary\"] # str | List of stop words. (optional)\n",
    "num_topics = 8 # int | Number of topics to be extracted from the dataset. (optional) (default to 8)\n",
    "num_keywords = 8 # int | Number of keywords per topic that is extracted from the dataset. (optional) (default to 8)\n",
    "summary_length = 6 # int | The maximum number of bullet points a user wants to see in each topic summary. (optional) (default to 6)\n",
    "context_amount = 0 # int | The number of sentences surrounding key summary sentences in the documents that they come from. (optional) (default to 0)\n",
    "num_docs = 20 # int | The maximum number of key documents to use for summarization. (optional) (default to 20)\n",
    "excluded_docs = '' # str | List of document IDs that should be excluded from the analysis. Example, [\"docid1\", \"docid2\", ..., \"docidN\"]  (optional)\n",
    "\n",
    "metadata_selection =\"\" # dict | JSON object specifying metadata-based queries on the dataset, of type {\"metadata_field\": \"selected_values\"} (optional)\n",
    "time_period = \"\"     # str | Time period selection. Choices: [\"1M\",\"3M\",\"6M\",\"12M\",\"3Y\",\"5Y\",\"\"]  (optional)\n",
    "period_start = \"\" # str | Start date for the period to analyze within the dataset. Format: \"YYYY-MM-DD HH:MM:SS\"\n",
    "period_end = \"\" # str | End date for the period to analyze within the dataset. Format: \"YYYY-MM-DD HH:MM:SS\"\n",
    "api_response = None\n",
    "\n",
    "try:\n",
    "    payload = nucleus_api.TopicSummaryModel\t(\n",
    "        dataset=dataset, \n",
    "        query=query,\n",
    "        custom_stop_words=custom_stop_words, \n",
    "        num_topics=num_topics, \n",
    "        num_keywords=num_keywords,\n",
    "        metadata_selection=metadata_selection,\n",
    "        summary_length=summary_length, \n",
    "        context_amount=context_amount, \n",
    "        num_docs=num_docs)\n",
    "    api_response = api_instance.post_topic_summary_api(payload)        \n",
    "except ApiException as e:\n",
    "    print(\"Exception when calling TopicsApi->post_topic_summary_api: %s\\n\" % e)\n",
    "\n",
    "#pprint(api_response)  # raw API response\n",
    "if api_response != None:\n",
    "    i = 1\n",
    "    for res in api_response.result:\n",
    "        print('Topic', i, 'summary:')\n",
    "        print('    Keywords:', res.topic)\n",
    "        for j in range(len(res.summary)):\n",
    "            print(res.summary[j])\n",
    "            print('    Document ID:', res.summary[j].sourceid)\n",
    "            print('        Title:', res.summary[j].title)\n",
    "            print('        Sentences:', res.summary[j].sentences)\n",
    "            print('        Author:', res.summary[j].attribute['author'])\n",
    "            print('        Time:', datetime.datetime.fromtimestamp(float(res.summary[j].attribute['time'])))\n",
    "        \n",
    "        print('---------------')\n",
    "        i = i + 1\n",
    "    \n",
    "print('-------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get topic sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------- Get topic sentiment ------------------------\n",
      "Topic 1 sentiment:\n",
      "    Keywords: illegal aliens;united permission;permission illegal;enter united;aliens treated;aliens illegal;treated entered;entered legally\n",
      "    Sentiment: 0.0\n",
      "    Strength: 0.12742550951026896\n",
      "    Document IDs: 3397215194896514820 4825367511331474696\n",
      "    Document Sentiments: 0.0 0.0\n",
      "    Document Scores: [0.5276908390973678, 0.4723091609026322]\n",
      "---------------\n",
      "Topic 2 sentiment:\n",
      "    Keywords: bruce ohr;ohr justice;justice department;christopher steele;helping disgraced;disgraced christopher;believe accused;accused helping\n",
      "    Sentiment: 0.04887978448256715\n",
      "    Strength: 0.15848864730324944\n",
      "    Document IDs: 6303783743713708484 7290029718334628379 12936417737022695482 13072902166717108911 13460407141547160473 16828254283062838304\n",
      "    Document Sentiments: 0.09090909090909091 0.0 0.13333333333333333 0.0 0.0 0.0\n",
      "    Document Scores: [0.3338527496109849, 0.10865322086639134, 0.13897150888449122, 0.11696195335587906, 0.11696195335587906, 0.18459861392637444]\n",
      "---------------\n",
      "Topic 3 sentiment:\n",
      "    Keywords: america great;statement america;andrew cuomo;great great;hillary clinton;worse hightax;hightax andrew;cuomo statement\n",
      "    Sentiment: 0.1250872844490546\n",
      "    Strength: 0.1414528728878926\n",
      "    Document IDs: 776902852041351634 2205902445999073018 3545423942726121399 5864841412738683134 8047817457772465264 10010199882756041615 14722230792170818214\n",
      "    Document Sentiments: -0.2222222222222222 0.15384615384615385 0.2727272727272727 0.18181818181818182 0.13333333333333333 0.0 -0.25\n",
      "    Document Scores: [0.04697817464244509, 0.10838650114544661, 0.17432869876971374, 0.37647933941398076, 0.1535540131272911, 0.06980601093745509, 0.07046726196366764]\n",
      "---------------\n",
      "Topic 4 sentiment:\n",
      "    Keywords: witch hunt;donald trump;rigged witch;frame donald;unfortunate situation;trump rigged;situation decided;decided frame\n",
      "    Sentiment: 0.1254352727698969\n",
      "    Strength: 0.14465377548369468\n",
      "    Document IDs: 2365960778917245307 5217366909427623007 5821020073909755150 9035906359710233744 14722230792170818214 14988735547592816033 15429790537822270921 18423518517048905190\n",
      "    Document Sentiments: 0.13333333333333333 0.25 0.3333333333333333 0.18181818181818182 -0.09090909090909091 0.0 0.0 0.0\n",
      "    Document Scores: [0.05021609281731102, 0.3842031578849017, 0.07939861424856225, 0.058639762857290514, 0.1588287311393124, 0.08697682411717726, 0.0561432985519732, 0.12559351838347158]\n",
      "---------------\n",
      "Topic 5 sentiment:\n",
      "    Keywords: peter strzok;fired fbi;agent peter;fbi agent;strzok fraud;rigged investigation;investigation started;fraud rigged\n",
      "    Sentiment: 0.0\n",
      "    Strength: 0.11929991261700007\n",
      "    Document IDs: 14241248046650668697 14988735547592816033 16828254283062838304\n",
      "    Document Sentiments: 0.0 0.0 0.0\n",
      "    Document Scores: [0.39424354347196056, 0.2689694925985998, 0.33678696392943963]\n",
      "---------------\n",
      "Topic 6 sentiment:\n",
      "    Keywords: trump campaign;lou dobbs;evidence collusion;dobbs special;democrats evidence;conflicts angry;collusion trump;angry democrats\n",
      "    Sentiment: 0.18758823719191003\n",
      "    Strength: 0.12282413166027603\n",
      "    Document IDs: 3325720912382988533 4746121785136787662 11357478787751126246 16856013646116686449\n",
      "    Document Sentiments: 0.25 0.18181818181818182 0.2 0.18181818181818182\n",
      "    Document Scores: [0.07241443597895086, 0.44089332669353165, 0.045798910633985924, 0.44089332669353165]\n",
      "---------------\n",
      "Topic 7 sentiment:\n",
      "    Keywords: fake news;news media;wacky omarosa;presidential lowlife;omarosa modern;modern form;lowlife omarosa;communication fake\n",
      "    Sentiment: 0.0\n",
      "    Strength: 0.1057105715676719\n",
      "    Document IDs: 950604085993420810 4767189974744133712 6173618630202756293 7014079786619530089 14463102537742211332 16442619160892914817\n",
      "    Document Sentiments: 0.0 0.0 0.0 0.0 0.0 0.0\n",
      "    Document Scores: [0.23039534818969745, 0.41001698103957174, 0.09744049505564219, 0.09129367232912593, 0.07955983105683684, 0.09129367232912593]\n",
      "---------------\n",
      "Topic 8 sentiment:\n",
      "    Keywords: john brennan;cia director;tuckercarlson speaking;speaking john;limited intellectually;intellectually cia;director good;brennan limited\n",
      "    Sentiment: 0.0\n",
      "    Strength: 0.08014457896994633\n",
      "    Document IDs: 7967605045913198983 9035906359710233744 14355443980237980691\n",
      "    Document Sentiments: 0.0 0.0 0.0\n",
      "    Document Scores: [0.6183463586231183, 0.1402168432016156, 0.2414367981752661]\n",
      "---------------\n",
      "-------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print('---------------- Get topic sentiment ------------------------')\n",
    "dataset = 'trump_tweets' # str | Dataset name.\n",
    "#query = '(\"Trump\" OR \"president\")' # str | Fulltext query, using mysql MATCH boolean query format. Example, (\\\"word1\\\" OR \\\"word2\\\") AND (\\\"word3\\\" OR \\\"word4\\\") (optional)\n",
    "query = ''\n",
    "custom_stop_words = [\"real\",\"hillary\"] # str | List of stop words. (optional)\n",
    "num_topics = 8 # int | Number of topics to be extracted from the dataset. (optional) (default to 8)\n",
    "num_keywords = 8 # int | Number of keywords per topic that is extracted from the dataset. (optional) (default to 8)\n",
    "excluded_docs = '' # str | List of document IDs that should be excluded from the analysis. Example, [\"docid1\", \"docid2\", ..., \"docidN\"]  (optional)\n",
    "custom_dict_file = {\"great\": 1.0, \"awful\": -1.0, \"clinton\":-1.0, \"trump\":1.0} # file | Custom sentiment dictionary JSON file. Example, {\"field1\": value1, ..., \"fieldN\": valueN} (optional)\n",
    "\n",
    "metadata_selection =\"\" # dict | JSON object specifying metadata-based queries on the dataset, of type {\"metadata_field\": \"selected_values\"} (optional)\n",
    "time_period = \"\"     # str | Time period selection. Choices: [\"1M\",\"3M\",\"6M\",\"12M\",\"3Y\",\"5Y\",\"\"] (optional)\n",
    "period_start = \"\" # str | Start date for the period to analyze within the dataset. Format: \"YYYY-MM-DD HH:MM:SS\"\n",
    "period_end = \"\" # str | End date for the period to analyze within the dataset. Format: \"YYYY-MM-DD HH:MM:SS\"\n",
    "\n",
    "try:\n",
    "    payload = nucleus_api.TopicSentimentModel(\n",
    "        dataset=dataset, \n",
    "        query=query, \n",
    "        custom_stop_words=custom_stop_words, \n",
    "        num_topics=num_topics, \n",
    "        num_keywords=num_keywords,\n",
    "        custom_dict_file=custom_dict_file)\n",
    "    api_response = api_instance.post_topic_sentiment_api(payload)\n",
    "    \n",
    "except ApiException as e:\n",
    "    print(\"Exception when calling TopicsApi->post_topic_sentiment_api: %s\\n\" % e)\n",
    "\n",
    "i = 1\n",
    "for res in api_response.result:\n",
    "    print('Topic', i, 'sentiment:')\n",
    "    print('    Keywords:', res.topic)\n",
    "    print('    Sentiment:', res.sentiment)\n",
    "    print('    Strength:', res.strength)\n",
    "    \n",
    "    doc_id_str = ' '.join(str(x) for x in res.doc_id)\n",
    "    doc_sentiment_str = ' '.join(str(x) for x in res.doc_sentiment)\n",
    "    doc_score_str = ' '.join(str(x) for x in res.doc_score)\n",
    "    print('    Document IDs:', doc_id_str)\n",
    "    print('    Document Sentiments:', doc_sentiment_str)\n",
    "    print('    Document Scores:', doc_score_str)\n",
    "    \n",
    "    print('---------------')\n",
    "    i = i + 1\n",
    "    \n",
    "#pprint(api_response)\n",
    "print('-------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get topic consensus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------- Get topic consensus ------------------------\n",
      "Topic 1 consensus:\n",
      "    Keywords: illegal aliens;united permission;permission illegal;enter united;aliens treated;aliens illegal;treated entered;entered legally\n",
      "    Consensus: 1.0\n",
      "    Strength: 0.12742550951026896\n",
      "---------------\n",
      "Topic 2 consensus:\n",
      "    Keywords: bruce ohr;ohr justice;justice department;christopher steele;helping disgraced;disgraced christopher;believe accused;accused helping\n",
      "    Consensus: 0.527175741504524\n",
      "    Strength: 0.15848864730324944\n",
      "---------------\n",
      "Topic 3 consensus:\n",
      "    Keywords: america great;statement america;andrew cuomo;great great;hillary clinton;worse hightax;hightax andrew;cuomo statement\n",
      "    Consensus: 0.8127485524564323\n",
      "    Strength: 0.1414528728878926\n",
      "---------------\n",
      "Topic 4 consensus:\n",
      "    Keywords: witch hunt;donald trump;rigged witch;frame donald;unfortunate situation;trump rigged;situation decided;decided frame\n",
      "    Consensus: 0.5724576278080655\n",
      "    Strength: 0.14465377548369468\n",
      "---------------\n",
      "Topic 5 consensus:\n",
      "    Keywords: peter strzok;fired fbi;agent peter;fbi agent;strzok fraud;rigged investigation;investigation started;fraud rigged\n",
      "    Consensus: 1.0\n",
      "    Strength: 0.11929991261700007\n",
      "---------------\n",
      "Topic 6 consensus:\n",
      "    Keywords: trump campaign;lou dobbs;evidence collusion;dobbs special;democrats evidence;conflicts angry;collusion trump;angry democrats\n",
      "    Consensus: 1.0\n",
      "    Strength: 0.12282413166027603\n",
      "---------------\n",
      "Topic 7 consensus:\n",
      "    Keywords: fake news;news media;wacky omarosa;presidential lowlife;omarosa modern;modern form;lowlife omarosa;communication fake\n",
      "    Consensus: 1.0\n",
      "    Strength: 0.1057105715676719\n",
      "---------------\n",
      "Topic 8 consensus:\n",
      "    Keywords: john brennan;cia director;tuckercarlson speaking;speaking john;limited intellectually;intellectually cia;director good;brennan limited\n",
      "    Consensus: 1.0\n",
      "    Strength: 0.08014457896994633\n",
      "---------------\n",
      "-------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print('---------------- Get topic consensus ------------------------')\n",
    "dataset = 'trump_tweets' # str | Dataset name.\n",
    "query = '' # str | Fulltext query, using mysql MATCH boolean query format. Example, (\\\"word1\\\" OR \\\"word2\\\") AND (\\\"word3\\\" OR \\\"word4\\\") (optional)\n",
    "custom_stop_words = [\"real\",\"hillary\"] # str | List of stop words. (optional)\n",
    "num_topics = 8 # int | Number of topics to be extracted from the dataset. (optional) (default to 8)\n",
    "num_keywords = 8 # int | Number of keywords per topic that is extracted from the dataset. (optional) (default to 8)\n",
    "excluded_docs = [''] # str | List of document IDs that should be excluded from the analysis. Example, [\"docid1\", \"docid2\", ..., \"docidN\"]  (optional)\n",
    "custom_dict_file = {\"great\": 1.0, \"awful\": -1.0, \"clinton\":-1.0, \"trump\":1.0} # file | Custom sentiment dictionary JSON file. Example, {\"field1\": value1, ..., \"fieldN\": valueN} (optional)\n",
    "\n",
    "metadata_selection =\"\" # dict | JSON object specifying metadata-based queries on the dataset, of type {\"metadata_field\": \"selected_values\"} (optional)\n",
    "time_period = \"\"     # str | Time period selection. Choices: [\"1M\",\"3M\",\"6M\",\"12M\",\"3Y\",\"5Y\",\"\"] (optional)\n",
    "period_start = \"\" # str | Start date for the period to analyze within the dataset. Format: \"YYYY-MM-DD HH:MM:SS\"\n",
    "period_end = \"\" # str | End date for the period to analyze within the dataset. Format: \"YYYY-MM-DD HH:MM:SS\"\n",
    "\n",
    "try:\n",
    "    payload = nucleus_api.TopicConsensusModel(\n",
    "        dataset=dataset, \n",
    "        query=query, \n",
    "        custom_stop_words=custom_stop_words, \n",
    "        num_topics=num_topics, \n",
    "        num_keywords=num_keywords,\n",
    "        custom_dict_file=custom_dict_file)\n",
    "    api_response = api_instance.post_topic_consensus_api(payload)\n",
    "except ApiException as e:\n",
    "    print(\"Exception when calling TopicsApi->post_topic_consensus_api: %s\\n\" % e)\n",
    "    \n",
    "i = 1\n",
    "for res in api_response.result:\n",
    "    print('Topic', i, 'consensus:')\n",
    "    print('    Keywords:', res.topic)\n",
    "    print('    Consensus:', res.consensus)\n",
    "    print('    Strength:', res.strength)\n",
    "    \n",
    "    print('---------------')\n",
    "    i = i + 1\n",
    "    \n",
    "#pprint(api_response) # raw API response\n",
    "print('-------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get topic historical analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------ Get topic historical analysis ----------------\n",
      "Exception when calling TopicsApi->post_topic_historical_analysis_api: (400)\n",
      "Reason: BAD REQUEST\n",
      "HTTP response headers: HTTPHeaderDict({'Server': 'gunicorn/19.9.0', 'Date': 'Sat, 16 Mar 2019 06:24:43 GMT', 'Connection': 'close', 'Content-Type': 'application/json', 'Content-Length': '64'})\n",
      "HTTP response body: {\"message\": \"{'message': 'The query returned an empty model'}\"}\n",
      "\n",
      "\n",
      "\n",
      "-------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print('------------ Get topic historical analysis ----------------')\n",
    "\n",
    "dataset = 'trump_tweets'   # str | Dataset name.\n",
    "update_period = 'd' # str | Frequency at which the historical anlaysis is performed. choices=[\"d\",\"m\",\"H\",\"M\"] (default to d)\n",
    "query = '' # str | Fulltext query, using mysql MATCH boolean query format. Example, (\\\"word1\\\" OR \\\"word2\\\") AND (\\\"word3\\\" OR \\\"word4\\\") (optional)\n",
    "custom_stop_words = [\"real\",\"hillary\"] # str | List of stop words (optional)\n",
    "num_topics = 8 # int | Number of topics to be extracted from the dataset. (optional) (default to 8)\n",
    "num_keywords = 8 # int | Number of keywords per topic that is extracted from the dataset. (optional) (default to 8)\n",
    "inc_step = 1 # int | Number of increments of the udpate period in between two historical computations. (optional) (default to 1)\n",
    "excluded_docs = [''] # str | List of document IDs that should be excluded from the analysis. Example, [\"docid1\", \"docid2\", ..., \"docidN\"]  (optional)\n",
    "custom_dict_file = {} # file | Custom sentiment dictionary JSON file. Example, {\"field1\": value1, ..., \"fieldN\": valueN} (optional)\n",
    "\n",
    "metadata_selection =\"\" # dict | JSON object specifying metadata-based queries on the dataset, of type {\"metadata_field\": \"selected_values\"} (optional)\n",
    "time_period = \"6M\"     # str | Time period selection. Choices: [\"1M\",\"3M\",\"6M\",\"12M\",\"3Y\",\"5Y\",\"\"] (optional)\n",
    "period_start = \"\" # str | Start date for the period to analyze within the dataset. Format: \"YYYY-MM-DD HH:MM:SS\"\n",
    "period_end = \"\" # str | End date for the period to analyze within the dataset. Format: \"YYYY-MM-DD HH:MM:SS\"\n",
    "api_response = None\n",
    "try:\n",
    "    payload = nucleus_api.TopicHistoryModel(\n",
    "        dataset=dataset, \n",
    "        time_period=time_period, \n",
    "        update_period=update_period, \n",
    "        query=query, \n",
    "        custom_stop_words=custom_stop_words, \n",
    "        num_topics=num_topics, \n",
    "        num_keywords=num_keywords, \n",
    "        metadata_selection=metadata_selection, \n",
    "        inc_step=inc_step, \n",
    "        excluded_docs=excluded_docs,\n",
    "        custom_dict_file=custom_dict_file)\n",
    "    api_response = api_instance.post_topic_historical_analysis_api(payload)\n",
    "except ApiException as e:\n",
    "    print(\"Exception when calling TopicsApi->post_topic_historical_analysis_api: %s\\n\" % e)\n",
    "\n",
    "if api_response != None:\n",
    "    #print('api_response=', api_response)\n",
    "    results = api_response.result\n",
    "\n",
    "    # chart the historical metrics when running in Jupyter Notebook\n",
    "    if running_notebook:\n",
    "        print('Plotting historical metrics data...')\n",
    "        historical_metrics = []\n",
    "        for res in results:\n",
    "            # conctruct a list of historical metrics dictionaries for charting\n",
    "            historical_metrics.append({\n",
    "                'topic'    : res.topic,\n",
    "                'time_stamps' : np.array(res.time_stamps),\n",
    "                'strength' : np.array(res.strength, dtype=np.float32),\n",
    "                'consensus': np.array(res.consensus, dtype=np.float32), \n",
    "                'sentiment': np.array(res.sentiment, dtype=np.float32)})\n",
    "\n",
    "        selected_topics = range(len(historical_metrics)) \n",
    "        topic_charts_historical(historical_metrics, selected_topics, True)\n",
    "    else:\n",
    "        print('Printing historical metrics data...')\n",
    "        print('NOTE: historical metrics data can be plotted when running the example in Jupyter Notebook')\n",
    "        i = 1\n",
    "        for res in results:\n",
    "            print('Topic', i, res.topic)\n",
    "            print('    Timestamps:', res.time_stamps)\n",
    "            print('    Strength:', res.strength)\n",
    "            print('    Consensus:', res.consensus)\n",
    "            print('    Sentiment:', res.sentiment)\n",
    "            print('----------------')\n",
    "            i = i + 1\n",
    "\n",
    "print('-------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get author connectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------- Get author connectivity -------------------\n",
      "Mainstream connections:\n",
      "    Topic: worse hightax;hightax andrew;andrew cuomo\n",
      "    Authors: D_Trump44\n",
      "Niche connections:\n",
      "    Topic: statement america;cuomo statement;america great\n",
      "    Authors: D_Trump24\n",
      "-------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print('----------------- Get author connectivity -------------------')\n",
    "dataset = dataset # str | Dataset name.\n",
    "target_author = 'D_Trump16' # str | Name of the author to be analyzed.\n",
    "query = '' # str | Fulltext query, using mysql MATCH boolean query format. Subject covered by the author, on which to focus the analysis of connectivity. Example, (\\\"word1\\\" OR \\\"word2\\\") AND (\\\"word3\\\" OR \\\"word4\\\") (optional)\n",
    "custom_stop_words = [\"real\",\"hillary\"] # str | List of words possibly used by the target author that are considered not information-bearing. (optional)\n",
    "excluded_docs = [''] # str | List of document IDs that should be excluded from the analysis. Example, [\"docid1\", \"docid2\", ..., \"docidN\"]  (optional)\n",
    "\n",
    "metadata_selection =\"\" # dict | JSON object specifying metadata-based queries on the dataset, of type {\"metadata_field\": \"selected_values\"} (optional)\n",
    "time_period = \"12M\"     # str | Time period selection. Choices: [\"1M\",\"3M\",\"6M\",\"12M\",\"3Y\",\"5Y\",\"\"] (optional)\n",
    "period_start = \"\" # str | Start date for the period to analyze within the dataset. Format: \"YYYY-MM-DD HH:MM:SS\"\n",
    "period_end = \"\" # str | End date for the period to analyze within the dataset. Format: \"YYYY-MM-DD HH:MM:SS\"\n",
    "\n",
    "try:\n",
    "    payload = nucleus_api.AuthorConnection(dataset=dataset, \n",
    "                                            target_author=target_author, \n",
    "                                            query=query, \n",
    "                                            custom_stop_words=custom_stop_words, \n",
    "                                            time_period=time_period, \n",
    "                                            metadata_selection=metadata_selection, \n",
    "                                            excluded_docs=excluded_docs)\n",
    "    api_response = api_instance.post_author_connectivity_api(payload)    \n",
    "except ApiException as e:\n",
    "    print(\"Exception when calling TopicsApi->post_author_connectivity_api: %s\\n\" % e)\n",
    "\n",
    "res = api_response.result\n",
    "print('Mainstream connections:')\n",
    "for mc in res.mainstream_connection:\n",
    "    print('    Topic:', mc.topic)\n",
    "    print('    Authors:', \" \".join(str(x) for x in mc.authors))\n",
    "    \n",
    "print('Niche connections:')\n",
    "for nc in res.niche_connection:\n",
    "    print('    Topic:', nc.topic)\n",
    "    print('    Authors:', \" \".join(str(x) for x in nc.authors))  \n",
    "    \n",
    "#pprint(api_response)   # raw API response\n",
    "print('-------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get topic delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------- Get topic deltas -----------------------\n",
      "Topic 1 changes in exposure:\n",
      "    Keywords: crooked hillary;collusion obstruction;obstruction crooked;hillary democrats;democrats dnc;hillary clinton;clinton sham;charge crooked\n",
      "    Document ID: ['372746459070796601', '776902852041351634', '1292265014981711161', '1620156333107313580', '1854520462215508183', '2365960778917245307', '4555868983588618437', '4746121785136787662', '4767189974744133712', '4825367511331474696', '5821020073909755150', '6173618630202756293', '6303783743713708484', '6468365417517605478', '6658627797545824528', '7887407208809957066', '8991483632660067955', '9384092744660032334', '10547779125865178270', '10595041987461739196', '11485414282913601829', '11760987759040078706', '11781010922933920259', '12926169240333359924', '13077487413648394209', '14241248046650668697', '14463102537742211332', '14579590163033179898', '14722230792170818214', '14988735547592816033', '15758769748652033371', '16084996898873488732', '16085621648536044385', '16836421281688546980', '16856013646116686449', '17263586507006051906', '17884878914091736049', '18423518517048905190'] ['950604085993420810', '1380411530707030282', '2205902445999073018', '2383865888350638791', '2952292854093486503', '3325720912382988533', '3328210989202773955', '3499421997204683102', '3545423942726121399', '3683627708016583172', '4625946039318940221', '5098711196746147249', '5217366909427623007', '5566900818722282521', '5620968974223273808', '5864841412738683134', '7014079786619530089', '7180359259391996839', '7242230233701612989', '7290029718334628379', '7967605045913198983', '8047817457772465264', '8192928964490616283', '9035906359710233744', '10006474250568936611', '10010199882756041615', '10370745183868017022', '11198341698462345569', '11357478787751126246', '11767302015801488535', '12936417737022695482', '12945860735388879748', '13072902166717108911', '13460407141547160473', '13630404863291543956', '14355443980237980691', '14425090730044024457', '14507484009580738024', '15030888264722461978', '15106002331703125396', '16442619160892914817', '16828254283062838304', '17770243791305455215']\n",
      "    Per Source Change in Exposure: ['-0.0', '-0.256884931791319', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.4545655649763954', '-0.0', '-0.0', '-0.0539978185756086', '-0.08979589394560457', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.14475579071107253']\n",
      "---------------\n",
      "Topic 2 changes in exposure:\n",
      "    Keywords: fort drum;drum york;york heroes;great fort;landed fort;total endorsement;complete total;white house\n",
      "    Document ID: ['372746459070796601', '776902852041351634', '1292265014981711161', '1620156333107313580', '1854520462215508183', '2365960778917245307', '4555868983588618437', '4746121785136787662', '4767189974744133712', '4825367511331474696', '5821020073909755150', '6173618630202756293', '6303783743713708484', '6468365417517605478', '6658627797545824528', '7887407208809957066', '8991483632660067955', '9384092744660032334', '10547779125865178270', '10595041987461739196', '11485414282913601829', '11760987759040078706', '11781010922933920259', '12926169240333359924', '13077487413648394209', '14241248046650668697', '14463102537742211332', '14579590163033179898', '14722230792170818214', '14988735547592816033', '15758769748652033371', '16084996898873488732', '16085621648536044385', '16836421281688546980', '16856013646116686449', '17263586507006051906', '17884878914091736049', '18423518517048905190'] ['950604085993420810', '1380411530707030282', '2205902445999073018', '2383865888350638791', '2952292854093486503', '3325720912382988533', '3328210989202773955', '3499421997204683102', '3545423942726121399', '3683627708016583172', '4625946039318940221', '5098711196746147249', '5217366909427623007', '5566900818722282521', '5620968974223273808', '5864841412738683134', '7014079786619530089', '7180359259391996839', '7242230233701612989', '7290029718334628379', '7967605045913198983', '8047817457772465264', '8192928964490616283', '9035906359710233744', '10006474250568936611', '10010199882756041615', '10370745183868017022', '11198341698462345569', '11357478787751126246', '11767302015801488535', '12936417737022695482', '12945860735388879748', '13072902166717108911', '13460407141547160473', '13630404863291543956', '14355443980237980691', '14425090730044024457', '14507484009580738024', '15030888264722461978', '15106002331703125396', '16442619160892914817', '16828254283062838304', '17770243791305455215']\n",
      "    Per Source Change in Exposure: ['-2.9027771030266697e-09', '-0.0', '-0.0013562652271168973', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0007357895617223518', '-0.0', '-0.0', '-0.586804274490024', '-0.0', '-0.0013102766915938093', '-0.0', '-0.0', '-0.0', '-0.0', '-2.9027771030266697e-09', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.4097933849143136', '-0.0', '-3.309675117411477e-09', '-0.0', '-0.0']\n",
      "---------------\n",
      "Topic 3 changes in exposure:\n",
      "    Keywords: donald trump;fake dossier;premised fake;paid hillary;mueller investigation;investigation premised;dossier paid;bob mueller\n",
      "    Document ID: ['372746459070796601', '776902852041351634', '1292265014981711161', '1620156333107313580', '1854520462215508183', '2365960778917245307', '4555868983588618437', '4746121785136787662', '4767189974744133712', '4825367511331474696', '5821020073909755150', '6173618630202756293', '6303783743713708484', '6468365417517605478', '6658627797545824528', '7887407208809957066', '8991483632660067955', '9384092744660032334', '10547779125865178270', '10595041987461739196', '11485414282913601829', '11760987759040078706', '11781010922933920259', '12926169240333359924', '13077487413648394209', '14241248046650668697', '14463102537742211332', '14579590163033179898', '14722230792170818214', '14988735547592816033', '15758769748652033371', '16084996898873488732', '16085621648536044385', '16836421281688546980', '16856013646116686449', '17263586507006051906', '17884878914091736049', '18423518517048905190'] ['950604085993420810', '1380411530707030282', '2205902445999073018', '2383865888350638791', '2952292854093486503', '3325720912382988533', '3328210989202773955', '3499421997204683102', '3545423942726121399', '3683627708016583172', '4625946039318940221', '5098711196746147249', '5217366909427623007', '5566900818722282521', '5620968974223273808', '5864841412738683134', '7014079786619530089', '7180359259391996839', '7242230233701612989', '7290029718334628379', '7967605045913198983', '8047817457772465264', '8192928964490616283', '9035906359710233744', '10006474250568936611', '10010199882756041615', '10370745183868017022', '11198341698462345569', '11357478787751126246', '11767302015801488535', '12936417737022695482', '12945860735388879748', '13072902166717108911', '13460407141547160473', '13630404863291543956', '14355443980237980691', '14425090730044024457', '14507484009580738024', '15030888264722461978', '15106002331703125396', '16442619160892914817', '16828254283062838304', '17770243791305455215']\n",
      "    Per Source Change in Exposure: ['-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.662746062528657', '-0.0', '-0.0', '-0.0', '-0.0', '-0.11664640255672522', '-0.0', '-0.08901264512299661', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.13159488979162123', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0']\n",
      "---------------\n",
      "Topic 4 changes in exposure:\n",
      "    Keywords: trump campaign;lou dobbs;evidence collusion;dobbs special;democrats evidence;conflicts angry;collusion trump;angry democrats\n",
      "    Document ID: ['372746459070796601', '776902852041351634', '1292265014981711161', '1620156333107313580', '1854520462215508183', '2365960778917245307', '4555868983588618437', '4746121785136787662', '4767189974744133712', '4825367511331474696', '5821020073909755150', '6173618630202756293', '6303783743713708484', '6468365417517605478', '6658627797545824528', '7887407208809957066', '8991483632660067955', '9384092744660032334', '10547779125865178270', '10595041987461739196', '11485414282913601829', '11760987759040078706', '11781010922933920259', '12926169240333359924', '13077487413648394209', '14241248046650668697', '14463102537742211332', '14579590163033179898', '14722230792170818214', '14988735547592816033', '15758769748652033371', '16084996898873488732', '16085621648536044385', '16836421281688546980', '16856013646116686449', '17263586507006051906', '17884878914091736049', '18423518517048905190'] ['950604085993420810', '1380411530707030282', '2205902445999073018', '2383865888350638791', '2952292854093486503', '3325720912382988533', '3328210989202773955', '3499421997204683102', '3545423942726121399', '3683627708016583172', '4625946039318940221', '5098711196746147249', '5217366909427623007', '5566900818722282521', '5620968974223273808', '5864841412738683134', '7014079786619530089', '7180359259391996839', '7242230233701612989', '7290029718334628379', '7967605045913198983', '8047817457772465264', '8192928964490616283', '9035906359710233744', '10006474250568936611', '10010199882756041615', '10370745183868017022', '11198341698462345569', '11357478787751126246', '11767302015801488535', '12936417737022695482', '12945860735388879748', '13072902166717108911', '13460407141547160473', '13630404863291543956', '14355443980237980691', '14425090730044024457', '14507484009580738024', '15030888264722461978', '15106002331703125396', '16442619160892914817', '16828254283062838304', '17770243791305455215']\n",
      "    Per Source Change in Exposure: ['-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.5', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.5', '-0.0', '-0.0', '-0.0']\n",
      "---------------\n",
      "Topic 5 changes in exposure:\n",
      "    Keywords: wacky omarosa;fake news;omarosa modern;news media;modern form;lowlife omarosa;form communication;communication fake\n",
      "    Document ID: ['372746459070796601', '776902852041351634', '1292265014981711161', '1620156333107313580', '1854520462215508183', '2365960778917245307', '4555868983588618437', '4746121785136787662', '4767189974744133712', '4825367511331474696', '5821020073909755150', '6173618630202756293', '6303783743713708484', '6468365417517605478', '6658627797545824528', '7887407208809957066', '8991483632660067955', '9384092744660032334', '10547779125865178270', '10595041987461739196', '11485414282913601829', '11760987759040078706', '11781010922933920259', '12926169240333359924', '13077487413648394209', '14241248046650668697', '14463102537742211332', '14579590163033179898', '14722230792170818214', '14988735547592816033', '15758769748652033371', '16084996898873488732', '16085621648536044385', '16836421281688546980', '16856013646116686449', '17263586507006051906', '17884878914091736049', '18423518517048905190'] ['950604085993420810', '1380411530707030282', '2205902445999073018', '2383865888350638791', '2952292854093486503', '3325720912382988533', '3328210989202773955', '3499421997204683102', '3545423942726121399', '3683627708016583172', '4625946039318940221', '5098711196746147249', '5217366909427623007', '5566900818722282521', '5620968974223273808', '5864841412738683134', '7014079786619530089', '7180359259391996839', '7242230233701612989', '7290029718334628379', '7967605045913198983', '8047817457772465264', '8192928964490616283', '9035906359710233744', '10006474250568936611', '10010199882756041615', '10370745183868017022', '11198341698462345569', '11357478787751126246', '11767302015801488535', '12936417737022695482', '12945860735388879748', '13072902166717108911', '13460407141547160473', '13630404863291543956', '14355443980237980691', '14425090730044024457', '14507484009580738024', '15030888264722461978', '15106002331703125396', '16442619160892914817', '16828254283062838304', '17770243791305455215']\n",
      "    Per Source Change in Exposure: ['-0.08341163435835747', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.6464026575704956', '-0.0', '-0.0', '-0.17378637291430893', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.09639933515683802', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0']\n",
      "---------------\n",
      "Topic 6 changes in exposure:\n",
      "    Keywords: peter strzok;fired fbi;agent peter;strzok fraud;rigged investigation;investigation started;fraud rigged;fbi agent\n",
      "    Document ID: ['372746459070796601', '776902852041351634', '1292265014981711161', '1620156333107313580', '1854520462215508183', '2365960778917245307', '4555868983588618437', '4746121785136787662', '4767189974744133712', '4825367511331474696', '5821020073909755150', '6173618630202756293', '6303783743713708484', '6468365417517605478', '6658627797545824528', '7887407208809957066', '8991483632660067955', '9384092744660032334', '10547779125865178270', '10595041987461739196', '11485414282913601829', '11760987759040078706', '11781010922933920259', '12926169240333359924', '13077487413648394209', '14241248046650668697', '14463102537742211332', '14579590163033179898', '14722230792170818214', '14988735547592816033', '15758769748652033371', '16084996898873488732', '16085621648536044385', '16836421281688546980', '16856013646116686449', '17263586507006051906', '17884878914091736049', '18423518517048905190'] ['950604085993420810', '1380411530707030282', '2205902445999073018', '2383865888350638791', '2952292854093486503', '3325720912382988533', '3328210989202773955', '3499421997204683102', '3545423942726121399', '3683627708016583172', '4625946039318940221', '5098711196746147249', '5217366909427623007', '5566900818722282521', '5620968974223273808', '5864841412738683134', '7014079786619530089', '7180359259391996839', '7242230233701612989', '7290029718334628379', '7967605045913198983', '8047817457772465264', '8192928964490616283', '9035906359710233744', '10006474250568936611', '10010199882756041615', '10370745183868017022', '11198341698462345569', '11357478787751126246', '11767302015801488535', '12936417737022695482', '12945860735388879748', '13072902166717108911', '13460407141547160473', '13630404863291543956', '14355443980237980691', '14425090730044024457', '14507484009580738024', '15030888264722461978', '15106002331703125396', '16442619160892914817', '16828254283062838304', '17770243791305455215']\n",
      "    Per Source Change in Exposure: ['-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.6688122977203869', '-0.0', '-0.0', '-0.0', '-0.3311877022796132', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0']\n",
      "---------------\n",
      "Topic 7 changes in exposure:\n",
      "    Keywords: illegal aliens;united permission;treated enters;permission illegal;enters legally;enter united;aliens treated;aliens illegal\n",
      "    Document ID: ['372746459070796601', '776902852041351634', '1292265014981711161', '1620156333107313580', '1854520462215508183', '2365960778917245307', '4555868983588618437', '4746121785136787662', '4767189974744133712', '4825367511331474696', '5821020073909755150', '6173618630202756293', '6303783743713708484', '6468365417517605478', '6658627797545824528', '7887407208809957066', '8991483632660067955', '9384092744660032334', '10547779125865178270', '10595041987461739196', '11485414282913601829', '11760987759040078706', '11781010922933920259', '12926169240333359924', '13077487413648394209', '14241248046650668697', '14463102537742211332', '14579590163033179898', '14722230792170818214', '14988735547592816033', '15758769748652033371', '16084996898873488732', '16085621648536044385', '16836421281688546980', '16856013646116686449', '17263586507006051906', '17884878914091736049', '18423518517048905190'] ['950604085993420810', '1380411530707030282', '2205902445999073018', '2383865888350638791', '2952292854093486503', '3325720912382988533', '3328210989202773955', '3499421997204683102', '3545423942726121399', '3683627708016583172', '4625946039318940221', '5098711196746147249', '5217366909427623007', '5566900818722282521', '5620968974223273808', '5864841412738683134', '7014079786619530089', '7180359259391996839', '7242230233701612989', '7290029718334628379', '7967605045913198983', '8047817457772465264', '8192928964490616283', '9035906359710233744', '10006474250568936611', '10010199882756041615', '10370745183868017022', '11198341698462345569', '11357478787751126246', '11767302015801488535', '12936417737022695482', '12945860735388879748', '13072902166717108911', '13460407141547160473', '13630404863291543956', '14355443980237980691', '14425090730044024457', '14507484009580738024', '15030888264722461978', '15106002331703125396', '16442619160892914817', '16828254283062838304', '17770243791305455215']\n",
      "    Per Source Change in Exposure: ['-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-1.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0']\n",
      "---------------\n",
      "Topic 8 changes in exposure:\n",
      "    Keywords: heard heard;heard trump;claims heard;witch hunt;strzok firing;special councel;judicial watch;campaign russia\n",
      "    Document ID: ['372746459070796601', '776902852041351634', '1292265014981711161', '1620156333107313580', '1854520462215508183', '2365960778917245307', '4555868983588618437', '4746121785136787662', '4767189974744133712', '4825367511331474696', '5821020073909755150', '6173618630202756293', '6303783743713708484', '6468365417517605478', '6658627797545824528', '7887407208809957066', '8991483632660067955', '9384092744660032334', '10547779125865178270', '10595041987461739196', '11485414282913601829', '11760987759040078706', '11781010922933920259', '12926169240333359924', '13077487413648394209', '14241248046650668697', '14463102537742211332', '14579590163033179898', '14722230792170818214', '14988735547592816033', '15758769748652033371', '16084996898873488732', '16085621648536044385', '16836421281688546980', '16856013646116686449', '17263586507006051906', '17884878914091736049', '18423518517048905190'] ['950604085993420810', '1380411530707030282', '2205902445999073018', '2383865888350638791', '2952292854093486503', '3325720912382988533', '3328210989202773955', '3499421997204683102', '3545423942726121399', '3683627708016583172', '4625946039318940221', '5098711196746147249', '5217366909427623007', '5566900818722282521', '5620968974223273808', '5864841412738683134', '7014079786619530089', '7180359259391996839', '7242230233701612989', '7290029718334628379', '7967605045913198983', '8047817457772465264', '8192928964490616283', '9035906359710233744', '10006474250568936611', '10010199882756041615', '10370745183868017022', '11198341698462345569', '11357478787751126246', '11767302015801488535', '12936417737022695482', '12945860735388879748', '13072902166717108911', '13460407141547160473', '13630404863291543956', '14355443980237980691', '14425090730044024457', '14507484009580738024', '15030888264722461978', '15106002331703125396', '16442619160892914817', '16828254283062838304', '17770243791305455215']\n",
      "    Per Source Change in Exposure: ['-0.0', '-0.0', '-0.0', '-0.0', '-0.00026166405918572614', '-0.0', '-0.0', '-0.00022949456147327026', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.9985683176239998', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.0', '-0.00018711516853780086', '-0.0001505153791763537', '-0.0', '-0.0003010307583527073', '-0.0', '-0.0', '-0.00011474728073663513', '-0.0', '-0.0', '-0.00018711516853780086']\n",
      "---------------\n",
      "-------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print('------------------- Get topic deltas -----------------------')\n",
    "dataset = 'trump_tweets' \n",
    "#dataset = dataset # str | Dataset name.\n",
    "#query = '(\"Trump\" OR \"president\")' # str | Fulltext query, using mysql MATCH boolean query format. Example, (\\\"word1\\\" OR \\\"word2\\\") AND (\\\"word3\\\" OR \\\"word4\\\") (optional)\n",
    "query = ''\n",
    "custom_stop_words = [\"\"] # str | List of stop words. (optional)\n",
    "num_topics = 8 # int | Number of topics to be extracted from the dataset. (optional) (default to 8)\n",
    "num_keywords = 8 # int | Number of keywords per topic that is extracted from the dataset. (optional) (default to 8)\n",
    "metadata_selection =\"\" # dict | JSON object specifying metadata-based queries on the dataset, of type {\"metadata_field\": \"selected_values\"} (optional)\n",
    "period_0_start = '2018-08-12 00:00:00'\n",
    "period_0_end = '2018-08-15 13:00:00'\n",
    "period_1_start = '2018-08-16 00:00:00'\n",
    "period_1_end = '2018-08-19 00:00:00'\n",
    "excluded_docs = '' # str | List of document IDs that should be excluded from the analysis. Example, [\"docid1\", \"docid2\", ..., \"docidN\"]  (optional)\n",
    "\n",
    "try:\n",
    "    payload = nucleus_api.TopicDeltaModel(\n",
    "        dataset=dataset, \n",
    "        query=query, \n",
    "        custom_stop_words=custom_stop_words, \n",
    "        num_topics=num_topics, \n",
    "        num_keywords=num_keywords,\n",
    "        period_0_start=period_0_start,\n",
    "        period_0_end=period_0_end,\n",
    "        period_1_start=period_1_start,\n",
    "        period_1_end=period_1_end,\n",
    "        metadata_selection=metadata_selection)\n",
    "    api_response = api_instance.post_topic_delta_api(payload)        \n",
    "except ApiException as e:\n",
    "    print(\"Exception when calling TopicsApi->post_topic_delta_api: %s\\n\" % e)\n",
    "\n",
    "i = 1\n",
    "for res in api_response.result:\n",
    "    print('Topic', i, 'changes in exposure:')\n",
    "    print('    Keywords:', res.topic)\n",
    "    print('    Document ID:', res.doc_id_t0, res.doc_id_t1)\n",
    "    print('    Per Source Change in Exposure:', res.doc_topic_exposure_delta)\n",
    "    print('---------------')\n",
    "    i = i + 1\n",
    "    \n",
    "    \n",
    "#pprint(api_response)  # raw API response\n",
    "print('-------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get document information without content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document ID: 2383865888350638791\n",
      "    Title: D_Trump2018_8_18_1_47\n",
      "    Author: D_Trump10\n",
      "    Time: 2018-08-17 18:47:00\n",
      "---------------\n",
      "Document ID: 3397215194896514820\n",
      "    Title: D_Trump2018_8_15_13_18\n",
      "    Author: D_Trump57\n",
      "    Time: 2018-08-15 06:18:00\n",
      "---------------\n",
      "Document ID: 776902852041351634\n",
      "    Title: D_Trump2018_8_13_16_9\n",
      "    Author: D_Trump90\n",
      "    Time: 2018-08-13 09:09:00\n",
      "---------------\n",
      "-------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "dataset = 'trump_tweets' # str | Dataset name.\n",
    "doc_titles = ['D_Trump2018_8_18_1_47']   # str | The title of the documents to retrieve. Example: [\"title1\", \"title2\", ..., \"titleN\"]  (optional)\n",
    "doc_ids = ['3397215194896514820', '776902852041351634']      # str | The docid of the documents to retrieve. Example: [\"docid1\", \"docid2\", ..., \"docidN\"]  (optional)\n",
    "\n",
    "try:\n",
    "    payload = nucleus_api.DocInfo(\n",
    "        dataset=dataset, \n",
    "        doc_titles=doc_titles, \n",
    "        doc_ids=doc_ids,\n",
    "        metadata_selection='')\n",
    "    api_response = api_instance.post_doc_info(payload)\n",
    "except ApiException as e:\n",
    "    print(\"Exception when calling post_doc_info: %s\\n\" % e)\n",
    "    \n",
    "for res in api_response.result:\n",
    "    print('Document ID:', res.sourceid)\n",
    "    print('    Title:', res.title)\n",
    "    print('    Author:', res.attribute['author'])\n",
    "    print('    Time:', datetime.datetime.fromtimestamp(float(res.attribute['time'])))\n",
    "\n",
    "    print('---------------')\n",
    "    \n",
    "    \n",
    "#pprint(api_response)  # raw response from API server\n",
    "print('-------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display document info with a metadata selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document ID: 5864841412738683134\n",
      "    Title: D_Trump2018_8_17_19_25\n",
      "    Author: D_Trump16\n",
      "    Time: 2018-08-17 12:25:00\n",
      "---------------\n",
      "-------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "dataset = 'trump_tweets' # str | Dataset name.\n",
    "metadata_selection = {\"author\": \"D_Trump16\"}      # dict | A selector off metadata. Example: {\"field\": \"value\"}  (optional)\n",
    "\n",
    "try:\n",
    "    payload = nucleus_api.DocInfo(dataset=dataset, metadata_selection=metadata_selection)\n",
    "    api_response = api_instance.post_doc_info(payload)\n",
    "    \n",
    "except ApiException as e:\n",
    "    print(\"Exception when calling DocumentsApi->post_doc_info_api: %s\\n\" % e)\n",
    "\n",
    "for res in api_response.result:\n",
    "    print('Document ID:', res.sourceid)\n",
    "    print('    Title:', res.title)\n",
    "    print('    Author:', res.attribute['author'])\n",
    "    print('    Time:', datetime.datetime.fromtimestamp(float(res.attribute['time'])))\n",
    "\n",
    "    print('---------------')\n",
    "\n",
    "\n",
    "#pprint(api_response) # raw response from API server\n",
    "print('-------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display document details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document ID: 776902852041351634\n",
      "    Title: D_Trump2018_8_13_16_9\n",
      "    Author: D_Trump90\n",
      "    Time: 2018-08-13 09:09:00\n",
      "    Content None\n",
      "---------------\n",
      "-------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "dataset = 'trump_tweets' # str | Dataset name.\n",
    "#doc_titles = ['D_Trump2018_8_18_1_47']   # str | The title of the documents to retrieve. Example: [\"title1\", \"title2\", ..., \"titleN\"]  (optional)\n",
    "doc_ids = ['776902852041351634']      # str | The docid of the documents to retrieve. Example: [\"docid1\", \"docid2\", ..., \"docidN\"]  (optional)\n",
    "\n",
    "try:\n",
    "    payload = nucleus_api.DocDisplay(dataset, doc_ids=doc_ids)\n",
    "    api_response = api_instance.post_doc_display(payload)\n",
    "    \n",
    "except ApiException as e:\n",
    "    print(\"Exception when calling DocumentsApi->post_doc_display_api: %s\\n\" % e)\n",
    "\n",
    "for res in api_response.result:\n",
    "    print('Document ID:', res.sourceid)\n",
    "    print('    Title:', res.title)\n",
    "    print('    Author:', res.attribute['author'])\n",
    "    print('    Time:', datetime.datetime.fromtimestamp(float(res.attribute['time'])))\n",
    "    print('    Content', res.content)\n",
    "\n",
    "    print('---------------')\n",
    "\n",
    "\n",
    "#pprint(api_response) # raw response from API server\n",
    "print('-------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display document details with a metadata selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document ID: 5864841412738683134\n",
      "    Title: D_Trump2018_8_17_19_25\n",
      "    Author: D_Trump16\n",
      "    Time: 2018-08-17 12:25:00\n",
      "    Content None\n",
      "---------------\n",
      "-------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "dataset = 'trump_tweets' # str | Dataset name.\n",
    "metadata_selection = {\"author\": \"D_Trump16\"}      # dict | A selector off metadata. Example: {\"field\": \"value\"}  (optional)\n",
    "\n",
    "try:\n",
    "    payload = nucleus_api.DocDisplay(dataset=dataset, metadata_selection=metadata_selection)\n",
    "    api_response = api_instance.post_doc_display(payload)\n",
    "    \n",
    "except ApiException as e:\n",
    "    print(\"Exception when calling DocumentsApi->post_doc_display_api: %s\\n\" % e)\n",
    "\n",
    "for res in api_response.result:\n",
    "    print('Document ID:', res.sourceid)\n",
    "    print('    Title:', res.title)\n",
    "    print('    Author:', res.attribute['author'])\n",
    "    print('    Time:', datetime.datetime.fromtimestamp(float(res.attribute['time'])))\n",
    "    print('    Content', res.content)\n",
    "\n",
    "    print('---------------')\n",
    "\n",
    "\n",
    "#pprint(api_response) # raw response from API server\n",
    "print('-------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get document recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------- Get document recommendations -----------------\n",
      "Document recommendations for topic 1 :\n",
      "    Keywords: illegal aliens;united permission;permission illegal;enter united;aliens treated;aliens illegal;treated entered;entered legally\n",
      "    Recommendation 1 :\n",
      "        Document ID: 3397215194896514820\n",
      "        Title: D_Trump2018_8_15_13_18\n",
      "        Attribute: {'docid': 3397215194896514820, 'time': 1534339080, 'author': 'D_Trump57'}\n",
      "        Author: D_Trump57\n",
      "        Time: 2018-08-15 06:18:00\n",
      "    Recommendation 2 :\n",
      "        Document ID: 4825367511331474696\n",
      "        Title: D_Trump2018_8_15_12_44\n",
      "        Attribute: {'docid': 4825367511331474696, 'time': 1534337040, 'author': 'D_Trump63'}\n",
      "        Author: D_Trump63\n",
      "        Time: 2018-08-15 05:44:00\n",
      "---------------\n",
      "Document recommendations for topic 2 :\n",
      "    Keywords: bruce ohr;ohr justice;justice department;christopher steele;helping disgraced;disgraced christopher;believe accused;accused helping\n",
      "    Recommendation 1 :\n",
      "        Document ID: 6303783743713708484\n",
      "        Title: D_Trump2018_8_14_11_55\n",
      "        Attribute: {'docid': 6303783743713708484, 'time': 1534247700, 'author': 'D_Trump70'}\n",
      "        Author: D_Trump70\n",
      "        Time: 2018-08-14 04:55:00\n",
      "    Recommendation 2 :\n",
      "        Document ID: 16828254283062838304\n",
      "        Title: D_Trump2018_8_16_23_37\n",
      "        Attribute: {'docid': 16828254283062838304, 'time': 1534462620, 'author': 'D_Trump33'}\n",
      "        Author: D_Trump33\n",
      "        Time: 2018-08-16 16:37:00\n",
      "    Recommendation 3 :\n",
      "        Document ID: 7290029718334628379\n",
      "        Title: D_Trump2018_8_17_22_29\n",
      "        Attribute: {'docid': 7290029718334628379, 'time': 1534544940, 'author': 'D_Trump13'}\n",
      "        Author: D_Trump13\n",
      "        Time: 2018-08-17 15:29:00\n",
      "---------------\n",
      "Document recommendations for topic 3 :\n",
      "    Keywords: america great;statement america;andrew cuomo;great great;hillary clinton;worse hightax;hightax andrew;cuomo statement\n",
      "    Recommendation 1 :\n",
      "        Document ID: 5864841412738683134\n",
      "        Title: D_Trump2018_8_17_19_25\n",
      "        Attribute: {'docid': 5864841412738683134, 'time': 1534533900, 'author': 'D_Trump16'}\n",
      "        Author: D_Trump16\n",
      "        Time: 2018-08-17 12:25:00\n",
      "    Recommendation 2 :\n",
      "        Document ID: 3545423942726121399\n",
      "        Title: D_Trump2018_8_17_11_44\n",
      "        Attribute: {'docid': 3545423942726121399, 'time': 1534506240, 'author': 'D_Trump24'}\n",
      "        Author: D_Trump24\n",
      "        Time: 2018-08-17 04:44:00\n",
      "    Recommendation 3 :\n",
      "        Document ID: 8047817457772465264\n",
      "        Title: D_Trump2018_8_17_14_6\n",
      "        Attribute: {'docid': 8047817457772465264, 'time': 1534514760, 'author': 'D_Trump19'}\n",
      "        Author: D_Trump19\n",
      "        Time: 2018-08-17 07:06:00\n",
      "    Recommendation 4 :\n",
      "        Document ID: 2205902445999073018\n",
      "        Title: D_Trump2018_8_17_14_17\n",
      "        Attribute: {'docid': 2205902445999073018, 'time': 1534515420, 'author': 'D_Trump17'}\n",
      "        Author: D_Trump17\n",
      "        Time: 2018-08-17 07:17:00\n",
      "---------------\n",
      "Document recommendations for topic 4 :\n",
      "    Keywords: witch hunt;donald trump;rigged witch;frame donald;unfortunate situation;trump rigged;situation decided;decided frame\n",
      "    Recommendation 1 :\n",
      "        Document ID: 5217366909427623007\n",
      "        Title: D_Trump2018_8_16_1_14\n",
      "        Attribute: {'docid': 5217366909427623007, 'time': 1534382040, 'author': 'D_Trump46'}\n",
      "        Author: D_Trump46\n",
      "        Time: 2018-08-15 18:14:00\n",
      "    Recommendation 2 :\n",
      "        Document ID: 14722230792170818214\n",
      "        Title: D_Trump2018_8_14_12_6\n",
      "        Attribute: {'docid': 14722230792170818214, 'time': 1534248360, 'author': 'D_Trump69'}\n",
      "        Author: D_Trump69\n",
      "        Time: 2018-08-14 05:06:00\n",
      "    Recommendation 3 :\n",
      "        Document ID: 18423518517048905190\n",
      "        Title: D_Trump2018_8_14_13_10\n",
      "        Attribute: {'docid': 18423518517048905190, 'time': 1534252200, 'author': 'D_Trump67'}\n",
      "        Author: D_Trump67\n",
      "        Time: 2018-08-14 06:10:00\n",
      "---------------\n",
      "Document recommendations for topic 5 :\n",
      "    Keywords: peter strzok;fired fbi;agent peter;fbi agent;strzok fraud;rigged investigation;investigation started;fraud rigged\n",
      "    Recommendation 1 :\n",
      "        Document ID: 14241248046650668697\n",
      "        Title: D_Trump2018_8_14_13_1\n",
      "        Attribute: {'docid': 14241248046650668697, 'time': 1534251660, 'author': 'D_Trump68'}\n",
      "        Author: D_Trump68\n",
      "        Time: 2018-08-14 06:01:00\n",
      "    Recommendation 2 :\n",
      "        Document ID: 16828254283062838304\n",
      "        Title: D_Trump2018_8_16_23_37\n",
      "        Attribute: {'docid': 16828254283062838304, 'time': 1534462620, 'author': 'D_Trump33'}\n",
      "        Author: D_Trump33\n",
      "        Time: 2018-08-16 16:37:00\n",
      "    Recommendation 3 :\n",
      "        Document ID: 14988735547592816033\n",
      "        Title: D_Trump2018_8_13_16_4\n",
      "        Attribute: {'docid': 14988735547592816033, 'time': 1534176240, 'author': 'D_Trump91'}\n",
      "        Author: D_Trump91\n",
      "        Time: 2018-08-13 09:04:00\n",
      "---------------\n",
      "Document recommendations for topic 6 :\n",
      "    Keywords: trump campaign;lou dobbs;evidence collusion;dobbs special;democrats evidence;conflicts angry;collusion trump;angry democrats\n",
      "    Recommendation 1 :\n",
      "        Document ID: 16856013646116686449\n",
      "        Title: D_Trump2018_8_14_13_15\n",
      "        Attribute: {'docid': 16856013646116686449, 'time': 1534252500, 'author': 'D_Trump66'}\n",
      "        Author: D_Trump66\n",
      "        Time: 2018-08-14 06:15:00\n",
      "    Recommendation 2 :\n",
      "        Document ID: 4746121785136787662\n",
      "        Title: D_Trump2018_8_14_11_21\n",
      "        Attribute: {'docid': 4746121785136787662, 'time': 1534245660, 'author': 'D_Trump73'}\n",
      "        Author: D_Trump73\n",
      "        Time: 2018-08-14 04:21:00\n",
      "---------------\n",
      "Document recommendations for topic 7 :\n",
      "    Keywords: fake news;news media;wacky omarosa;presidential lowlife;omarosa modern;modern form;lowlife omarosa;communication fake\n",
      "    Recommendation 1 :\n",
      "        Document ID: 4767189974744133712\n",
      "        Title: D_Trump2018_8_13_14_21\n",
      "        Attribute: {'docid': 4767189974744133712, 'time': 1534170060, 'author': 'D_Trump93'}\n",
      "        Author: D_Trump93\n",
      "        Time: 2018-08-13 07:21:00\n",
      "    Recommendation 2 :\n",
      "        Document ID: 950604085993420810\n",
      "        Title: D_Trump2018_8_16_12_50\n",
      "        Attribute: {'docid': 950604085993420810, 'time': 1534423800, 'author': 'D_Trump40'}\n",
      "        Author: D_Trump40\n",
      "        Time: 2018-08-16 05:50:00\n",
      "---------------\n",
      "Document recommendations for topic 8 :\n",
      "    Keywords: john brennan;cia director;tuckercarlson speaking;speaking john;limited intellectually;intellectually cia;director good;brennan limited\n",
      "    Recommendation 1 :\n",
      "        Document ID: 7967605045913198983\n",
      "        Title: D_Trump2018_8_17_0_45\n",
      "        Attribute: {'docid': 7967605045913198983, 'time': 1534466700, 'author': 'D_Trump30'}\n",
      "        Author: D_Trump30\n",
      "        Time: 2018-08-16 17:45:00\n",
      "    Recommendation 2 :\n",
      "        Document ID: 14355443980237980691\n",
      "        Title: D_Trump2018_8_18_13_12\n",
      "        Attribute: {'docid': 14355443980237980691, 'time': 1534597920, 'author': 'D_Trump4'}\n",
      "        Author: D_Trump4\n",
      "        Time: 2018-08-18 06:12:00\n",
      "---------------\n",
      "-------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print('------------- Get document recommendations -----------------')\n",
    "\n",
    "dataset = 'trump_tweets' # str | Dataset name.\n",
    "#query = '(\"Trump\" OR \"president\")' # str | Fulltext query, using mysql MATCH boolean query format. Example, (\\\"word1\\\" OR \\\"word2\\\") AND (\\\"word3\\\" OR \\\"word4\\\") (optional)\n",
    "query = ''\n",
    "custom_stop_words = [\"real\",\"hillary\"] # str | List of stop words. (optional)\n",
    "num_topics = 8 # int | Number of topics to be extracted from the dataset. (optional) (default to 8)\n",
    "num_keywords = 8 # int | Number of keywords per topic that is extracted from the dataset. (optional) (default to 8)\n",
    "excluded_docs = '' # str | List of document IDs that should be excluded from the analysis. Example, [\"docid1\", \"docid2\", ..., \"docidN\"]  (optional)\n",
    "\n",
    "try:\n",
    "    payload = nucleus_api.DocumentRecommendModel(\n",
    "        dataset=dataset, \n",
    "        query=query, \n",
    "        custom_stop_words=custom_stop_words, \n",
    "        num_topics=num_topics, \n",
    "        num_keywords=num_keywords)\n",
    "    api_response = api_instance.post_doc_recommend_api(payload)\n",
    "except ApiException as e:\n",
    "    print(\"Exception when calling DocumentsApi->post_doc_recommend_api: %s\\n\" % e)\n",
    "    \n",
    "i = 1\n",
    "for res in api_response.result:\n",
    "    print('Document recommendations for topic', i, ':')\n",
    "    print('    Keywords:', res.topic)\n",
    "\n",
    "    j = 1\n",
    "    for doc in res.recommendations:\n",
    "        print('    Recommendation', j, ':')\n",
    "        print('        Document ID:', doc.sourceid)\n",
    "        print('        Title:', doc.title)\n",
    "        print('        Attribute:', doc.attribute)\n",
    "        print('        Author:', doc.attribute['author'])\n",
    "        print('        Time:', datetime.datetime.fromtimestamp(float(doc.attribute['time'])))\n",
    "        j = j + 1\n",
    "    \n",
    "    print('---------------')\n",
    "    i = i + 1\n",
    "    \n",
    "#pprint(api_response)   # raw API response\n",
    "print('-------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get document summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------ Get document summary  --------------------\n",
      "Summary for D_Trump2018_8_17_14_10\n",
      "    * Big pushback on Governor Andrew Cuomo of New York for his really dumb statement about America’s lack of greatness.\n",
      "-------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print('------------------ Get document summary  --------------------')\n",
    "\n",
    "dataset = 'trump_tweets' # str | Dataset name.\n",
    "doc_title = 'D_Trump2018_8_17_14_10' # str | The title of the document to be summarized.\n",
    "custom_stop_words = [\"real\",\"hillary\"] # List of stop words. (optional)\n",
    "summary_length = 6 # int | The maximum number of bullet points a user wants to see in the document summary. (optional) (default to 6)\n",
    "context_amount = 0 # int | The number of sentences surrounding key summary sentences in the documents that they come from. (optional) (default to 0)\n",
    "short_sentence_length = 0 # int | The sentence length below which a sentence is excluded from summarization (optional) (default to 4)\n",
    "long_sentence_length = 40 # int | The sentence length beyond which a sentence is excluded from summarization (optional) (default to 40)\n",
    "\n",
    "try:\n",
    "    payload = nucleus_api.DocumentSummaryModel(\n",
    "        dataset=dataset, \n",
    "        doc_title=doc_title, \n",
    "        custom_stop_words=custom_stop_words, \n",
    "        summary_length=summary_length, \n",
    "        context_amount=context_amount,\n",
    "        short_sentence_length=short_sentence_length,\n",
    "        long_sentence_length=long_sentence_length)\n",
    "    api_response = api_instance.post_doc_summary_api(payload)\n",
    "    \n",
    "    print('Summary for', api_response.result.doc_title)\n",
    "    for sent in api_response.result.summary.sentences:\n",
    "        print('    *', sent)\n",
    "\n",
    "    #pprint(api_response)   # raw API response\n",
    "    \n",
    "except ApiException as e:\n",
    "    print(\"Exception when calling DocumentsApi->post_doc_summary_api: %s\\n\" % e)\n",
    "\n",
    "\n",
    "print('-------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summarize file from URL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary for quarles20181109a-newname.pdf :\n",
      "    * But the stress test conducted by the Federal Reserve is only one part of our stress testing regime.\n",
      "    * Stress Capital Buffer\n",
      "Many of you are familiar with the Federal Reserve’s proposal to integrate the stress test with the regulatory capital rule--known as the stress capital buffer (SCB).1 I believe the SCB proposal represents an important milestone as we enter the next chapter of our stress testing regime.\n",
      "    * Transparency\n",
      "Transparency of the stress test and its inputs and outputs is key to the credibility of the stress test, and there are several initiatives underway to provide additional transparency regarding the supervisory stress test models and scenario design process.\n",
      "    * We are currently considering options to provide additional transparency regarding scenarios and scenario design and I expect that the Board will seek comment on the advisability of, and possible approaches to, gathering the public’s input on scenarios and salient risks facing the banking system each year.\n",
      "    * As originally conceived, CCAR had both a quantitative component--based on the supervisory stress test--and a qualitative component--based on the Federal Reserve’s assessment of a firm’s stress testing practices.\n",
      "    * Conclusion\n",
      "As we begin the next chapter in stress testing, my objective is to ensure the continued credibility of the program by increasing its transparency, simplicity, and stability while maintaining the strength of the supervisory and internal stress testing elements that are central to the program today.\n",
      "-------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "######################################################################################\n",
    "# file_params fields descriptions:  \n",
    "#   file_url              : string, the URL at which the file is stored (could be a S3 bucket address for instance)\n",
    "#   filename              : OPTIONAL string, filename saved on the server. also serves as the doc_title for summarization\n",
    "#   custom_stop_words     : OPTIONAL a string list, user-provided list of stopwords to be excluded from the content analysis leading to document summarization\n",
    "#                            [\"word1\", \"word2\", ...]. DEFAULT: empty\n",
    "#   summary_length        : OPTIONAL an integer, the maximum number of bullet points a user wants to see in the document summary. DEFAULT: 6\n",
    "#   context_amount        : OPTIONAL an integer, the number of sentences surrounding key summary sentences in the original document that a user wants to see in the document summary. DEFAULT: 0\n",
    "#   short_sentence_length : OPTIONAL an integer, the sentence length below which a sentence is excluded from summarization. DEFAULT: 4 words\n",
    "#   long_sentence_length  : OPTIONAL an integer, the sentence length beyond which a sentence is excluded from summarization. DEFAULT: 40 words\n",
    "#\n",
    "file_params = {\n",
    "    'file_url': 'https://s3-us-west-2.amazonaws.com/sumup-public/nucleus-sdk/quarles20181109a.docx',\n",
    "    'filename': 'quarles20181109a-newname.pdf',   \n",
    "    'custom_stop_words': [\"document\", \"sometimes\"], \n",
    "    'summary_length': 6,\n",
    "    'context_amount': 0, \n",
    "    'short_sentence_length': 4, \n",
    "    'long_sentence_length': 40}\n",
    "\n",
    "\n",
    "result = nucleus_helper.summarize_file_url(api_instance, file_params)\n",
    "\n",
    "#print(result)   \n",
    "print('Summary for', result.doc_title, ':')\n",
    "for sent in result.summary.sentences:\n",
    "    print('    *', sent)\n",
    "\n",
    "print('-------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get document sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------ Get document sentiment  --------------------\n",
      "Sentiment for D_Trump2018_8_17_14_10\n",
      "-0.3333333333333333\n",
      "-------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print('------------------ Get document sentiment  --------------------')\n",
    "\n",
    "dataset = 'trump_tweets' # str | Dataset name.\n",
    "doc_title = 'D_Trump2018_8_17_14_10' # str | The title of the document to be analyzed.\n",
    "custom_stop_words = [\"real\",\"hillary\"] # List of stop words. (optional)\n",
    "num_topics = 8 # int | Number of topics to be extracted from the document. (optional) (default to 8)\n",
    "num_keywords = 8 # int | Number of keywords per topic that is extracted from the document. (optional) (default to 8)\n",
    "\n",
    "try:\n",
    "    payload = nucleus_api.DocumentSentimentModel(\n",
    "        dataset=dataset, \n",
    "        doc_title=doc_title, \n",
    "        custom_stop_words=custom_stop_words, \n",
    "        num_topics=num_topics, \n",
    "        num_keywords=num_keywords)\n",
    "    api_response = api_instance.post_doc_sentiment_api(payload)\n",
    "    \n",
    "    print('Sentiment for', api_response.result.doc_title)\n",
    "    print(api_response.result.sentiment)\n",
    "\n",
    "    #pprint(api_response)   # raw API response\n",
    "    \n",
    "except ApiException as e:\n",
    "    print(\"Exception when calling DocumentsApi->post_doc_sentiment_api: %s\\n\" % e)\n",
    "\n",
    "\n",
    "print('-------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
