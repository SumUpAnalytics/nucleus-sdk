{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "-<h1><center>  构建情感词典 -  Nucleus API实例</center></h1>\n",
    "\n",
    "\n",
    "<h1><center>  所有权及保密条款属SumUp Analytics所有</center></h1>\n",
    "\n",
    "\n",
    "<h1><center>  免责声明和服务条款可通过 www.sumup.ai 获取</center></h1>\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "## 目标: \n",
    "-\t开发一个管道来创建一个带对比度分析的自定义情感词典\n",
    "  - 促进情绪建模的数据标签\n",
    "  - 定义一种编程方法，用于在用户选择的语料库中创建情感词典\n",
    "\n",
    "**SumUp对比分析的工作原理是用语库中两种不同类别的文档，由用户根据元数据或内容定义**\n",
    "\n",
    "## 数据:\n",
    "-\t任何文档集合，其中至少有一部分文档标记为情感类别，如 POSITIVE(正面) / NEUTRAL(中性) / NEGATIVE(负面)\n",
    "\n",
    "\n",
    "## Nucleus APIs:\n",
    "-\t数据集创建 API\n",
    " - \t*api_instance.post_upload_file(file, dataset)*\n",
    " - \t*nucleus_helper.import_files(api_instance, dataset, file_iters, processes=1)*\n",
    "\n",
    "        nucleus_helper.import_files利用api_instance.post_upload_file并行执行来加速数据集的创建\n",
    "\n",
    "-\t主题建模 API\n",
    " - \t*api_instance.post_topic_api(payload)*\n",
    "\n",
    "\n",
    "-\t对比主题建模 API\n",
    " - \t*api_instance.post_topic_contrast_api(payload)*\n",
    " \n",
    "\n",
    "-\t文件分类 API\n",
    " - \t*api_instance.post_doc_classify_api(payload)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 方法:\n",
    "\n",
    "### 1.\t培训，验证，测试数据集准备\n",
    "-\t创建包含所有相关文档的Nucleus数据集\n",
    "\n",
    "\n",
    "-   必须标记输入文档\n",
    "    - 这些文档都存储在CSV或JSON中，并且一列数据对应于情感标签\n",
    "    - 或者，您需要在Nucleus中构建数据集时将情绪标签指定为额外的元数据字段\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('---- Train / Validate / Test dataset ----')\n",
    "folder = 'Sellside_research'         \n",
    "dataset = 'Sellside_research'# str | 文件的目标数据集。\n",
    "\n",
    "# 以递归方式从文件夹构建文件\n",
    "# 其中每个采用以下格式：\n",
    "# {'filename': filename,   # 要上传的文件名。 必填\n",
    "#  'metadata': {           # 该文件的元数据。 选填\n",
    "#      'key1': val1,       # 只要名称，密钥就可以有任意名称\n",
    "#      'key2': val2        # 包含字母数字（0-9 | a-z | A-Z）和下划线（_）\n",
    "#   } \n",
    "# }\n",
    "\n",
    "# 如果文档不是CSV或JSON，则必须在file_iter对象中指定情感标签作为额外的元数据字段。\n",
    "\n",
    "# 如果您正在读取已提供情绪标签的文件，则无需传递file_dict中的“元数据”（metadata)\n",
    "\n",
    "file_iter = []\n",
    "for root, dirs, files in os.walk(folder):\n",
    "    for file in files:\n",
    "        file_dict = {'filename': os.path.join(root, file),\n",
    "                     'metadata': {'sentiment': 'positive' # 这里构建一些逻辑来决定如何分配 POS / NEU / NEG\n",
    "                                }}\n",
    "        file_iter.append(file_dict)\n",
    "\n",
    "file_props = nucleus_helper.upload_files(api_instance, dataset, file_iter, processes=4)\n",
    "for fp in file_props:\n",
    "    print(fp.filename, '(', fp.size, 'bytes) has been added to dataset', dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.\t未标记的数据集准备\n",
    "-\t创建包含所有相关文档的Nucleus数据集\n",
    "\n",
    "\n",
    "-   输入文档未标记"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('---- Dataset to label ----')\n",
    "folder = 'Sellside_research_unlabeled'         \n",
    "dataset = 'Sellside_research_unlabeled'# 字符串 | 将加入文档的目标数据集。\n",
    "\n",
    "# 以递归方式从文件夹构建文件。\n",
    "# 其中每个文件采用以下格式：\n",
    "# {'filename': filename,   # 要上传的文件名。 必填  }\n",
    "\n",
    "file_iter = []\n",
    "for root, dirs, files in os.walk(folder):\n",
    "    for file in files:\n",
    "        file_dict = {'filename': os.path.join(root, file)}\n",
    "        file_iter.append(file_dict)\n",
    "\n",
    "file_props = nucleus_helper.upload_files(api_instance, dataset, file_iter, processes=4)\n",
    "for fp in file_props:\n",
    "    print(fp.filename, '(', fp.size, 'bytes) has been added to dataset', dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 加速数据标签化\n",
    "\n",
    "-     在此例子中，我们将一类文档定义为具有正面情绪。 第二类有负面情绪\n",
    "-     我们在训练集上提取对比主题，用TopicContrastModel分隔这两个类别\n",
    "-     我们根据创建的日期将数据集划分为训练，验证和测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_selection_contrast = {\"sentiment\": [\"positive\", \"negative\"]} # dict | 元数据选择定义了两类文档，以便相互对比和总结\n",
    "\n",
    "query = '' # str | 数据集语言特定的全文查询，使用mysql MATCH boolean query格式（可选）\n",
    "custom_stop_words = [\"\"] # 停用词列表。 （可选）\n",
    "excluded_docs = '' # str | 应从分析中排除的文档ID列表。 例如，[“docid1”，“docid2”，...，“docidN”]（可选）\n",
    "syntax_variables = False # bool | 是否考虑每类文档的语法以帮助对比分析（可选）（默认为False）\n",
    "num_keywords = 20 # integer | 从数据集中提取的比较主题的关键字数。 (可选) (默认为50)\n",
    "remove_redundancies = False # bool | 如果为True，则此选项会从分析中删除准重复项。 准复制将具有相同的NLP表示，但不一定是完全相同的文本。 （可选）（默认为False）\n",
    "\n",
    "payload = nucleus_api.TopicContrastModel(dataset='Sellside_research', \n",
    "                                        metadata_selection_contrast=metadata_selection_contrast,\n",
    "                                        custom_stop_words=custom_stop_words,\n",
    "                                        period_start='2017-01-01',\n",
    "                                        period_end='2018-01-01')\n",
    "api_response = api_instance.post_topic_contrast_api(payload)\n",
    "\n",
    "print('Contrasted Topic')\n",
    "print('    Keywords:', api_response.result.keywords)\n",
    "print('    Keywords Weight:', api_response.result.keywords_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-     确定此对比主题在验证数据集上对语料库的情感标记, 执行的效果\n",
    "-     我们进一步详细说明如何微调情绪贴标机"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_topics = {\"keywords\": api_response.result.keywords, \"weights\": api_response.result.keywords_weight} # dict | 对比主题用于分隔两类文件。 重量可选\n",
    "metadata_selection_contrast = {\"sentiment\": [\"positive\", \"negative\"]} # dict | 元数据选择定义了文档可以分类的两类文档\n",
    "\n",
    "query = '' # str | 数据集语言特定的全文查询，使用mysql MATCH boolean query格式（可选）\n",
    "custom_stop_words = [\"\"] # 停用词列表。 （可选的）\n",
    "excluded_docs = '' # str | 应从分析中排除的文档ID列表。 例如，[“docid1”，“docid2”，...，“docidN”]（可选）\n",
    "syntax_variables = False # bool | 如果为True，则分类器将在内容变量之上包含与语法相关的变量（可选）（默认为False）\n",
    "threshold = 0 # float | Threshold value for a document exposure to the contrastic topic, above which the document is assigned to class 1 specified through metadata_selection. （可选）（默认为0）\n",
    "remove_redundancies = False # bool | 如果为True，则此选项会从分析中删除准重复项。 准复制将具有相同的NLP表示，但不一定是完全相同的文本。 （可选）（默认为False）\n",
    "\n",
    "\n",
    "payload = nucleus_api.DocClassifyModel(dataset=\"Sellside_research\",\n",
    "                                        fixed_topics=fixed_topics,\n",
    "                                        metadata_selection_contrast=metadata_selection_contrast,\n",
    "                                        custom_stop_words=custom_stop_words,\n",
    "                                        validation_phase=True, # 此参数告诉API数据已标记，因此生成perf度量标准\n",
    "                                        period_start='2018-01-01',\n",
    "                                        period_end='2019-01-01')\n",
    "api_response = api_instance.post_doc_classify_api(payload)\n",
    "\n",
    "print('Detailed Results')\n",
    "print('    Docids:', api_response.result.detailed_results.docids)\n",
    "print('    Exposure:', api_response.result.detailed_results.exposures)\n",
    "print('    Estimated Category:', api_response.result.detailed_results.estimated_class)\n",
    "print('    Actual Category:', api_response.result.detailed_results.true_class)\n",
    "print('\\n')\n",
    "\n",
    "print('Perf Metrics')\n",
    "print('    Accuracy:', api_response.result.perf_metrics.hit_rate)\n",
    "print('    Recall:', api_response.result.perf_metrics.recall)\n",
    "print('    Precision:', api_response.result.perf_metrics.precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-     对标签模型满意后，可以将其应用于测试数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_topics = {\"keywords\": api_response.result.keywords, \"weights\": api_response.result.keywords_weight} # dict | 对比主题用于分隔两类文件。 重量可选\n",
    "metadata_selection_contrast = {\"sentiment\": [\"positive\", \"negative\"]} # dict | 元数据选择定义了文档可以分类的两类文档\n",
    "\n",
    "query = '' # str | Dataset-language-specific全文查询，使用mysql MATCH boolean query格式（可选）\n",
    "custom_stop_words = [\"\"] # 停用词列表。 （可选的）\n",
    "excluded_docs = '' # str | 应从分析中排除的文档ID列表。 例如，[“docid1”，“docid2”，...，“docidN”]（可选）\n",
    "syntax_variables = False # bool | 如果为True，则分类器将在内容变量之上包含与语法相关的变量（可选）（默认为False）\n",
    "threshold = 0 # float | Threshold value for a document exposure to the contrastic topic, above which the document is assigned to class 1 specified through metadata_selection. （可选）（默认为0）\n",
    "remove_redundancies = False # bool | If True, this option removes quasi-duplicates from the analysis. A quasi-duplicate would have the same NLP representation, but not necessarily the exact same text. (optional) (default False)\n",
    "\n",
    "\n",
    "payload = nucleus_api.DocClassifyModel(dataset=\"Sellside_research\",\n",
    "                                        fixed_topics=fixed_topics,\n",
    "                                        metadata_selection_contrast=metadata_selection_contrast,\n",
    "                                        custom_stop_words=custom_stop_words,\n",
    "                                        validation_phase=True, # 此参数告诉API数据已标记，因此生成perf度量标准\n",
    "                                        period_start='2019-01-01',\n",
    "                                        period_end='2019-07-01')\n",
    "api_response = api_instance.post_doc_classify_api(payload)\n",
    "\n",
    "print('Detailed Results')\n",
    "print('    Docids:', api_response.result.detailed_results.docids)\n",
    "print('    Exposure:', api_response.result.detailed_results.exposures)\n",
    "print('    Estimated Category:', api_response.result.detailed_results.estimated_class)\n",
    "print('    Actual Category:', api_response.result.detailed_results.true_class)\n",
    "print('\\n')\n",
    "\n",
    "print('Perf Metrics')\n",
    "print('    Accuracy:', api_response.result.perf_metrics.hit_rate)\n",
    "print('    Recall:', api_response.result.perf_metrics.recall)\n",
    "print('    Precision:', api_response.result.perf_metrics.precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-     您现在可以在未标记的数据集上运行上述模型\n",
    "-     您将为每个文档检索“estimated_class”，从而完成数据集标注\n",
    "-     您可以对任何一对情绪标签重复此过程，交叉验证未标记数据集的情绪标签"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_topics = {\"keywords\": api_response.result.keywords, \"weights\": api_response.result.keywords_weight} # dict | 对比主题用于分隔两类文件。 重量可选\n",
    "metadata_selection_contrast = {\"sentiment\": [\"positive\", \"negative\"]} # dict | 元数据选择定义了文档可以分类的两类文档\n",
    "\n",
    "query = '' # str | Dataset-language-specific全文查询，使用mysql MATCH boolean query格式（可选）\n",
    "custom_stop_words = [\"\"] # 停用词列表。 （可选）\n",
    "excluded_docs = '' # str | 应从分析中排除的文档ID列表。 例如，[“docid1”，“docid2”，...，“docidN”]（可选）\n",
    "syntax_variables = False # bool | 如果为True，则分类器将在内容变量之上包含与语法相关的变量（可选）（默认为False）\n",
    "threshold = 0 # float | Threshold value for a document exposure to the contrastic topic, above which the document is assigned to class 1 specified through metadata_selection. （可选）（默认为0）\n",
    "remove_redundancies = False # bool | 如果为True，则此选项会从分析中删除准重复项。 准复制将具有相同的NLP表示，但不一定是完全相同的文本。 （可选）（默认为False）\n",
    "\n",
    "\n",
    "payload = nucleus_api.DocClassifyModel(dataset=\"Sellside_research_unlabeled\",\n",
    "                                        fixed_topics=fixed_topics,\n",
    "                                        metadata_selection_contrast=metadata_selection_contrast,\n",
    "                                        custom_stop_words=custom_stop_words,\n",
    "                                        validation_phase=False)\n",
    "api_response = api_instance.post_doc_classify_api(payload)\n",
    "\n",
    "print('Detailed Results')\n",
    "print('    Docids:', api_response.result.detailed_results.docids)\n",
    "print('    Estimated Category:', api_response.result.detailed_results.estimated_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 生成情感词典\n",
    "\n",
    "-     使用整个数据集：训练/验证/测试数据+上一步中标记的数据\n",
    "-     生成最能与上述任何两个标签形成对比的主题\n",
    "-     您可以对任何一对情绪标签重复此过程，交叉验证每个单词的情绪标签"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('---- Complete dataset ----')\n",
    "folder = 'Sellside_research_combined'         \n",
    "dataset = 'Sellside_research_combined'# str | 将插入文件的目标数据集。\n",
    "\n",
    "# 以递归方式从文件夹构建文件。\n",
    "# 每个文件采用以下格式：\n",
    "# {'filename': filename,   # 要上传的文件名。 必填\n",
    "#  'metadata': {           # 该文件的元数据。 可选\n",
    "#      'key1': val1,       # 只要名称，密钥就可以有任意名称\n",
    "#      'key2': val2        # 包含字母数字（0-9 | a-z | A-Z）和下划线（_）\n",
    "#   } \n",
    "# }\n",
    "\n",
    "# 如果文档不是CSV或JSON，则必须在file_iter对象中指定情感标签作为额外的元数据字段。\n",
    "\n",
    "# 如果您正在读取已提供情绪标签的文件，则无需传递file_dict中的“元数据”\n",
    "\n",
    "file_iter = []\n",
    "for root, dirs, files in os.walk(folder):\n",
    "    for file in files:\n",
    "        file_dict = {'filename': os.path.join(root, file),\n",
    "                     'metadata': {'sentiment': 'positive' # 这里传递上一步中获得的标签\n",
    "                                }}\n",
    "        file_iter.append(file_dict)\n",
    "\n",
    "file_props = nucleus_helper.upload_files(api_instance, dataset, file_iter, processes=4)\n",
    "for fp in file_props:\n",
    "    print(fp.filename, '(', fp.size, 'bytes) has been added to dataset', dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_selection_contrast = {\"sentiment\": [\"positive\", \"negative\"]} # dict | 元数据选择定义了两类文档，以便相互对比和总结\n",
    "\n",
    "query = '' # str | Dataset-language-specific全文查询，使用mysql MATCH boolean query格式（可选）\n",
    "custom_stop_words = [\"\"] # 停用词列表。 （可选）\n",
    "excluded_docs = '' # str | 应从分析中排除的文档ID列表。 例如，[“docid1”，“docid2”，...，“docidN”]（可选）\n",
    "syntax_variables = False # bool | 指定是否考虑每类文档的语法方面以帮助对比它们（可选）（默认为False）\n",
    "num_keywords = 20 # integer | 从数据集中提取的比较主题的关键字数。 (可选) (默认为50)\n",
    "remove_redundancies = False # bool | 如果为True，则此选项会从分析中删除准重复项。 准复制将具有相同的NLP表示，但不一定是完全相同的文本。（可选）（默认为False）\n",
    "\n",
    "payload = nucleus_api.TopicContrastModel(dataset='Sellside_research_combined', \n",
    "                                        metadata_selection_contrast=metadata_selection_contrast,\n",
    "                                        custom_stop_words=custom_stop_words)\n",
    "api_response = api_instance.post_topic_contrast_api(payload)\n",
    "\n",
    "print('Contrasted Topic')\n",
    "print('    Keywords:', api_response.result.keywords)\n",
    "print('    Keywords Weight:', api_response.result.keywords_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.\t微调\n",
    "\n",
    "#### a.\t从对比度分析中排除某些内容\n",
    "\n",
    "-   通过使用对比度分析API中的`custom_stop_words`参数，排除无关的关键字/主题以定制对比度分析\n",
    "\n",
    "\n",
    "-\t在语料库中的文档中提取对比主题的关键字，并打印这些主题的关键字"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('------------- Get list of topics from dataset --------------')\n",
    "\n",
    "payload = nucleus_api.Topics(dataset='Sellside_research',                         \n",
    "                            query='',                       \n",
    "                            num_topics=8, \n",
    "                            num_keywords=8,\n",
    "                            metadata_selection=metadata_selection_contrast)\n",
    "api_response = api_instance.post_topic_api(payload)        \n",
    "    \n",
    "for i, res in enumerate(api_response.result.topics):\n",
    "    print('Topic', i, ' keywords: ', res.keywords)    \n",
    "    print('---------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用您的专业知识，可以确定特定主题或关键字是否没有足够的差异来促进对比度分析。\n",
    "\n",
    "然后，您可以通过创建包含这些单词的`custom_stop_words`变量来定制对比度分析。 如下所示，初始化变量并将其传递到第2节主代码的有效负载（payload)中："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_stop_words = [\"disclaimer\", \"disclosure\"] # str | 停用词列表。 （可选的）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. 为您的对比主题指定`metadata_selection_contrast`\n",
    "\n",
    "-     对比两个不同实体的文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_selection_contrast = {\"research_analyst\": [\"MS\", \"JPM\"]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-     对比包含不同关键字的文档"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_selection_contrast = {\"content\": \"fundamentals\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c. 微调对比主题\n",
    "**num_keywords**: “num_keywords”越大，在对比主题中保留的单词就越多，而附加的单词对分离你所处理的两类情感的影响就越小\n",
    "\n",
    "**syntax_variables**: 如果为True，则某些词性功能会自动包含在对比主题模型中。 如果某些作者的写作风格差异很大，这可能会有所帮助。 社交媒体数据和新闻频繁发生这种情况。 它不太可能出现在机构出版物中\n",
    "\n",
    "**threshold**: 这是文档必须具有的最小曝光，以分配给您定义的category_1的对比主题。 完美模型的阈值为0（默认值）。 您可能会发现，通过选择不同的值，可以在验证中获得更高的性能指标。 这可以在用于训练和验证的较小样本中特别解释，或者如果在对比主题中存在作为关键词出现的通用词"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "版权(c) 2019年 SumUp Analytics 公司 版权所有。 \n",
    "\n",
    "通知：所有信息均属于SumUp Analytics Inc公司及其供应商的财产。 本合同所包含的知识产权和技术概念属于SumUp Analytics Inc.及其供应商的专利，可由美国和外国专利、在工艺中的专利以及受贸易秘密或版权法保护的专利涵盖。 \n",
    "\n",
    "除非得到SumUp Analytics公司的事先书面批准，否则严禁传播此类信息或复制此材料。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
