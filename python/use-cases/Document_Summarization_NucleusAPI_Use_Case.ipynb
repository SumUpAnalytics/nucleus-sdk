{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>  Document Summarization - Nucleus APIs Use Cases</center></h1>\n",
    "\n",
    "\n",
    "<h1><center>  SumUp Analytics, Proprietary & Confidential</center></h1>\n",
    "\n",
    "\n",
    "<h1><center>  Disclaimers and Terms of Service available at www.sumup.ai</center></h1>\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "## Objective: \n",
    "-\tDevelop a pipeline to customize and fine tune document summaries\n",
    "\n",
    "\n",
    "## Data:\n",
    "-\tAny collection of documents, ideally from the same field, possibly with further refinement in terms of categorization such as document type\n",
    "\n",
    "    **The Nucleus Datafeed can be leveraged for all content from major Central Banks**\n",
    "\n",
    "\n",
    "## Nucleus APIs used:\n",
    "-\tDataset creation API\n",
    " - \t*api_instance.post_upload_file(file, dataset)*\n",
    " - \t*nucleus_helper.import_files(api_instance, dataset, file_iters, processes=1)*\n",
    "\n",
    "        nucleus_helper.import_files leverages api_instance.post_upload_file with parallel execution to speed-up the dataset creation\n",
    "\n",
    "\n",
    "-\tTopic Modeling API\n",
    " - \t*api_instance.post_topic_api(payload)*\n",
    "\n",
    "\n",
    "-\tDocument Summary API\n",
    " - \t*api_instance.post_doc_summary_api(payload)*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach:\n",
    "\n",
    "### 1.\tDataset Preparation\n",
    "-\tCreate a Nucleus dataset containing all relevant documents\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('---- Case 1: you are using your own corpus, coming from a local folder ----')\n",
    "folder = 'Corporate_documents'         \n",
    "dataset = 'Corporate_docs'# str | Destination dataset where the file will be inserted.\n",
    "\n",
    "# build file iterable from a folder recursively. \n",
    "# Each item in the iterable is in the format below:\n",
    "# {'filename': filename,   # filename to be uploaded. REQUIRED\n",
    "#  'metadata': {           # metadata for the file. Optional\n",
    "#      'key1': val1,       # keys can have arbiturary names as long as the names only\n",
    "#      'key2': val2        # contain alphanumeric (0-9|a-z|A-Z) and underscore (_)\n",
    "#   } \n",
    "# }\n",
    "file_iter = []\n",
    "for root, dirs, files in os.walk(folder):\n",
    "    for file in files:\n",
    "        if Path(file).suffix == '.pdf': # .txt .doc .docx .rtf .html .csv also supported\n",
    "            file_dict = {'filename': os.path.join(root, file),\n",
    "                         'metadata': {'company': 'Apple',\n",
    "                                      'category': 'Press Release',\n",
    "                                      'date': '2019-01-01'}}\n",
    "            file_iter.append(file_dict)\n",
    "\n",
    "file_props = nucleus_helper.upload_files(api_instance, dataset, file_iter, processes=4)\n",
    "for fp in file_props:\n",
    "    print(fp.filename, '(', fp.size, 'bytes) has been added to dataset', dataset)\n",
    "\n",
    "    \n",
    "    \n",
    "print('---- Case 2: you are using an embedded datafeed ----')\n",
    "dataset = 'sumup/central_banks_chinese'# embedded datafeeds in Nucleus.\n",
    "metadata_selection = {'bank': 'people_bank_of_china', 'document_category': ('speech', 'press release', 'publication')}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.\tDocument Summarization\n",
    "-\tMost input parameters to document summarization are controlling the size of the summary and the contribution of sentences considered too short or too long (so affecting UX)\n",
    "\n",
    "\n",
    "-\tThe one parameter that allows to fine tune what type of content is considered relevant for summarization is the list of custom stop words \n",
    "\n",
    "\n",
    "-\tFurther down, we discuss how to construct a customized stopwords list to refine document summaries\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('---------------- Get doc summaries ------------------------')\n",
    "# These are all possible input arguments to the summarization API\n",
    "custom_stop_words = [\"decree\",\"motion\"] # List of stop words. (optional)\n",
    "summary_length = 6 # int | The maximum number of bullet points a user wants to see in the document summary. (optional) (default to 6)\n",
    "context_amount = 0 # int | The number of sentences surrounding key summary sentences in the documents that they come from. (optional) (default to 0)\n",
    "short_sentence_length = 0 # int | The sentence length below which a sentence is excluded from summarization (optional) (default to 4)\n",
    "long_sentence_length = 40 # int | The sentence length beyond which a sentence is excluded from summarization (optional) (default to 40)\n",
    "\n",
    "payload = nucleus_api.DocumentSummaryModel(dataset='Corporate_docs', \n",
    "                                        doc_title='my_title', \n",
    "                                        summary_length=summary_length)\n",
    "api_response = api_instance.post_doc_summary_api(payload)\n",
    "\n",
    "print('Summary for', api_response.result.doc_title)\n",
    "for sent in api_response.result.summary.sentences:\n",
    "    print('    *', sent)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.\tFine Tuning\n",
    "\n",
    "#### a.\tExtracting topics found across documents of your corpus\n",
    "-\tSee whether some tailoring may be applied to your summaries by excluding certain topics considered not information-bearing for your end-user or your application. This is achieved by using the custom_stop_words parameter in input to the Doc Summary API\n",
    "\n",
    "\n",
    "-\tIdentify and Extract key topics on subset of documents within your corpus, such that this subset is comprised of documents sharing some similar attributes (could be the type of doc, for instance) and print the keywords of these topics\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('------------- Get list of topics from dataset --------------')\n",
    "\n",
    "payload = nucleus_api.Topics(dataset='Corporate_docs',                         \n",
    "                            query='',                       \n",
    "                            num_topics=8, \n",
    "                            num_keywords=8,\n",
    "                            metadata_selection=metadata_selection)\n",
    "api_response = api_instance.post_topic_api(payload)        \n",
    "    \n",
    "for i, res in enumerate(api_response.result.topics):\n",
    "    print('Topic', i, ' keywords: ', res.keywords)    \n",
    "    print('---------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using your domain expertise / client input / advisor input, you can determine whether certain of those topics or keywords are not differentiated enough to contribute to document summaries. \n",
    "\n",
    "You can then tailor the document summaries by creating a custom_stop_words variable that contains those words. Initialize the variable as follows, for instance, and pass it in the payload of the main code of section 2: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_stop_words = [\"decree\",\"motion\"] # str | List of stop words. (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b.\tIsolating specific subsets of documents within your corpus\n",
    "**Document types**: You can refine the extraction of topics and isolation of non-information-bearing topics by leveraging the metadata selector provided during the construction of the dataset, to get any level of granularity you are interested in. \n",
    "\n",
    "Rerun the code from two blocks above on a subset of the whole corpus. Create a variable metadata_selection and pass it in to the payload:\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you created a dataset where one metadata is the category of the document, \n",
    "# and one possible value for this category is 'speech'\n",
    "# you could focus the topic analysis and the creation of a customized stopword list for all speech documents \n",
    "# within your corpus and later on in production\n",
    "metadata_selection = {\"document_category\": \"speech\"}   # str | json object of {\\\"metadata_field\\\":[\\\"selected_values\\\"]} (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c. Creating custom stopword lists on certain themes within your corpus\n",
    "**query**: You can refine the extraction of topics and isolation of non-information-bearing topics by leveraging the query variable of the Topic API.\n",
    "\n",
    "Rerun the code from 3 blocks above on the content from your corpus that mentions a specific theme. Create a variable query and pass it in to the payload:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '(veto rights OR jury decision OR verdict)' # str | Fulltext query, using mysql MATCH boolean query format. Example: \"(word1 OR word2) AND (word3 OR word4)\" (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) 2018 SumUp Analytics, Inc. All Rights Reserved.\n",
    "\n",
    "NOTICE: All information contained herein is, and remains the property of SumUp Analytics Inc. and its suppliers, if any. The intellectual and technical concepts contained herein are proprietary to SumUp Analytics Inc. and its suppliers and may be covered by U.S. and Foreign Patents, patents in process, and are protected by trade secret or copyright law.\n",
    "\n",
    "Dissemination of this information or reproduction of this material is strictly forbidden unless prior written permission is obtained from SumUp Analytics Inc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
