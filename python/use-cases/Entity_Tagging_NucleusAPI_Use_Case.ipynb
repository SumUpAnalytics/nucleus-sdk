{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>  Entity Tagging - Nucleus APIs Use Cases</center></h1>\n",
    "\n",
    "\n",
    "<h1><center>  SumUp Analytics, Proprietary & Confidential</center></h1>\n",
    "\n",
    "\n",
    "<h1><center>  Disclaimers and Terms of Service available at www.sumup.ai</center></h1>\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "## Objective: \n",
    "-\tGenerate a metadata with entities found in each document of a given dataset, given a list of a-prioris\n",
    "\n",
    "\n",
    "## Data:\n",
    "-\tAny collection of documents\n",
    "\n",
    "-   A list of entities that you are interested in detecting and tagging in the dataset\n",
    "\n",
    "\n",
    "## Nucleus APIs used:\n",
    "-\tDataset creation API\n",
    " - \t*api_instance.post_upload_file(file, dataset)*\n",
    " - \t*nucleus_helper.import_files(api_instance, dataset, file_iters, processes=1)*\n",
    "\n",
    "        nucleus_helper.import_files leverages api_instance.post_upload_file with parallel execution to speed-up the dataset creation\n",
    "\n",
    "\n",
    "-\tDataset Tagging API\n",
    " - \t*api_instance.post_dataset_tagging(payload)*\n",
    "\n",
    "\n",
    "-\tDocument Info API\n",
    " - \t*api_instance.post_doc_info(payload)*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach:\n",
    "\n",
    "### 1.\tDataset Preparation\n",
    "-\tCreate a Nucleus dataset containing all relevant documents\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('---- Upload documents from a local folder into a new Nucleus dataset ----')\n",
    "folder = 'Corporate_documents'         \n",
    "dataset = 'Corporate_docs'# str | Destination dataset where the file will be inserted.\n",
    "\n",
    "# build file iterable from a folder recursively. \n",
    "# Each item in the iterable is in the format below:\n",
    "# {'filename': filename,   # filename to be uploaded. REQUIRED\n",
    "#  'metadata': {           # metadata for the file. Optional\n",
    "#      'key1': val1,       # keys can have arbiturary names as long as the names only\n",
    "#      'key2': val2        # contain alphanumeric (0-9|a-z|A-Z) and underscore (_)\n",
    "#   } \n",
    "# }\n",
    "file_iter = []\n",
    "for root, dirs, files in os.walk(folder):\n",
    "    for file in files:\n",
    "        #if Path(file).suffix == '.pdf': # .txt .doc .docx .rtf .html .csv also supported\n",
    "        file_dict = {'filename': os.path.join(root, file),\n",
    "                     'metadata': {'category': 'News'}} # You don't have the tickers from each news, let's tag them\n",
    "        file_iter.append(file_dict)\n",
    "\n",
    "file_props = nucleus_helper.upload_files(api_instance, dataset, file_iter, processes=4)\n",
    "for fp in file_props:\n",
    "    print(fp.filename, '(', fp.size, 'bytes) has been added to dataset', dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.\tDataset Tagging\n",
    "-\tDefine a list of company tickers (or any other entity relevant to you)\n",
    "\n",
    "\n",
    "-\tLoop through this list and use the Dataset Tagging API to determine which documents contain a given ticker\n",
    "\n",
    "\n",
    "-\tFurther down, we discuss how to construct a customized stopwords list to refine document summaries\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('---------------- Tag dataset ------------------------')\n",
    "\n",
    "payload = nucleus_api.DatasetTaggingModel(dataset='Corporate_docs', \n",
    "                                    query='AAPL OR Apple', \n",
    "                                    metadata_selection='', \n",
    "                                    time_period='')\n",
    "api_response = api_instance.post_dataset_tagging(payload)\n",
    "\n",
    "print('Information about dataset', dataset)\n",
    "print('    Entity Tagged:', api_response.result.entity_tagged)\n",
    "print('    Docids tagged with Entity:', api_response.result.docids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create our list of entities and loop through it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities = [['AAPL', 'Apple'], ['GOOG', 'Google', 'Alphabet']]\n",
    "\n",
    "docs_tagged = []\n",
    "entities_tagged = []\n",
    "for i in range(len(entities)):\n",
    "    query = \" OR \".join(entities[i])\n",
    "    payload = nucleus_api.DatasetTaggingModel(dataset='Corporate_docs', \n",
    "                                    query=query, \n",
    "                                    metadata_selection='', \n",
    "                                    time_period='')\n",
    "    api_response = api_instance.post_dataset_tagging(payload)\n",
    "\n",
    "    for docid in api_response['docids']::\n",
    "        docs_tagged.append(docid)\n",
    "        entities_tagged.append(api_response['entity_tagged'][0]) # Retain the first naming of an entity as label\n",
    "\n",
    "# Let's regroup the entities that are tagged per document so we have a unique list of docids\n",
    "# and all entities tagged in them\n",
    "\n",
    "# This table will be useful to generate an updated dataset with tickers provided as metadata\n",
    "# so what we really care about are filenames rather than docids  \n",
    "from collections import defaultdict\n",
    "d = defaultdict(list)\n",
    "for i, entity in enumerate(entities_tagged):\n",
    "    payload = nucleus_api.DocInfo(\n",
    "        dataset='Corporate_docs', \n",
    "        doc_ids=docs_tagged[i],\n",
    "        metadata_selection='')\n",
    "    api_response = api_instance.post_doc_info(payload)\n",
    "    key = api_response.result[0].attribute['filename']\n",
    "    d[key].append(entity)\n",
    "d = dict(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using these tags, we can construct a second dataset enriched with this extra metadata, which will be very convenient notably in signal research and compliance analytics.\n",
    "\n",
    "We can use the filename to match raw documents with documents that have been tagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'Corporate_docs_2'# str | Destination dataset where the file will be inserted.\n",
    "\n",
    "file_iter = []\n",
    "for root, dirs, files in os.walk(folder):\n",
    "    for file in files:\n",
    "        #if Path(file).suffix == '.pdf': # .txt .doc .docx .rtf .html .csv also supported\n",
    "        \n",
    "        # We know the filename of the file currently being injected, we can match it against the \n",
    "        # table of tagged documents\n",
    "        if d[os.path.join(root, file)] != [] # Only build the new dataset with the documents that have tagged entities\n",
    "            tickers = d[os.path.join(root, file)]\n",
    "            file_dict = {'filename': os.path.join(root, file),\n",
    "                         'metadata': {'companies': tickers,\n",
    "                                      'category': 'News'}}\n",
    "            \n",
    "            file_iter.append(file_dict)\n",
    "\n",
    "file_props = nucleus_helper.upload_files(api_instance, dataset, file_iter, processes=4)\n",
    "for fp in file_props:\n",
    "    print(fp.filename, '(', fp.size, 'bytes) has been added to dataset', dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.\tFine Tuning\n",
    "\n",
    "#### a.\tExpanding the list of synonyms for a given entity\n",
    "**query**: You can refine the dataset tagging and expand your list of tickers (or other entity of relevance) to contain as many alternatives as you want. \n",
    "\n",
    "You can also create a conservative superset list of tickers once, keep that list saved and reuse it for every of the datasets you want to tag. \n",
    "\n",
    "Finally, you can also do the same with foreign companies. For instance, you could define an entry of your list as ['Nintendo', 'NTDOY', '任天堂株式会社']\n",
    "\n",
    "Pass that expanded list, looping through all distinct tickers, to the query argument in the main code of section 2. and rerun that code:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities = [['AAPL', 'Apple', 'iPhone'], ['GOOG', 'Google', 'Alphabet', 'Android'], ['NTDOY', 'Nintendo', '任天堂株式会社']]\n",
    "\n",
    "docs_tagged = []\n",
    "entities_tagged = []\n",
    "for i in range(len(entities)):\n",
    "    query = \" OR \".join(entities[i])\n",
    "    payload = nucleus_api.DatasetTaggingModel(dataset='Corporate_docs', \n",
    "                                    query=query, \n",
    "                                    metadata_selection='', \n",
    "                                    time_period='')\n",
    "    api_response = api_instance.post_dataset_tagging(payload)\n",
    "\n",
    "    for docid in api_response['docids']:\n",
    "        docs_tagged.append(docid)\n",
    "        entities_tagged.append(api_response['entity_tagged'][0]) # Retain the first naming of an entity as label\n",
    "\n",
    "# Let's regroup the entities that are tagged per document so we have a unique list of docids\n",
    "# and all entities tagged in them\n",
    "\n",
    "# This table will be useful to generate an updated dataset with tickers provided as metadata\n",
    "# so what we really care about are filenames rather than docids  \n",
    "d = defaultdict(list)\n",
    "for i, entity in enumerate(entities_tagged):\n",
    "    payload = nucleus_api.DocInfo(\n",
    "        dataset='Corporate_docs', \n",
    "        doc_ids=docs_tagged[i],\n",
    "        metadata_selection='')\n",
    "    api_response = api_instance.post_doc_info(payload)\n",
    "    key = api_response.result[0].attribute['filename']\n",
    "    d[key].append(entity)\n",
    "d = dict(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) 2019 SumUp Analytics, Inc. All Rights Reserved.\n",
    "\n",
    "NOTICE: All information contained herein is, and remains the property of SumUp Analytics Inc. and its suppliers, if any. The intellectual and technical concepts contained herein are proprietary to SumUp Analytics Inc. and its suppliers and may be covered by U.S. and Foreign Patents, patents in process, and are protected by trade secret or copyright law.\n",
    "\n",
    "Dissemination of this information or reproduction of this material is strictly forbidden unless prior written permission is obtained from SumUp Analytics Inc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
