{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>  Topics Transfer Learning - Nucleus APIs Use Cases</center></h1>\n",
    "\n",
    "\n",
    "<h1><center>  SumUp Analytics, Proprietary & Confidential</center></h1>\n",
    "\n",
    "\n",
    "<h1><center>  Disclaimers and Terms of Service available at www.sumup.ai</center></h1>\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "## Objective: \n",
    "-\tExtract topics on a reference dataset and measure their key metrics (strength, sentiment, consensus, exposures) in a validation dataset\n",
    "\n",
    "\n",
    "## Data:\n",
    "-\tAny two datasets, whether they are time ordered or chosen through another methodology\n",
    "\n",
    "    **The Nucleus Datafeed can be leveraged for all content from major Central Banks**\n",
    "\n",
    "\n",
    "## Nucleus APIs used:\n",
    "-\tDataset creation API\n",
    " - \t*api_instance.post_upload_file(file, dataset)*\n",
    " - \t*nucleus_helper.import_files(api_instance, dataset, file_iters, processes=1)*\n",
    "\n",
    "        nucleus_helper.import_files leverages api_instance.post_upload_file with parallel execution to speed-up the dataset creation\n",
    "\n",
    "\n",
    "-\tTopic Modeling API\n",
    " - \t*api_instance.post_topic_api(payload)*\n",
    "\n",
    "\n",
    "-\tTopic  Transfer API\n",
    " - \t*api_instance.post_topic_transfer_api(payload)*\n",
    "\n",
    "\n",
    "-\tTopic Sentiment Transfer API\n",
    " - \t*api_instance.post_topic_sentiment_transfer_api(payload)*\n",
    "\n",
    "\n",
    "-\tTopic Consensus Transfer API\n",
    " - \t*api_instance.post_topic_consensus_transfer_api(payload)*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach:\n",
    "\n",
    "### 1.\tDataset Preparation\n",
    "-\tCreate a Nucleus dataset containing all relevant documents over a chosen historical period\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leverage your own corpus\n",
    "print('---- Case 1: you are using your own corpus, coming from a local folder ----')\n",
    "folder = 'News_feed'         \n",
    "dataset = 'News_feed'# str | Destination dataset where the file will be inserted.\n",
    "\n",
    "# build file iterable from a folder recursively. \n",
    "# Each item in the iterable is in the format below:\n",
    "# {'filename': filename,   # filename to be uploaded. REQUIRED\n",
    "#  'metadata': {           # metadata for the file. Optional\n",
    "#      'key1': val1,       # keys can have arbiturary names as long as the names only\n",
    "#      'key2': val2        # contain alphanumeric (0-9|a-z|A-Z) and underscore (_)\n",
    "#   } \n",
    "# }\n",
    "file_iter = []\n",
    "for root, dirs, files in os.walk(folder):\n",
    "    for file in files:\n",
    "        #if Path(file).suffix == '.pdf': # .txt .doc .docx .rtf .html .csv also supported\n",
    "        file_dict = {'filename': os.path.join(root, file),\n",
    "                     'metadata': {'source': 'Tech Crunch',\n",
    "                                  'author': 'Sarah Moore'\n",
    "                                  'category': 'Media',\n",
    "                                  'date': '2019-01-01'}}\n",
    "        file_iter.append(file_dict)\n",
    "\n",
    "file_props = nucleus_helper.upload_files(api_instance, dataset, file_iter, processes=4)\n",
    "for fp in file_props:\n",
    "    print(fp.filename, '(', fp.size, 'bytes) has been added to dataset', dataset)\n",
    "\n",
    "    \n",
    "# Leverage a Nucleus embedded feed    \n",
    "print('---- Case 2: you are using an embedded datafeed ----')\n",
    "dataset = 'sumup/rss_feed_finance'# embedded datafeeds in Nucleus.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-\tFor a given date in that period, retain only a subset of the documents published during a chosen lookback period\n",
    "\n",
    "**This can be done directly into the APIs that perform content analysis, see below**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.\tTransfer Learning\n",
    "-\tIdentify and Extract key topics on a reference dataset \n",
    "\n",
    "\n",
    "-\tMeasure the strength, sentiment, consensus on each topic onto the validation dataset\n",
    "\n",
    "\n",
    "-   Measure the exposure and sentiment contribution of each document in the validation dataset to each topic\n",
    "\n",
    "\n",
    "-\tFurther down, we discuss how to refine the transfer learning by leveraging the different parameters available to the user\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('------------------- Get topic transfer -----------------------')\n",
    "\n",
    "payload = nucleus_api.TopicTransferModel(dataset0='News_feed', \n",
    "                                         dataset1=\"test_feed\",\n",
    "                                        query='', \n",
    "                                        custom_stop_words='', \n",
    "                                        num_topics=8, \n",
    "                                        num_keywords=8,\n",
    "                                        metadata_selection='')\n",
    "api_response = api_instance.post_topic_transfer_api(payload)\n",
    "\n",
    "doc_ids_t1 = api_response.result.doc_ids_t1\n",
    "topics = api_response.result.topics\n",
    "for i,res in enumerate(topics):\n",
    "    print('Topic', i, 'exposure within validation dataset:')\n",
    "    print('    Keywords:', res.keywords)\n",
    "    print('    Strength:', res.strength)\n",
    "    print('    Document IDs:', doc_ids_t1)\n",
    "    print('    Exposure per Doc in Validation Dataset:', res.doc_topic_exposures_t1)\n",
    "    print('---------------')\n",
    "    \n",
    "print('-------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-\tRepeat the above task for Topic Sentiment transfer, and Topic Consensus transfer, depending on what aspect of the analysis you would like to transfer from reference to validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('------------------- Get topic sentiment transfer -----------------------')\n",
    "\n",
    "payload = nucleus_api.TopicSentimentTransferModel(dataset0='News_feed', \n",
    "                                        query='', \n",
    "                                        custom_stop_words='', \n",
    "                                        num_topics=8, \n",
    "                                        num_keywords=8,\n",
    "                                        period_0_start='2018-08-12',\n",
    "                                        period_0_end='2018-08-15',\n",
    "                                        period_1_start='2018-08-16',\n",
    "                                        period_1_end='2018-08-19',\n",
    "                                        metadata_selection='')\n",
    "api_response = api_instance.post_topic_sentiment_transfer_api(payload)\n",
    "\n",
    "topics = api_response.result\n",
    "for i,res in enumerate(topics):\n",
    "    print('Topic', i, 'exposure within validation dataset:')\n",
    "    print('    Keywords:', res.keywords)\n",
    "    print('    Strength:', res.strength)\n",
    "    print('    Sentiment:', res.sentiment)\n",
    "    print('    Document IDs:', res.doc_ids_t1)\n",
    "    print('    Sentiment per Doc in Validation Dataset:', res.doc_sentiments_t1)\n",
    "    print('---------------')\n",
    "    \n",
    "print('-------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('------------------- Get topic consensus transfer -----------------------')\n",
    "\n",
    "payload = nucleus_api.TopicConsensusTransferModel(dataset0='News_feed', \n",
    "                                        query='', \n",
    "                                        custom_stop_words='', \n",
    "                                        num_topics=8, \n",
    "                                        num_keywords=8,\n",
    "                                        period_0_start='2018-08-12',\n",
    "                                        period_0_end='2018-08-15',\n",
    "                                        period_1_start='2018-08-16',\n",
    "                                        period_1_end='2018-08-19',\n",
    "                                        metadata_selection='')\n",
    "api_response = api_instance.post_topic_consensus_transfer_api(payload)\n",
    "\n",
    "topics = api_response.result\n",
    "for i,res in enumerate(topics):\n",
    "    print('Topic', i, 'exposure within validation dataset:')\n",
    "    print('    Keywords:', res.keywords)\n",
    "    print('    Consensus:', res.consensus)\n",
    "    print('---------------')\n",
    "    \n",
    "print('-------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.\tResults Interpretation\n",
    "-\tPossible comparison between metrics on the reference and the validation dataset, or use metrics on the validation dataset for production signal generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.\tFine Tuning\n",
    "\n",
    "### a.\tTailoring the topics\n",
    "-\tSee whether some tailoring may be applied to your transfer learning by excluding certain topics considered not impactful. This is achieved by using the custom_stop_words parameter in input to any of the Topic * Transfer APIs\n",
    "\n",
    "\n",
    "-\tIdentify and Extract key topics on the reference documents and print their keywords\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('------------- Get list of topics from dataset --------------')\n",
    "\n",
    "payload = nucleus_api.Topics(dataset='News_feed',                         \n",
    "                            query='',                       \n",
    "                            num_topics=8, \n",
    "                            num_keywords=8,\n",
    "                            metadata_selection=metadata_selection,\n",
    "                            period_start='2018-08-12',\n",
    "                            period_end='2018-08-15')\n",
    "api_response = api_instance.post_topic_api(payload)        \n",
    "    \n",
    "for i, res in enumerate(api_response.result.topics):\n",
    "    print('Topic', i, ' keywords: ', res.keywords)    \n",
    "    print('---------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can then tailor the transfer learning by creating a custom_stop_words variable. Initialize the variable as follows, for instance, and pass it in the payload of the main code of section 2: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_stop_words = [\"conference\",\"interview\"] # str | List of stop words. (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b.\tFocusing the transfer learning on certain subjects\n",
    "In case you decide to focus the transfer learning, for instance on policy and macro-economic subjects, simply substitute the query variable in the main code of section 2. with: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '(inflation OR growth OR unemployment OR stability OR regulation)' # str | Fulltext query, using mysql MATCH boolean query format. Example: \"(word1 OR word2) AND (word3 OR word4)\" (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c.\tAlternative specifications for the validation dataset\n",
    "**validation dataset**: Two approaches are possible. \n",
    "\n",
    "1) the reference and validation datasets are time ordered. In such case, simply append the documents belonging to the validation dataset to the reference dataset, and use time selectors to define which time period is reference, and which is validation\n",
    "\n",
    "2) the reference and validation datasets are not necessarily time ordered. In such case, you need to pass in two different datasets to the Topic Transfer APIs. dataset0 will be your reference dataset and dataset1 will be the validation dataset.\n",
    "\n",
    "Note that Topic Transfer may not lead to any result if the topics extracted from the reference dataset aren't present in the validation dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d. Specifying topics exogenously\n",
    "\n",
    "You can impose topics to be transferred to your validation dataset. These fixed topics can be chosen through whichever approach you decide. To pass them to any Transfer Learning API, use the fixed_topics optional input argument in the payload."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: in English and you decide of weights\n",
    "fixed_topics = [{\"keywords\":[\"inflation expectations\", \"forward rates\", \"board projections\"], \"weights\":[0.7, 0.2, 0.1]}]\n",
    "\n",
    "# Example 2: in English and you don't provide weights. Equal weights will then be used\n",
    "fixed_topics = [{\"keywords\":[\"inflation expectations\", \"forward rates\", \"board projections\"]}]\n",
    "\n",
    "# Example 3: in Chinese (if your dataset is in Chinese) and you don't provide weights\n",
    "fixed_topics = [{\"keywords\":[\"操作\", \"流动性\", \"基点\", \"元\", \"点\", \"央行\", \"进一步\", \"投资\"]},\n",
    "                {\"keywords\":[\"认为\", \"价格\", \"数据\", \"调查\", \"全国\", \"统计\", \"金融市场\", \"要求\"]}]\n",
    "\n",
    "\n",
    "payload = nucleus_api.TopicTransferModel(dataset0='News_feed', \n",
    "                                        dataset1=\"test_feed\",\n",
    "                                        fixed_topics=fixed_topics,\n",
    "                                        query='', \n",
    "                                        custom_stop_words='', \n",
    "                                        num_topics=8, \n",
    "                                        num_keywords=8,\n",
    "                                        metadata_selection='')\n",
    "api_response = api_instance.post_topic_transfer_api(payload)\n",
    "\n",
    "doc_ids_t1 = api_response.result.doc_ids_t1\n",
    "topics = api_response.result.topics\n",
    "for i,res in enumerate(topics):\n",
    "    print('Topic', i, 'exposure within validation dataset:')\n",
    "    print('    Keywords:', res.keywords)\n",
    "    print('    Strength:', res.strength)\n",
    "    print('    Document IDs:', doc_ids_t1)\n",
    "    print('    Exposure per Doc in Validation Dataset:', res.doc_topic_exposures_t1)\n",
    "    print('---------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) 2019 SumUp Analytics, Inc. All Rights Reserved.\n",
    "\n",
    "NOTICE: All information contained herein is, and remains the property of SumUp Analytics Inc. and its suppliers, if any. The intellectual and technical concepts contained herein are proprietary to SumUp Analytics Inc. and its suppliers and may be covered by U.S. and Foreign Patents, patents in process, and are protected by trade secret or copyright law.\n",
    "\n",
    "Dissemination of this information or reproduction of this material is strictly forbidden unless prior written permission is obtained from SumUp Analytics Inc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
