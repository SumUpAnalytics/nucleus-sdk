{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>  Contrast Analysis - Nucleus APIs Use Cases</center></h1>\n",
    "\n",
    "\n",
    "<h1><center>  SumUp Analytics, Proprietary & Confidential</center></h1>\n",
    "\n",
    "\n",
    "<h1><center>  Disclaimers and Terms of Service available at www.sumup.ai</center></h1>\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "## Objective: \n",
    "-\tDevelop a pipeline to customize and fine-tune contrast analysis of datasets\n",
    "  - Extraction of a contrasted topic\n",
    "  - Contrasted Summarization\n",
    "  - Classification of documents into 2 predefined categories\n",
    "\n",
    "**SumUp contrast analysis works on the premise of two distinct categories of documents within a corpus, defined by the user based on metadata or content**\n",
    "\n",
    "## Data:\n",
    "-\tAny collection of documents, ideally from the same field, possibly with further refinement in terms of categorization such as document type\n",
    "\n",
    "    **The Nucleus Datafeed can be leveraged for all content from major Central Banks and SEC filings**\n",
    "\n",
    "\n",
    "## Nucleus APIs:\n",
    "-\tDataset creation API\n",
    " - \t*api_instance.post_upload_file(file, dataset)*\n",
    " - \t*nucleus_helper.import_files(api_instance, dataset, file_iters, processes=1)*\n",
    "\n",
    "        nucleus_helper.import_files leverages api_instance.post_upload_file with parallel execution to speed-up the dataset creation\n",
    "\n",
    "\n",
    "-\tTopic Modeling API\n",
    " - \t*api_instance.post_topic_api(payload)*\n",
    "\n",
    "\n",
    "-\tContrasted Topic Modeling API\n",
    " - \t*api_instance.post_topic_contrast_api(payload)*\n",
    " \n",
    " \n",
    "-\tDocument Contrasted Summary API\n",
    " - \t*api_instance.post_document_contrast_summary_api(payload)*\n",
    "\n",
    "\n",
    "-\tDocuments Classification API\n",
    " - \t*api_instance.post_doc_classify_api(payload)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach:\n",
    "\n",
    "### 1.\tDataset Preparation\n",
    "-\tCreate a Nucleus dataset containing all relevant documents\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import nucleus_api.api.nucleus_api as nucleus_helper\n",
    "import nucleus_api\n",
    "from nucleus_api.rest import ApiException\n",
    "\n",
    "configuration = nucleus_api.Configuration()\n",
    "configuration.host = 'UPDATE-WITH-API-SERVER-HOSTNAME'\n",
    "configuration.api_key['x-api-key'] = 'UPDATE-WITH-API-KEY'\n",
    "\n",
    "# Create API instance\n",
    "api_instance = nucleus_api.NucleusApi(nucleus_api.ApiClient(configuration))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('---- Case 1: you are using your own corpus, coming from a local folder ----')\n",
    "folder = 'Sellside_research'         \n",
    "dataset = 'Sellside_research'# str | Destination dataset where the file will be inserted.\n",
    "\n",
    "# build file iterable from a folder recursively. \n",
    "# Each item in the iterable is in the format below:\n",
    "# {'filename': filename,   # filename to be uploaded. REQUIRED\n",
    "#  'metadata': {           # metadata for the file. Optional\n",
    "#      'key1': val1,       # keys can have arbiturary names as long as the names only\n",
    "#      'key2': val2        # contain alphanumeric (0-9|a-z|A-Z) and underscore (_)\n",
    "#   } \n",
    "# }\n",
    "file_iter = []\n",
    "for root, dirs, files in os.walk(folder):\n",
    "    for file in files:\n",
    "        #if Path(file).suffix == '.pdf': # .txt .doc .docx .rtf .html .csv also supported\n",
    "            file_dict = {'filename': os.path.join(root, file),\n",
    "                         'metadata': {'company': 'Apple',\n",
    "                                      'research_analyst': 'MS',\n",
    "                                      'date': '2019-01-01'}}\n",
    "            file_iter.append(file_dict)\n",
    "\n",
    "file_props = nucleus_helper.upload_files(api_instance, dataset, file_iter, processes=4)\n",
    "for fp in file_props:\n",
    "    print(fp.filename, '(', fp.size, 'bytes) has been added to dataset', dataset)\n",
    "\n",
    "    \n",
    "    \n",
    "print('---- Case 2: you are using an embedded datafeed ----')\n",
    "dataset = 'sumup/central_banks_chinese'# embedded datafeeds in Nucleus.\n",
    "metadata_selection = {'bank': 'people_bank_of_china', 'document_category': ('speech', 'press release')}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Contrasted Topic Modeling\n",
    "\n",
    "-     In this example, we define one category of documents as being produced by research analysts at Morgan Stanley. The second category of documents will be comprised of all other research reports.\n",
    "-     We extract one topic that separates those two categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_selection_contrast = {\"research_analyst\": \"MS\"} # dict | The metadata selection defining the two categories of documents to contrast and summarize against each other\n",
    "\n",
    "query = '' # str | Dataset-language-specific fulltext query, using mysql MATCH boolean query format (optional)\n",
    "custom_stop_words = [\"morgan stanley\"] # List of stop words. (optional)\n",
    "excluded_docs = '' # str | List of document IDs that should be excluded from the analysis. Example, [\"docid1\", \"docid2\", ..., \"docidN\"]  (optional)\n",
    "syntax_variables = False # bool | Specifies whether to take into account syntax aspects of each category of documents to help with contrasting them (optional) (default to False)\n",
    "num_keywords = 20 # integer | Number of keywords for the contrasted topic that is extracted from the dataset. (optional) (default to 50)\n",
    "remove_redundancies = False # bool | If True, this option removes quasi-duplicates from the analysis and reatins only one copy of it. A quasi-duplicate would have the same NLP representation, but not necessarily the exact same text. (optional) (default False)\n",
    "\n",
    "payload = nucleus_api.TopicContrastModel(dataset='Sellside_research', \n",
    "                                        metadata_selection_contrast=metadata_selection_contrast,\n",
    "                                        custom_stop_words=custom_stop_words,\n",
    "                                        period_start='2018-01-01',\n",
    "                                        period_end='2019-01-01')\n",
    "try:\n",
    "    api_response = api_instance.post_topic_contrast_api(payload)\n",
    "    api_ok = True\n",
    "except ApiException as e:\n",
    "    api_error = json.loads(e.body)\n",
    "    print('ERROR:', api_error['message'])\n",
    "    api_ok = False\n",
    "\n",
    "if api_ok:   \n",
    "    print('Contrasted Topic')\n",
    "    print('    Keywords:', api_response.result.keywords)\n",
    "    print('    Keywords Weight:', api_response.result.keywords_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.\tDocument Contrasted Summarization\n",
    "\n",
    "-   With the same dataset, we aim to find summary sentences that separate documents in two categories\n",
    "\n",
    "\n",
    "-\tUse the following input parameters to control the size of the summary and filter sentences that are too short or too lengthy.\n",
    "    - `summary_length`\n",
    "    - `context_amount` (the number of sentences around each key summary sentence)\n",
    "    - `short_sentence_length`\n",
    "    - `long_sentence_length`\n",
    "    \n",
    "\n",
    "-\tSet the following parameters to adjust or refine the focus and content of the contrasted summary\n",
    "    - `custom_stop_words` (list of custom stopwords)\n",
    "    - `syntax_variables` (including / excluding syntax variables)\n",
    "    - `num_keywords` (controlling the breadth of the contrasted summary)\n",
    "    - `remove_redundancies` (removing redundancies)\n",
    "\n",
    "\n",
    "-\tFurther down, we discuss how to construct a customized stopwords list to refine document contrasted summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('---------------- Get doc contrasted summaries ------------------------')\n",
    "metadata_selection_contrast = {\"research_analyst\": \"MS\"} # dict | The metadata selection defining the two categories of documents to contrast and summarize against each other\n",
    "\n",
    "query = '' # str | Dataset-language-specific fulltext query, using mysql MATCH boolean query format (optional)\n",
    "custom_stop_words = [\"morgan stanley\"] # List of stop words. (optional)\n",
    "summary_length = 6 # int | The maximum number of bullet points a user wants to see in the contrasted summary. (optional) (default to 6)\n",
    "context_amount = 0 # int | The number of sentences surrounding key summary sentences in the documents that they come from. (optional) (default to 0)\n",
    "short_sentence_length = 0 # int | The sentence length below which a sentence is excluded from summarization (optional) (default to 4)\n",
    "long_sentence_length = 40 # int | The sentence length beyond which a sentence is excluded from summarization (optional) (default to 40)\n",
    "excluded_docs = '' # str | List of document IDs that should be excluded from the analysis. Example, [\"docid1\", \"docid2\", ..., \"docidN\"]  (optional)\n",
    "syntax_variables = True # bool | Specifies whether to take into account syntax aspects of each category of documents to help with contrasting them (optional) (default to False)\n",
    "num_keywords = 20 # integer | Number of keywords for the contrasted topic that is extracted from the dataset and used in the summary. (optional) (default to 50)\n",
    "remove_redundancies = False # bool | If True, this option removes quasi-duplicates from the analysis and reatins only one copy of it. A quasi-duplicate would have the same NLP representation, but not necessarily the exact same text. (optional) (default False)\n",
    "\n",
    "payload = nucleus_api.DocumentContrastSummaryModel(dataset=\"Sellside_research\", \n",
    "                                                    metadata_selection_contrast=metadata_selection_contrast,\n",
    "                                                    custom_stop_words=custom_stop_words,\n",
    "                                                    period_start='2018-01-01',\n",
    "                                                    period_end='2019-01-01')\n",
    "try:\n",
    "    api_response = api_instance.post_document_contrast_summary_api(payload)\n",
    "    api_ok = True\n",
    "except ApiException as e:\n",
    "    api_error = json.loads(e.body)\n",
    "    print('ERROR:', api_error['message'])\n",
    "    api_ok = False\n",
    "\n",
    "if api_ok:   \n",
    "    print('Summary for', [x for x in  metadata_selection_contrast.values()])\n",
    "    for sent in api_response.result.class_1_content.sentences:\n",
    "        print('    *', sent)\n",
    "    print('======')\n",
    "    for sent in api_response.result.class_2_content.sentences:\n",
    "        print('    *', sent)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Documents Classification\n",
    "\n",
    "This task requires 3 steps:\n",
    "-     Extract a contrast topic on a labeled dataset\n",
    "-     Train the classifier with the contrast topic by providing a labeled dataset. In this step, you can adjust the weight of each keyword from the contrasted topic, remove certain keywords, and even compare the contrasted topic produced by step 1 against topics of your own choosing\n",
    "-     Test the classifier with test set\n",
    "\n",
    "-     In the example below, we assume that the contrasted topic has already been obtained. The structure of 'fixed_topics' is exactly that which would come out of the Contrasted Topic API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_topics = {\"keywords\": [\"price target\", \"projected revenue\", \"economy\"], \"weights\": [0.5, 0.25, 0.25]} # dict | The contrasting topic used to separate the two categories of documents. Weights optional\n",
    "metadata_selection_contrast = {\"research_analyst\": \"MS\"} # dict | The metadata selection defining the two categories of documents that a document can be classified into\n",
    "\n",
    "query = '' # str | Dataset-language-specific fulltext query, using mysql MATCH boolean query format (optional)\n",
    "custom_stop_words = [\"morgan stanley\"] # List of stop words. (optional)\n",
    "excluded_docs = '' # str | List of document IDs that should be excluded from the analysis. Example, [\"docid1\", \"docid2\", ..., \"docidN\"]  (optional)\n",
    "syntax_variables = True # bool | If True, the classifier will include syntax-related variables on top of content variables (optional) (default to False)\n",
    "threshold = 0 # float | Threshold value for a document exposure to the contrastic topic, above which the document is assigned to class 1 specified through metadata_selection. (optional) (default to 0)\n",
    "remove_redundancies = False # bool | If True, this option removes quasi-duplicates from the analysis and reatins only one copy of it. A quasi-duplicate would have the same NLP representation, but not necessarily the exact same text. (optional) (default False)\n",
    "\n",
    "payload = nucleus_api.DocClassifyModel(dataset=\"Sellside_research\",\n",
    "                                        fixed_topics=fixed_topics,\n",
    "                                        metadata_selection_contrast=metadata_selection_contrast,\n",
    "                                        custom_stop_words=custom_stop_words,\n",
    "                                        validation_phase=True,\n",
    "                                        period_start='2018-01-01',\n",
    "                                        period_end='2019-01-01')\n",
    "try:\n",
    "    api_response = api_instance.post_doc_classify_api(payload)\n",
    "    api_ok = True\n",
    "except ApiException as e:\n",
    "    api_error = json.loads(e.body)\n",
    "    print('ERROR:', api_error['message'])\n",
    "    api_ok = False\n",
    "\n",
    "if api_ok:   \n",
    "    print('Detailed Results')\n",
    "    print('    Docids:', api_response.result.detailed_results.docids)\n",
    "    print('    Exposure:', api_response.result.detailed_results.exposures)\n",
    "    print('    Estimated Category:', api_response.result.detailed_results.estimated_class)\n",
    "    print('    Actual Category:', api_response.result.detailed_results.true_class)\n",
    "    print('\\n')\n",
    "\n",
    "    print('Perf Metrics')\n",
    "    print('    Accuracy:', api_response.result.perf_metrics.hit_rate)\n",
    "    print('    Recall:', api_response.result.perf_metrics.recall)\n",
    "    print('    Precision:', api_response.result.perf_metrics.precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can move to the testing phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_topics = {\"keywords\": [\"price target\", \"projected revenue\", \"economy\"], \"weights\": [0.5, 0.25, 0.25]} # dict | The contrasting topic used to separate the two categories of documents\n",
    "metadata_selection_contrast = {\"research_analyst\": \"MS\"} # dict | The metadata selection defining the two categories of documents that a document can be classified into\n",
    "\n",
    "payload = nucleus_api.DocClassifyModel(dataset=\"Sellside_research\",\n",
    "                                        fixed_topics=fixed_topics,\n",
    "                                        metadata_selection_contrast=metadata_selection_contrast,\n",
    "                                        custom_stop_words=custom_stop_words,\n",
    "                                        validation_phase=False,\n",
    "                                        period_start='2019-01-02',\n",
    "                                        period_end='2019-06-01')\n",
    "try:\n",
    "    api_response = api_instance.post_doc_classify_api(payload)\n",
    "    api_ok = True\n",
    "except ApiException as e:\n",
    "    api_error = json.loads(e.body)\n",
    "    print('ERROR:', api_error['message'])\n",
    "    api_ok = False\n",
    "\n",
    "if api_ok:   \n",
    "    print('Detailed Results')\n",
    "    print('    Docids:', api_response.result.detailed_results.docids)\n",
    "    print('    Exposure:', api_response.result.detailed_results.exposures)\n",
    "    print('    Estimated Category:', api_response.result.detailed_results.estimated_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.\tFine-tuning\n",
    "\n",
    "#### a.\tExcluding certain content from the contrast analysis\n",
    "\n",
    "-   Exclude irrelevant keywords / topics to tailor your contrast analysis by using the `custom_stop_words` parameter in the Topic Contrast API\n",
    "\n",
    "\n",
    "-\tExtract keywords from topics within your corpus and print the keywords of these topics. You could do the same when extracting contrasting topics\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('------------- Get list of topics from dataset --------------')\n",
    "\n",
    "payload = nucleus_api.Topics(dataset='Sellside_research',                         \n",
    "                            query='',                       \n",
    "                            num_topics=8, \n",
    "                            num_keywords=8,\n",
    "                            metadata_selection=metadata_selection_contrast)\n",
    "try:\n",
    "    api_response = api_instance.post_topic_api(payload)        \n",
    "    api_ok = True\n",
    "except ApiException as e:\n",
    "    api_error = json.loads(e.body)\n",
    "    print('ERROR:', api_error['message'])\n",
    "    api_ok = False\n",
    "\n",
    "if api_ok:       \n",
    "    for i, res in enumerate(api_response.result.topics):\n",
    "        print('Topic', i, ' keywords: ', res.keywords)    \n",
    "        print('---------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using your domain expertise or client / advisor input, you can determine if specific topics or keywords are not differentiated enough to contribute to contrast analysis. \n",
    "\n",
    "You can then tailor the contrast analysis by creating a `custom_stop_words` variable that contains those words. As shown below, initialize the variable and pass it in the payload of the main code of section 2: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_stop_words = [\"disclaimer\",\"disclosure\"] # str | List of stop words. (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. Focusing the contrasted summary on specific subjects potentially discussed in your corpus\n",
    "**query**: You can refine the contrast analysis by leveraging the query variable of the Doc Contrasted Summary API.\n",
    "\n",
    "Rerun Contrast Analysis APIs with a specific query or queries. Create a variable query and pass it in to the payload:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '(earnings OR cash flows)' # str | Fulltext query, using mysql MATCH boolean query format. Example: \"(word1 OR word2) AND (word3 OR word4)\" (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c. Specifying the metadata_selection_contrast for your contrasted topic\n",
    "\n",
    "-     Contrasting documents from two different entities\n",
    "\n",
    "    on your own data, e.g. sell-side research: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_selection_contrast = {\"research_analyst\": [\"MS\", \"JPM\"]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    on SumUp data feed, e.g. Central Banks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_selection_contrast = {\"bank\": [\"federal_reserve\", \"ECB\"]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-     Contrasting different documents from a given entity\n",
    "\n",
    "    on SumUp data feed, e.g. Central Banks: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_selection_contrast = {\"document_category\": [\"speech\", \"press release\"]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-     Contrasting documents that contain different keywords\n",
    "\n",
    "    on your own data, or on SumUp data feed: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_selection_contrast = {\"content\": \"fundamentals\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) 2019 SumUp Analytics, Inc. All Rights Reserved.\n",
    "\n",
    "NOTICE: All information contained herein is, and remains the property of SumUp Analytics Inc. and its suppliers, if any. The intellectual and technical concepts contained herein are proprietary to SumUp Analytics Inc. and its suppliers and may be covered by U.S. and Foreign Patents, patents in process, and are protected by trade secret or copyright law.\n",
    "\n",
    "Dissemination of this information or reproduction of this material is strictly forbidden unless prior written permission is obtained from SumUp Analytics Inc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
