{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>  Nucleus APIs 应用案例 - 股票筛选</center></h1>\n",
    "\n",
    "\n",
    "<h1><center>  SumUp Analytics, Proprietary & Confidential</center></h1>\n",
    "<h1><center>  Disclaimers and Terms of Service available at www.sumup.ai</center></h1>\n",
    "\n",
    "\n",
    "#  \n",
    "\n",
    " \n",
    "\n",
    "\n",
    "## 目的: \n",
    "-\t利用上市公司有关披露对单名股票进行排名和筛选\n",
    "\n",
    "\n",
    "## 数据集:\n",
    "-\t选定的公司列表，例如在同一行业部门或具有类似市值的公司的\n",
    " - \t公司的投资者报告\n",
    " - \t新闻稿\n",
    " - \t财报会议记录\n",
    "\n",
    "\n",
    "\n",
    "## 使用的Nucleus API:\n",
    "-\tDataset creation API\n",
    " - \t*api_instance.post_upload_file(file, dataset)*\n",
    " - \t*nucleus_helper.import_files(api_instance, dataset, file_iters, processes=1)*\n",
    "\n",
    "        nucleus_helper.import_files leverages api_instance.post_upload_file with parallel execution to speed-up the dataset creation\n",
    "\n",
    "\n",
    "-\tTopic Modeling API\n",
    " - \t*api_instance.post_topic_api(payload)*\n",
    "\n",
    "\n",
    "-\tTopic Sentiment API\n",
    " - \t*api_instance.post_topic_sentiment_api(payload)*\n",
    "\n",
    "\n",
    "-\tDocInfo API\n",
    " - \t*api_instance.post_doc_info(payload)*\n",
    "\n",
    "\n",
    "-\tDatasetInfo API\n",
    " - \t*api_instance.post_dataset_info(payload)*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 具体步骤:\n",
    "\n",
    "### 1.\t数据准备\n",
    "-\t创建一个包含选定历史期间内所有相关文档的Nucleus数据集\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('--------- Append all files from local folder to dataset in parallel -----------')\n",
    "folder = 'Corporate_documents'         \n",
    "dataset = 'Corporate_docs'# str | Destination dataset where the file will be inserted.\n",
    "\n",
    "# build file iterable from a folder recursively. \n",
    "# Each item in the iterable is in the format below:\n",
    "# {'filename': filename,   # filename to be uploaded. REQUIRED\n",
    "#  'metadata': {           # metadata for the file. Optional\n",
    "#      'key1': val1,       # keys can have arbiturary names as long as the names only\n",
    "#      'key2': val2        # contain alphanumeric (0-9|a-z|A-Z) and underscore (_)\n",
    "#   } \n",
    "# }\n",
    "file_iter = []\n",
    "for root, dirs, files in os.walk(folder):\n",
    "    for file in files:\n",
    "        if Path(file).suffix == '.pdf': # .txt .doc .docx .rtf .html .csv also supported\n",
    "            file_dict = {'filename': os.path.join(root, file),\n",
    "                         'metadata': {'ticker': 'CHU',\n",
    "                                      'company': 'China Unicom',\n",
    "                                      'category': 'Report',\n",
    "                                      'date': '2019-01-01'}}\n",
    "            file_iter.append(file_dict)\n",
    "\n",
    "file_props = nucleus_helper.upload_files(api_instance, dataset, file_iter, processes=4)\n",
    "for fp in file_props:\n",
    "    print(fp.filename, '(', fp.size, 'bytes) has been added to dataset', dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-\t在历史期间中指定任意日期，只选取特定回望周期的文档子集\n",
    "\n",
    "**此步骤可以直接在执行内容分析的API中完成，如下**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.\t情感和主题贡献度：筛选分析\n",
    "- 在指定时间点，识别并提取当时文档子集中的关键主题\n",
    "\n",
    "\n",
    "- 量化每个主题的情感，将所有关键主题分为“积极”和“消极”主题\n",
    "\n",
    "\n",
    "- 确定每家公司对每个关键主题的关联程度\n",
    "\n",
    "\n",
    "- 使用上述计算出每个公司跨所有关键主题的综合情感贡献度，得出公司的排名\n",
    " - 排列最前的公司是与积极话题最相关，和/或与消极话题最不相关的公司\n",
    " \n",
    " \n",
    "- 接下来，我们将讨论如何利用用户可用的不同参数来改进此分析。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Determine which companies are associated to the documents contributing to the topics\n",
    "import numpy as np\n",
    "\n",
    "payload = nucleus_api.DocInfo(dataset='Corporate_docs')\n",
    "api_response = api_instance.post_doc_info(payload)\n",
    "\n",
    "company_sources = []\n",
    "for res in api_response.result:        \n",
    "    company_sources.append(res.attribute['ticker']) \n",
    "\n",
    "company_list = np.unique(company_sources)\n",
    "\n",
    "\n",
    "print('-------- Get topic sentiment and exposure per firm ----------------')\n",
    "\n",
    "payload = nucleus_api.TopicSentimentModel(dataset='Corporate_docs',          \n",
    "                                query='',                   \n",
    "                                num_topics=20,\n",
    "                                num_keywords=8,\n",
    "                                period_start=\"2018-11-01 00:00:00\",\n",
    "                                period_end=\"2019-01-01 00:00:00\")\n",
    "api_response = api_instance.post_topic_sentiment_api(payload)    \n",
    "\n",
    "company_rankings = np.zeros(len(company_list), len(enumerate(api_response.result))\n",
    "for i, res in enumerate(api_response.result):\n",
    "    print('Topic', i, 'sentiment:')\n",
    "    print('    Keywords:', res.topic)\n",
    "\n",
    "    # Aggregate all document exposures within a topic into a company exposure, using the dataset metadata\n",
    "    payload = nucleus_api.DocInfo(dataset='Corporate_docs', doc_ids = res.doc_id)\n",
    "    api_response1 = api_instance.post_doc_info(payload)\n",
    "\n",
    "    company_sources = [] # This list will be much shorter than the whole dataset because not all documents contribute to a given topic\n",
    "    for res1 in api_response1.result:        \n",
    "        company_sources.append(res1.attribute['ticker']) \n",
    "\n",
    "    company_contributions = np.zeros([len(company_list), 1])\n",
    "    for j in range(len(company_list)):\n",
    "        for k in range(len(company_sources)):\n",
    "            if company_sources[k] == company_list[j]:\n",
    "                company_contributions[j] += res.doc_score[k]\n",
    "\n",
    "    company_rankings[:, i] = res.strength * res.sentiment * company_contributions[:]   \n",
    "\n",
    "    print('---------------')\n",
    "\n",
    "\n",
    "# Add up the ranking of companies per topic into the final credit screen\n",
    "Corporate_screen = np.mean(company_rankings, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import numpy as np\n",
    "\n",
    "print('------------ Retrieve all companies found in the dataset ----------')\n",
    "\n",
    "payload = nucleus_api.DocInfo(dataset='Corporate_docs')\n",
    "api_response = api_instance.post_doc_info(payload)\n",
    "\n",
    "company_sources = []\n",
    "for res in api_response.result:        \n",
    "    company_sources.append(res.attribute['ticker']) \n",
    "\n",
    "company_list = np.unique(company_sources)\n",
    "\n",
    "\n",
    "print('--------------- Retrieve the time range of the dataset -------------')\n",
    "\n",
    "payload = nucleus_api.DatasetInfo(dataset='Corporate_docs', query='')\n",
    "api_response = api_instance.post_dataset_info(payload)\n",
    "\n",
    "first_date = datetime.datetime.fromtimestamp(float(api_response.result.time_range[0]))\n",
    "last_date = datetime.datetime.fromtimestamp(float(api_response.result.time_range[1]))\n",
    "delta = last_date – first_date\n",
    "\n",
    "# Now loop through time and at each date, compute the ranking of companies\n",
    "T = 90 # The look-back period in days\n",
    "\n",
    "Corporate_screen = []\n",
    "for i in range(delta.days):  \n",
    "    if i == 1:\n",
    "        end_date = first_date + datetime.timedelta(days=T)\n",
    " \n",
    "    # first and last date used for the lookback period of T days\n",
    "    start_date = end_date - datetime.timedelta(days=T)\n",
    "    start_date_str = start_date.strftime(\"%Y-%M-%d 00:00:00\")\n",
    "\n",
    "    # We want a daily indicator\n",
    "    end_date = end_date + datetime.timedelta(days=1) \n",
    "    end_date_str = end_date.strftime(\"%Y-%M-%d 00:00:00\")\n",
    "\n",
    "    payload = nucleus_api.TopicSentimentModel(dataset=\"Corporate_docs\",      \n",
    "                                query='',                   \n",
    "                                num_topics=20,\n",
    "                                num_keywords=8,\n",
    "                                period_start= start_date_str,\n",
    "                                period_end= end_date_str)\n",
    "    api_response = api_instance.post_topic_sentiment_api(payload)\n",
    "\n",
    "    company_rankings = np.zeros(len(company_list), len(enumerate(api_response.result))\n",
    "    for l, res in enumerate(api_response.result):\n",
    "        # Aggregate all document exposures within a topic into a company exposure, using the dataset metadata\n",
    "        payload = nucleus_api.DocInfo(dataset='Corporate_docs', doc_ids=res.doc_id)\n",
    "        api_response1 = api_instance.post_doc_info(payload)\n",
    "\n",
    "        company_sources = [] # This list will be much shorter than the whole dataset because not all documents contribute to a given topic\n",
    "        for res1 in api_response1.result:        \n",
    "            company_sources.append(res1.attribute['ticker']) \n",
    "\n",
    "        company_contributions = np.zeros([len(company_list), 1])\n",
    "        for j in range(len(company_list)):\n",
    "            for k in range(len(company_sources)):\n",
    "                if company_sources[k] == company_list[j]:\n",
    "                    company_contributions[j] += res.doc_score[k]\n",
    "\n",
    "        company_rankings[:, l] = res.strength * res.sentiment * company_contributions[:]       \n",
    "\n",
    "    # Add up the ranking of companies per topic into the final credit screen\n",
    "    Corporate_screen.append(np.mean(company_rankings, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-\t对历史期间中的每个日期重复上述任务，以获取单个名称屏幕的完整历史记录"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.\t得出结论\n",
    "-\t将公司排名的时间序列与风险调整后的股本回报进行比较"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.\t用户自定义调整\n",
    "\n",
    "### a.\t调整主题范围\n",
    "-\t通过排除某些被认为不具影响力的主题，来调整筛选机制。这可以通过在主题情感API的输入中用户自定义的_stop_words参数实现的。\n",
    "\n",
    "\n",
    "-\t识别和提取文档子集上的关键主题并print其关键字\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('------------- Get list of topics from dataset --------------')\n",
    "\n",
    "payload = nucleus_api.Topics(dataset='Corporate_docs',                       \n",
    "                                query='',                       \n",
    "                                num_topics=20, \n",
    "                                num_keywords=8,\n",
    "                                period_start=\"2018-11-01 00:00:00\",\n",
    "                                period_end=\"2019-01-01 00:00:00\")\n",
    "api_response = api_instance.post_topic_api(payload)        \n",
    "    \n",
    "for i, res in enumerate(api_response.result):\n",
    "    print('Topic', i, ' keywords: ', res.topic)    \n",
    "    print('---------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后，您可以通过创建自定义的stopwords变量，来调整筛选机制，例如"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_stop_words = [\"call\",\"report\"] # str | List of stop words. (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b.\t聚焦特定主题\n",
    "如果您决定将筛选的重点放在特定主题上，如“财务健康”和“企业行动”，只需替换第2节主代码中的查询变量到以下即可"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '(\"earnings\" OR \"debt\" OR \"competition\" OR \"lawsuit\" OR \"restructuring\")' # str | Fulltext query, using mysql MATCH boolean query format. Example, (\\\"word1\\\" OR \\\"word2\\\") AND (\\\"word3\\\" OR \\\"word4\\\") (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c.\t探索文档类型、回望周期、提取的主题数量的影响\n",
    "**num_topics**: 通过更改第2节主代码中的变量num_topics，您可以改变主题数目即宽广度。较大数值将在建立排名时提供更大的广度，较小的值将缩小主题的广度。如果num_主题太大，一些非常边缘的主题可能会在衡量公司排名时带来很多干扰。\n",
    "\n",
    "**T**: 通过更改第2节主代码中的变量t来改变回望周期。较大的t值将得出一个缓慢变化的排名，较小的值将导致一个迅速变化的排名。如果t太小，可能会使数据集太小，这可能会导致公司排名收到较大的干扰。如果t太长，排名就不会很快反映出足够重要的新信息\n",
    "\n",
    "**Document types**: 仅选用某一类文档（如公司报告、新闻稿、财报会议记录中的一种），对比得出公司排名的变化。数据集的元数据是已知的，只需创立一个元数据变量，填入到第2节主代码中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_selection = {\"category\": \"Reports\"}   # str | json object of {\\\"metadata_field\\\":[\\\"selected_values\\\"]} (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.\t后续\n",
    "-\t可能的扩展：针对不同行业重复上述任务\n",
    " - 这将为您提供一个更宽广灵活的筛选机制，让您可以在每个行业细分中排列和筛选股票\n",
    " - 如果您将跨行业的所有公司混合在一个数据集中，配合行业标记，可以进而在行业的纬度上进行排列和筛选\n",
    "\n",
    "\n",
    "-\t对公司排名的时间序列与风险调整后的股本回报进行相关性分析\n",
    " - 可以研究价格影响的不同时间范围：1天、7天、几周，甚至更长的持续影响\n",
    " - 可以研究价格影响的不同时间滞后：市场受到新信息影响开始调整前的2到3天，前一周，甚至更长的时间间隔。\n",
    " - 您也可以做跟股息收益率或市盈率相关性的分析。与股息收益率较低的公司相比，股息收益率较高的公司是否表现出不同的价格影响？\n",
    "\n",
    "\n",
    "\n",
    "-\t可能的扩展：对筛选指（排名）标进行简单的变换\n",
    " - 例如，定义以下score,缩放和平滑排名 \n",
    "\n",
    "        Score(Company i) = ( Rank(Company i) – Average(Ranks, [Companies]) ) / Std(Ranks, [Companies])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) 2019 SumUp Analytics, Inc. All Rights Reserved.\n",
    "\n",
    "NOTICE: All information contained herein is, and remains the property of SumUp Analytics Inc. and its suppliers, if any. The intellectual and technical concepts contained herein are proprietary to SumUp Analytics Inc. and its suppliers and may be covered by U.S. and Foreign Patents, patents in process, and are protected by trade secret or copyright law.\n",
    "\n",
    "Dissemination of this information or reproduction of this material is strictly forbidden unless prior written permission is obtained from SumUp Analytics Inc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
