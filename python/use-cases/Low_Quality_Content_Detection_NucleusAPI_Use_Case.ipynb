{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>  Low Quality Content Detection - Nucleus APIs Use Cases</center></h1>\n",
    "\n",
    "\n",
    "<h1><center>  SumUp Analytics, Proprietary & Confidential</center></h1>\n",
    "\n",
    "\n",
    "<h1><center>  Disclaimers and Terms of Service available at www.sumup.ai</center></h1>\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "## Objective: \n",
    "-\tDevelop a pipeline to detect low quality content in social-media or gaming chatroom\n",
    "\n",
    "**In its current version, SumUp contrast analysis works comparing two categories against each other, where the user defines what the two categories are.**\n",
    "\n",
    "## Data:\n",
    "-\tA labeled corpus of posts from a social media or gaming platform\n",
    " -     You can have multiple labels in your corpus, but the algorithms will deal with two labels at a time when learning / predicting\n",
    " \n",
    " \n",
    " -     Illustrative labels for low-quality content detection: **\"Violence\", \"Drugs\", \"Pornographic\", \"Religiously Sensitive\", \"Politically Sensitive\", \"Scam\", \"Clickbait\", \"Fake\", \"All clear\"**\n",
    "\n",
    "\n",
    "\n",
    "## Nucleus APIs used:\n",
    "-\tDataset creation API\n",
    " - \t*api_instance.post_upload_file(file, dataset)*\n",
    " - \t*nucleus_helper.import_files(api_instance, dataset, file_iters, processes=1)*\n",
    "\n",
    "        nucleus_helper.import_files leverages api_instance.post_upload_file with parallel execution to speed-up the dataset creation\n",
    "\n",
    "\n",
    "-\tTopic Modeling API\n",
    " - \t*api_instance.post_topic_api(payload)*\n",
    "\n",
    "\n",
    "-\tContrasted Topic Modeling API\n",
    " - \t*api_instance.post_topic_contrast_api(payload)*\n",
    "\n",
    "\n",
    "-\tDocuments Classification API\n",
    " - \t*api_instance.post_doc_classify_api(payload)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach:\n",
    "\n",
    "### 1.\tDataset Preparation\n",
    "-\tCreate a Nucleus dataset containing all relevant documents\n",
    "\n",
    "\n",
    "-   We assume that the data is stored in a csv file. A similar code could be built to inject from a database table. There are some requirements on the name of data and metadata fields passed to the API to create a dataset\n",
    "\n",
    "\n",
    "    - Illustrative template for the data uploaded: [\"author\", \"label\", \"time\", \"content\", \"title\"]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import nucleus_api.api.nucleus_api as nucleus_helper\n",
    "import nucleus_api\n",
    "from nucleus_api.rest import ApiException\n",
    "\n",
    "configuration = nucleus_api.Configuration()\n",
    "configuration.host = 'UPDATE-WITH-API-SERVER-HOSTNAME'\n",
    "configuration.api_key['x-api-key'] = 'UPDATE-WITH-API-KEY'\n",
    "\n",
    "# Create API instance\n",
    "api_instance = nucleus_api.NucleusApi(nucleus_api.ApiClient(configuration))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file = 'Social_media_feed.csv'\n",
    "dataset = 'Social_media_feed'# str | Destination dataset where the file will be inserted.\n",
    "\n",
    "with open(csv_file, encoding='utf-8-sig') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    json_props = nucleus_helper.upload_jsons(api_instance, dataset, reader, processes=4)\n",
    "    \n",
    "    total_size = 0\n",
    "    total_jsons = 0\n",
    "    for jp in json_props:\n",
    "        total_size += jp.size\n",
    "        total_jsons += 1\n",
    "        \n",
    "    print(total_jsons, 'JSON records (', total_size, 'bytes) appended to', dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Contrasted Topic Modeling\n",
    "\n",
    "-     In this example, we define one category of documents tagged \"Violence\". The second category of documents are tagged \"All clear\".\n",
    "-     We extract one topic that separates those two categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contrasted Topic\n",
      "    Keywords: ['trump foreign', 'social media', 'rasmussen_poll realdonaldtrump', 'claims heard', 'donald trump', 'trump campaign', 'omarosa legitimate', 'strzok fbi', 'fake dossier', 'judgejeanine bob', 'director brennan', 'bruce ohr', 'christopher steele', 'fbi criminally', 'unfortunate situation', 'hillary clinton', 'fox news', 'lou dobbs', 'mark levin', 'concerned comey', 'department justice', 'brennan stain', 'department believe', 'collusion obstruction', 'pushback governor', 'heard heard', 'frame donald', 'gregg jarrett', 'cuomo resign', 'nelly time', 'gps fake', 'foreign policy', 'media totally', 'realdonaldtrump approval', 'presidential lowlife', 'governor andrew', 'resign ratings', 'situation decided', 'criminally investigated', 'america standing', 'boosting america', 'policy boosting', 'administration happen', 'trump administration', 'loudly trump', 'speaking loudly', 'republicanconservative voices', 'discriminating republicanconservative', 'totally discriminating', 'black voters', 'ratings black', 'approval ratings', 'investigated conducted', 'decided frame', 'court judge', 'based intelligence', 'fisa court', 'cuomo pushing', 'peter strzok', 'hero vietnam', 'politician bragging', 'crying lowlife', 'powerful true', 'johnson minnesota', 'received documents', 'blumenthal connecticut', 'supporter kris', 'mistakes john', 'wisconsin tremendous', 'bryan steil', 'leaving office', 'charge crooked', 'corrupt fbi', 'fired times', 'signed nondisclosure', 'russian witch', 'agent strzok', 'nomination governor', 'governor highest', 'built tariffs', 'gain federal', 'united permission', 'doj legal', 'war hero', 'fusion gps', 'politician cuomo', 'agent peter', 'statement america', 'connecticut politician', 'jeff johnson', 'crazed crying', 'friend supporter', 'wonderful powerful', 'illegal aliens', 'ohr justice', 'dossier paid', 'trade deals', 'statute violation', 'witch hunt', 'great great', 'congratulations bryan', 'walker wisconsin', 'great win', 'omarosa fired', 'omarosa signed', 'looked mistakes', 'richard blumenthal', 'andrew cuomo', 'fired fbi', 'fbi doj', 'statement based', 'wife nelly', 'fbi received', 'campaign colluded', 'fired agent', 'country built', 'financial gain', 'senator richard', 'intelligence leaving', 'fbi charge', 'incredibly corrupt', 'believe governor', 'cia director', 'wacky omarosa', 'america great', 'scott walker', 'win night', 'rigged russian', 'crooked hillary', 'enter united', 'trump fisa', 'republican nomination', 'white house', 'kelly board', 'star hockey', 'boston globe', 'local politicians', 'voices destroyed', 'unpopular governor', 'markburnetttv called', 'speaking business', 'great fort', 'happy birthday', 'harleydavidson owners', 'queen soul', 'great honor', 'pete stauber', 'attend parade', 'govmikehuckabee paycheck', 'easily win', 'fools focused', 'lto dozens', 'importantly remains', 'steele shopped', 'terrorist attack', 'money pouring', 'great job', 'win great', 'brooks koepka', 'holding wonderful', 'fake news', 'strzok firing', 'ohr doj']\n",
      "    Keywords Weight: [-0.02580634579811585, -0.02580634579811585, -0.02580634579811585, -0.02311484246751239, -0.021895350337814663, -0.021376754593451477, -0.017170632304079305, -0.016235960830870113, -0.015597370427191035, -0.01504033247569923, -0.013648724561252512, -0.010790594497802406, -0.010202073178289781, -0.009570384967246152, -0.009131026341842152, -0.008221061617834194, -0.004813678122023803, -0.004432285402193095, -0.004429591204664869, -0.00422515680251147, -0.00391099546030184, -0.003910995460301695, -0.0030887030699093296, -0.0019112219434091506, -0.0017028135744619401, -0.001347098764066027, -0.0010076298755215575, -0.0009099647240079872, -0.0006217391280434355, -0.0003664958865435054, -0.00029143046399791585, -7.570115910396843e-16, -7.570115910396843e-16, -7.570115910396843e-16, -6.544326234799633e-16, -1.6133662688793467e-16, -1.0038791767465938e-16, -1.5133175537781301e-18, -7.5665877688906505e-19, -1.7596715741606163e-21, -1.7596715741606163e-21, -1.7596715741606163e-21, -1.7596715741606163e-21, -1.7596715741606163e-21, -1.7596715741606163e-21, -1.7596715741606163e-21, -1.7596715741606163e-21, -1.7596715741606163e-21, -1.7596715741606163e-21, -1.7596715741606163e-21, -1.7596715741606163e-21, -1.7596715741606163e-21, -8.798357870803082e-22, -8.798357870803082e-22, 1.7596715741606163e-21, 8.766344568670763e-19, 2.08433097959325e-18, 5.113502084418153e-18, 2.7332357500881265e-17, 3.4947387993107635e-17, 6.385558314369603e-17, 2.2294800771402035e-16, 2.230743935370821e-16, 2.4359330270281345e-16, 2.891639314992213e-16, 3.644745107952863e-16, 3.9211372396567333e-16, 4.2528487876980474e-16, 4.523280290719681e-16, 4.727820374096407e-16, 4.929764424350783e-16, 5.424513684141782e-16, 5.797659287149023e-16, 6.529194094362778e-16, 6.529194094362778e-16, 6.709106021407737e-16, 7.001193906002658e-16, 7.386875616324404e-16, 7.482758050198564e-16, 8.742244014504951e-16, 8.742244014504951e-16, 1.078135971544968e-15, 1.6315367928192682e-15, 1.3829883799137554e-06, 0.0002914304639979679, 0.00081345157095705, 0.0009516026603065579, 0.001081074446418354, 0.0017223263562826704, 0.0024355815734320185, 0.002435581573432488, 0.002446041399129011, 0.002446041399129943, 0.002614956424364147, 0.003670322196786408, 0.004820580314661461, 0.005229912848728294, 0.005229912848728294, 0.006224315308247418, 0.00649060655167899, 0.007665494422160565, 0.007665494422161187, 0.007681184160706736, 0.008585316152039715, 0.008585316152039715, 0.008590886955714402, 0.00861042361872912, 0.009041989159587793, 0.009118029940015066, 0.00951563949003872, 0.009567203866550916, 0.01056856906483332, 0.011981454713640685, 0.013294162009052849, 0.01444431844261874, 0.015341448670016689, 0.015341448670016689, 0.015456681578385393, 0.016223610675226197, 0.01623596083087023, 0.016276574074664298, 0.016748825382189275, 0.017199927586062497, 0.0172054983897375, 0.017412403813022302, 0.018125320119616227, 0.018125320119616876, 0.019566499233528923, 0.01957500530536672, 0.020576591431594914, 0.021895350337815405, 0.02335523296834515, 0.02335523296834515, 0.0257908145417772, 0.0257908145417772, 0.0257908145417772, 0.0257908145417772, 0.0257908145417772, 0.0257908145417772, 0.0257908145417772, 0.0257908145417772, 0.0257908145417772, 0.0257908145417772, 0.0257908145417772, 0.0257908145417772, 0.0257908145417772, 0.0257908145417772, 0.0257908145417772, 0.0257908145417772, 0.0257908145417772, 0.0257908145417772, 0.0257908145417772, 0.0257908145417772, 0.0257908145417772, 0.0257908145417772, 0.0257908145417772, 0.0257908145417772, 0.0257908145417772, 0.0257908145417772, 0.0257908145417772, 0.025797787758908838, 0.025804760976040477, 0.03659186886527521]\n",
      "In-Sample Perf Metrics\n",
      "    Accuracy: 1.0\n",
      "    Recall: 1.0\n",
      "    Precision: 1.0\n"
     ]
    }
   ],
   "source": [
    "metadata_selection = {\"label\": [\"Violence\", \"All clear\"]} # dict | The metadata selection defining the two categories of documents to contrast and summarize against each other\n",
    "\n",
    "query = '' # str | Dataset-language-specific fulltext query, using mysql MATCH boolean query format (optional)\n",
    "custom_stop_words = [\"\"] # List of stop words. (optional)\n",
    "excluded_docs = '' # str | List of document IDs that should be excluded from the analysis. Example, [\"docid1\", \"docid2\", ..., \"docidN\"]  (optional)\n",
    "syntax_variables = True # bool | Specifies whether to take into account syntax aspects of each category of documents to help with contrasting them (optional) (default to False)\n",
    "compression = 0.002 # float | Parameter controlling the breadth of the contrasted topic. Contained between 0 and 1, the smaller it is, the more contrasting terms will be captured, with decreasing weight. (optional) (default to 0.000002)\n",
    "remove_redundancies = True # bool | If True, this option removes quasi-duplicates from the analysis. A quasi-duplicate would have the same NLP representation, but not necessarily the exact same text. (optional) (default True)\n",
    "\n",
    "payload = nucleus_api.TopicContrastModel(dataset='Social_media_feed', \n",
    "                                        metadata_selection=metadata_selection,\n",
    "                                        period_start='2018-01-01',\n",
    "                                        period_end='2019-01-01')\n",
    "try:\n",
    "    api_response = api_instance.post_topic_contrast_api(payload)\n",
    "    api_ok = True\n",
    "except ApiException as e:\n",
    "    api_error = json.loads(e.body)\n",
    "    print('ERROR:', api_error['message'])\n",
    "    api_ok = False\n",
    "\n",
    "if api_ok:\n",
    "    print('Contrasted Topic')\n",
    "    print('    Keywords:', api_response.result.keywords)\n",
    "    print('    Keywords Weight:', api_response.result.keywords_weight)\n",
    "\n",
    "    print('In-Sample Perf Metrics')\n",
    "    print('    Accuracy:', api_response.result.perf_metrics.hit_rate)\n",
    "    print('    Recall:', api_response.result.perf_metrics.recall)\n",
    "    print('    Precision:', api_response.result.perf_metrics.precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Documents Classification\n",
    "\n",
    "This task requires 3 steps:\n",
    "-     First, extract a contrasted topic on a labeled dataset\n",
    "-     Second, train the documents' classifier by providing a labeled dataset. In this step, you can adjust the weight of each keyword from the contrasted topic, remove certain keywords, and even compare the contrasted topic produced by step 1 against topics of your own choosing\n",
    "-     Third, test the classifier\n",
    "\n",
    "-     In the example below, we assume that the contrasted topic has already been obtained. The structure of 'fixed_topics' is exactly that which would come out of the Contrasted Topic API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "163 163\n",
      "Detailed Results\n",
      "    Docids: [372746459070796601, 656244823936517128, 776902852041351634, 950604085993420810, 1292265014981711161, 1380411530707030282, 1620156333107313580, 1854520462215508183, 2205902445999073018, 2365960778917245307, 2373450842905457495, 2383865888350638791, 2554924790797026542, 2952292854093486503, 3325720912382988533, 3397215194896514820, 3499421997204683102, 3545423942726121399, 3683627708016583172, 4555868983588618437, 4625946039318940221, 4746121785136787662, 4767189974744133712, 4825367511331474696, 5217366909427623007, 5566900818722282521, 5620968974223273808, 5821020073909755150, 5864841412738683134, 6173618630202756293, 6303783743713708484, 6468365417517605478, 7014079786619530089, 7180359259391996839, 7242230233701612989, 7290029718334628379, 7887407208809957066, 7967605045913198983, 8047817457772465264, 8073561612845847316, 8192928964490616283, 8991483632660067955, 9035906359710233744, 9384092744660032334, 9785400758777816854, 10006474250568936611, 10010199882756041615, 10131848627958266905, 10370745183868017022, 10547779125865178270, 10553372167446177417, 10595041987461739196, 11198341698462345569, 11357478787751126246, 11485414282913601829, 11760987759040078706, 11767302015801488535, 11781010922933920259, 12936417737022695482, 13072902166717108911, 13077487413648394209, 13460407141547160473, 13630404863291543956, 14241248046650668697, 14355443980237980691, 14463102537742211332, 14507484009580738024, 14579590163033179898, 14722230792170818214, 14988735547592816033, 15030888264722461978, 15429790537822270921, 15752099456024216117, 15758769748652033371, 16084996898873488732, 16259624839192846495, 16442619160892914817, 16485451943305749356, 16828254283062838304, 16856013646116686449, 17263586507006051906, 17884878914091736049, 18423518517048905190]\n",
      "    Exposure: [0.6031041885497171, -0.8947239041537959, 0.5345323126558266, 1.0, 0.5775453435127519, 0.5771942097311291, 0.5162581956712301, 0.8946689572734262, 0.5554498816144061, -0.516784807367435, 0.5774282989188884, 0.5547751647641369, 0.6668919190925875, 0.9997296970888822, -0.43669039122010483, 0.6032264376409845, 0.6322845774622146, 0.6030168132279197, 0.5545502591474005, 0.7557246162237473, 0.5771942097311291, -0.6032857099276496, 0.578130566482095, 0.6032264376409845, -0.7074330509108693, 0.48494013390167834, 0.8941854247264057, -0.5346998038499883, 0.6031915638714984, 0.9997296970889078, -0.47156090235402454, 0.6664864647259213, 0.7071067811865475, 0.8941854247264057, 0.7557246162237473, -0.5165690869135746, 0.9997296970888822, 0.5348114497917161, 0.5170956986097696, 0.6664864647259459, 0.6322845774622146, 0.6028596903671783, -0.5001658676954687, 0.48494013390167173, 0.5771942097311452, -0.6668878235939646, 0.7557246162237653, 0.5165373633174144, 0.6664864647259213, 0.5771942097311291, 0.8162758795249836, 0.9997296970888822, 0.8162758795249836, -0.4473619520768934, -0.8168527127367119, 0.6322845774622308, 0.755765610511168, 0.6322845774622146, -0.48523216537343977, 0.5345947253165623, 0.534378000841413, 0.8166069316290905, 0.8941854247264057, 0.45926554992618285, 0.5162581956712325, 0.8162758795250046, 0.5771942097311291, -0.8947239041537959, -0.6033009329113302, 0.534766968847327, 0.5545502591473919, 0.4998648485444373, 0.43649476540296506, -0.894723904153784, 0.48520236624366025, 0.5545502591473919, 0.5773502691896258, 0.9997296970888822, 0.5162581956712442, -0.6032857099276496, 0.7557246162237666, 0.9997296970888822, 0.6030585195986301]\n",
      "    Estimated Category: [\"['trump']\", \"not ['trump']\", \"['trump']\", \"['trump']\", \"['trump']\", \"['trump']\", \"['trump']\", \"['trump']\", \"['trump']\", \"not ['trump']\", \"['trump']\", \"['trump']\", \"['trump']\", \"['trump']\", \"not ['trump']\", \"['trump']\", \"['trump']\", \"['trump']\", \"['trump']\", \"['trump']\", \"['trump']\", \"not ['trump']\", \"['trump']\", \"['trump']\", \"not ['trump']\", \"['trump']\", \"['trump']\", \"not ['trump']\", \"['trump']\", \"['trump']\", \"not ['trump']\", \"['trump']\", \"['trump']\", \"['trump']\", \"['trump']\", \"not ['trump']\", \"['trump']\", \"['trump']\", \"['trump']\", \"['trump']\", \"['trump']\", \"['trump']\", \"not ['trump']\", \"['trump']\", \"['trump']\", \"not ['trump']\", \"['trump']\", \"['trump']\", \"['trump']\", \"['trump']\", \"['trump']\", \"['trump']\", \"['trump']\", \"not ['trump']\", \"not ['trump']\", \"['trump']\", \"['trump']\", \"['trump']\", \"not ['trump']\", \"['trump']\", \"['trump']\", \"['trump']\", \"['trump']\", \"['trump']\", \"['trump']\", \"['trump']\", \"['trump']\", \"not ['trump']\", \"not ['trump']\", \"['trump']\", \"['trump']\", \"['trump']\", \"['trump']\", \"not ['trump']\", \"['trump']\", \"['trump']\", \"['trump']\", \"['trump']\", \"['trump']\", \"not ['trump']\", \"['trump']\", \"['trump']\", \"['trump']\"]\n",
      "    Actual Category: []\n",
      "\n",
      "\n",
      "Out-Sample Perf Metrics\n",
      "    Accuracy: 0.20481927710843373\n",
      "    Recall: NaN\n",
      "    Precision: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Here we re-use the contrasted topic from section 2\n",
    "fixed_topics = {\"keywords\": api_response.result.keywords, \"weights\": api_response.result.keywords_weight} # dict | The contrasting topic used to separate the two categories of documents. Weights optional\n",
    "print(len(api_response.result.keywords), len(api_response.result.keywords_weight))\n",
    "metadata_selection = {\"label\": [\"Violence\", \"All clear\"]} # dict | The metadata selection defining the two categories of documents that a document can be classified into\n",
    "\n",
    "query = '' # str | Dataset-language-specific fulltext query, using mysql MATCH boolean query format (optional)\n",
    "custom_stop_words = [\"\"] # List of stop words. (optional)\n",
    "excluded_docs = '' # str | List of document IDs that should be excluded from the analysis. Example, [\"docid1\", \"docid2\", ..., \"docidN\"]  (optional)\n",
    "syntax_variables = True # bool | If True, the classifier will include syntax-related variables on top of content variables (optional) (default to False)\n",
    "threshold = 0 # float | Threshold value for a document exposure to the contrasted topic, above which the document is assigned to class 1 specified through metadata_selection. (optional) (default to 0)\n",
    "remove_redundancies = True # bool | If True, this option removes quasi-duplicates from the analysis. A quasi-duplicate would have the same NLP representation, but not necessarily the exact same text. (optional) (default True)\n",
    "\n",
    "\n",
    "payload = nucleus_api.DocClassifyModel(dataset=\"Social_media_feed\",\n",
    "                                        fixed_topics=fixed_topics,\n",
    "                                        metadata_selection=metadata_selection,\n",
    "                                        validation_phase=True,\n",
    "                                        period_start='2018-01-01',\n",
    "                                        period_end='2019-01-01')\n",
    "                                        #period_start='2019-01-01',\n",
    "                                        #period_end='2019-03-01')\n",
    "\n",
    "try:\n",
    "    api_response = api_instance.post_doc_classify_api(payload)\n",
    "    api_ok = True\n",
    "except ApiException as e:\n",
    "    api_error = json.loads(e.body)\n",
    "    print('ERROR:', api_error['message'])\n",
    "    api_ok = False\n",
    "\n",
    "if api_ok:\n",
    "    print('Detailed Results')\n",
    "    print('    Docids:', api_response.result.detailed_results.docids)\n",
    "    print('    Exposure:', api_response.result.detailed_results.exposures)\n",
    "    print('    Estimated Category:', api_response.result.detailed_results.estimated_class)\n",
    "    print('    Actual Category:', api_response.result.detailed_results.true_class)\n",
    "    print('\\n')\n",
    "\n",
    "    print('Out-Sample Perf Metrics')\n",
    "    print('    Accuracy:', api_response.result.perf_metrics.hit_rate)\n",
    "    print('    Recall:', api_response.result.perf_metrics.recall)\n",
    "    print('    Precision:', api_response.result.perf_metrics.precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can move to the testing phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = nucleus_api.DocClassifyModel(dataset=\"Social_media_feed\",\n",
    "                                        fixed_topics=fixed_topics,\n",
    "                                        metadata_selection=metadata_selection,\n",
    "                                        custom_stop_words=custom_stop_words,\n",
    "                                        validation_phase=False,\n",
    "                                        period_start='2019-03-01',\n",
    "                                        period_end='2019-06-01')\n",
    "\n",
    "try:\n",
    "    api_response = api_instance.post_doc_classify_api(payload)\n",
    "    api_ok = True\n",
    "except ApiException as e:\n",
    "    api_error = json.loads(e.body)\n",
    "    print('ERROR:', api_error['message'])\n",
    "    api_ok = False\n",
    "\n",
    "if api_ok:\n",
    "    print('Detailed Results')\n",
    "    print('    Docids:', api_response.result.detailed_results.docids)\n",
    "    print('    Exposure:', api_response.result.detailed_results.exposures)\n",
    "    print('    Estimated Category:', api_response.result.detailed_results.estimated_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.\tFine Tuning\n",
    "\n",
    "#### a. Specifying the metadata_selection for your contrasted topic\n",
    "\n",
    "-     Contrasting documents that contain different keywords \n",
    "\n",
    "    This can be useful to detect certain expressions that come up frequently in specific contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_selection = {\"content\": \"kill hate torture\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-     Contrasting documents that come from different authors\n",
    "\n",
    "    This can be useful to detect multiple accounts that link to the same actual person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_selection = {\"author\": \"@suspicious_author\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b.\tReducing noise in your low-quality content detection\n",
    "-\tSee whether some tailoring may be applied to your content classification by excluding certain topics considered not information-bearing for your end-user or your application. This is achieved by using the custom_stop_words parameter in input to the Contrasted Topic and Document Classify APIs\n",
    "\n",
    "\n",
    "-\tIdentify and Extract key topics on documents within your corpus and print the keywords of these topics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('------------- Get list of topics from dataset --------------')\n",
    "\n",
    "payload = nucleus_api.Topics(dataset='Social_media_feed',                         \n",
    "                            query='',                       \n",
    "                            num_topics=20, \n",
    "                            num_keywords=8,\n",
    "                            metadata_selection=metadata_selection)\n",
    "\n",
    "try:\n",
    "    api_response = api_instance.post_topic_api(payload)\n",
    "    api_ok = True\n",
    "except ApiException as e:\n",
    "    api_error = json.loads(e.body)\n",
    "    print('ERROR:', api_error['message'])\n",
    "    api_ok = False\n",
    "\n",
    "if api_ok:\n",
    "    for i, res in enumerate(api_response.result.topics):\n",
    "        print('Topic', i, ' keywords: ', res.keywords)    \n",
    "        print('---------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using your domain expertise / client input / advisor input, you can determine whether certain of those topics or keywords are not differentiated enough to contribute to low-quality content detection. \n",
    "\n",
    "You can then tailor the low-quality content detection by creating a custom_stop_words variable that contains those words. Initialize the variable as follows, for instance, and pass it in the payload of the main code of section 2: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_stop_words = [\"tough dude\",\"bad boy\"] # str | List of stop words. (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c. Focusing the content detection on specific subjects potentially discussed in your corpus\n",
    "**query**: You can refine the content detection by leveraging the query variable of the Contrasted Topic and Document Classify APIs.\n",
    "\n",
    "Rerun any of these 2 APIs on the content from your corpus that mentions a specific theme. Create a variable query and pass it in to the payload:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '(LOL OR league of legends OR WOW OR world of warcraft)' # str | Fulltext query, using mysql MATCH boolean query format. Example: \"(word1 OR word2) AND (word3 OR word4)\" (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) 2019 SumUp Analytics, Inc. All Rights Reserved.\n",
    "\n",
    "NOTICE: All information contained herein is, and remains the property of SumUp Analytics Inc. and its suppliers, if any. The intellectual and technical concepts contained herein are proprietary to SumUp Analytics Inc. and its suppliers and may be covered by U.S. and Foreign Patents, patents in process, and are protected by trade secret or copyright law.\n",
    "\n",
    "Dissemination of this information or reproduction of this material is strictly forbidden unless prior written permission is obtained from SumUp Analytics Inc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
