{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>  Low Quality Content Detection - Nucleus APIs Use Cases</center></h1>\n",
    "\n",
    "\n",
    "<h1><center>  SumUp Analytics, Proprietary & Confidential</center></h1>\n",
    "\n",
    "\n",
    "<h1><center>  Disclaimers and Terms of Service available at www.sumup.ai</center></h1>\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "## Objective: \n",
    "-\tDevelop a pipeline to detect low quality content in social-media or gaming chatroom\n",
    "\n",
    "**In its current version, SumUp contrast analysis works comparing two categories against each other, where the user defines what the two categories are.**\n",
    "\n",
    "## Data:\n",
    "-\tA labeled corpus of posts from a social media or gaming platform\n",
    " -     You can have multiple labels in your corpus, but the algorithms will deal with two labels at a time when learning / predicting\n",
    " \n",
    " \n",
    " -     Illustrative labels for low-quality content detection: **\"Violence\", \"Drugs\", \"Pornographic\", \"Religiously Sensitive\", \"Politically Sensitive\", \"Scam\", \"Clickbait\", \"Fake\", \"All clear\"**\n",
    "\n",
    "\n",
    "\n",
    "## Nucleus APIs used:\n",
    "-\tDataset creation API\n",
    " - \t*api_instance.post_upload_file(file, dataset)*\n",
    " - \t*nucleus_helper.import_files(api_instance, dataset, file_iters, processes=1)*\n",
    "\n",
    "        nucleus_helper.import_files leverages api_instance.post_upload_file with parallel execution to speed-up the dataset creation\n",
    "\n",
    "\n",
    "-\tTopic Modeling API\n",
    " - \t*api_instance.post_topic_api(payload)*\n",
    "\n",
    "\n",
    "-\tContrasted Topic Modeling API\n",
    " - \t*api_instance.post_topic_contrast_api(payload)*\n",
    "\n",
    "\n",
    "-\tDocuments Classification API\n",
    " - \t*api_instance.post_doc_classify_api(payload)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach:\n",
    "\n",
    "### 1.\tDataset Preparation\n",
    "-\tCreate a Nucleus dataset containing all relevant documents\n",
    "\n",
    "\n",
    "-   We assume that the data is stored in a csv file. A similar code could be built to inject from a database table. There are some requirements on the name of data and metadata fields passed to the API to create a dataset\n",
    "\n",
    "\n",
    "    - Illustrative template for the data uploaded: [\"author\", \"label\", \"time\", \"content\", \"title\"]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import nucleus_api.api.nucleus_api as nucleus_helper\n",
    "import nucleus_api\n",
    "from nucleus_api.rest import ApiException\n",
    "\n",
    "configuration = nucleus_api.Configuration()\n",
    "configuration.host = 'UPDATE-WITH-API-SERVER-HOSTNAME'\n",
    "configuration.api_key['x-api-key'] = 'UPDATE-WITH-API-KEY'\n",
    "\n",
    "# Create API instance\n",
    "api_instance = nucleus_api.NucleusApi(nucleus_api.ApiClient(configuration))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file = 'Social_media_feed.csv'\n",
    "dataset = 'Social_media_feed'# str | Destination dataset where the file will be inserted.\n",
    "\n",
    "with open(csv_file, encoding='utf-8-sig') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    json_props = nucleus_helper.upload_jsons(api_instance, dataset, reader, processes=4)\n",
    "    \n",
    "    total_size = 0\n",
    "    total_jsons = 0\n",
    "    for jp in json_props:\n",
    "        total_size += jp.size\n",
    "        total_jsons += 1\n",
    "        \n",
    "    print(total_jsons, 'JSON records (', total_size, 'bytes) appended to', dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Contrasted Topic Modeling\n",
    "\n",
    "-     In this example, we define one category of documents tagged \"Violence\". The second category of documents are tagged \"All clear\".\n",
    "-     We extract one topic that separates those two categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_selection = {\"label\": [\"Violence\", \"All clear\"]} # dict | The metadata selection defining the two categories of documents to contrast and summarize against each other\n",
    "\n",
    "query = '' # str | Dataset-language-specific fulltext query, using mysql MATCH boolean query format (optional)\n",
    "custom_stop_words = [\"\"] # List of stop words. (optional)\n",
    "excluded_docs = '' # str | List of document IDs that should be excluded from the analysis. Example, [\"docid1\", \"docid2\", ..., \"docidN\"]  (optional)\n",
    "syntax_variables = True # bool | Specifies whether to take into account syntax aspects of each category of documents to help with contrasting them (optional) (default to False)\n",
    "compression = 0.002 # float | Parameter controlling the breadth of the contrasted topic. Contained between 0 and 1, the smaller it is, the more contrasting terms will be captured, with decreasing weight. (optional) (default to 0.000002)\n",
    "remove_redundancies = True # bool | If True, this option removes quasi-duplicates from the analysis. A quasi-duplicate would have the same NLP representation, but not necessarily the exact same text. (optional) (default True)\n",
    "\n",
    "payload = nucleus_api.TopicContrastModel(dataset='Social_media_feed', \n",
    "                                        metadata_selection=metadata_selection,\n",
    "                                        period_start='2018-01-01',\n",
    "                                        period_end='2019-01-01')\n",
    "try:\n",
    "    api_response = api_instance.post_topic_contrast_api(payload)\n",
    "    api_ok = True\n",
    "except ApiException as e:\n",
    "    api_error = json.loads(e.body)\n",
    "    print('ERROR:', api_error['message'])\n",
    "    api_ok = False\n",
    "\n",
    "if api_ok:\n",
    "    print('Contrasted Topic')\n",
    "    print('    Keywords:', api_response.result.keywords)\n",
    "    print('    Keywords Weight:', api_response.result.keywords_weight)\n",
    "\n",
    "    print('In-Sample Perf Metrics')\n",
    "    print('    Accuracy:', api_response.result.perf_metrics.hit_rate)\n",
    "    print('    Recall:', api_response.result.perf_metrics.recall)\n",
    "    print('    Precision:', api_response.result.perf_metrics.precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Documents Classification\n",
    "\n",
    "This task requires 3 steps:\n",
    "-     First, extract a contrasted topic on a labeled dataset\n",
    "-     Second, train the documents' classifier by providing a labeled dataset. In this step, you can adjust the weight of each keyword from the contrasted topic, remove certain keywords, and even compare the contrasted topic produced by step 1 against topics of your own choosing\n",
    "-     Third, test the classifier\n",
    "\n",
    "\n",
    "-     In the example below, we assume that the contrasted topic has already been obtained. The structure of 'fixed_topics' is exactly that which would come out of the Contrasted Topic API\n",
    "-     We train the classifier to separate violent content from \"good\" content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we re-use the contrasted topic from section 2\n",
    "fixed_topics = {\"keywords\": api_response.result.keywords, \"weights\": api_response.result.keywords_weight} # dict | The contrasting topic used to separate the two categories of documents. Weights optional\n",
    "metadata_selection = {\"label\": [\"Violence\", \"All clear\"]} # dict | The metadata selection defining the two categories of documents that a document can be classified into\n",
    "\n",
    "query = '' # str | Dataset-language-specific fulltext query, using mysql MATCH boolean query format (optional)\n",
    "custom_stop_words = [\"\"] # List of stop words. (optional)\n",
    "excluded_docs = '' # str | List of document IDs that should be excluded from the analysis. Example, [\"docid1\", \"docid2\", ..., \"docidN\"]  (optional)\n",
    "syntax_variables = True # bool | If True, the classifier will include syntax-related variables on top of content variables (optional) (default to False)\n",
    "threshold = 0 # float | Threshold value for a document exposure to the contrasted topic, above which the document is assigned to class 1 specified through metadata_selection. (optional) (default to 0)\n",
    "remove_redundancies = True # bool | If True, this option removes quasi-duplicates from the analysis. A quasi-duplicate would have the same NLP representation, but not necessarily the exact same text. (optional) (default True)\n",
    "\n",
    "payload = nucleus_api.DocClassifyModel(dataset=\"Social_media_feed\",\n",
    "                                        fixed_topics=fixed_topics,\n",
    "                                        metadata_selection=metadata_selection,\n",
    "                                        validation_phase=True,\n",
    "                                        period_start='2018-01-01',\n",
    "                                        period_end='2019-01-01')\n",
    "                                        #period_start='2019-01-01',\n",
    "                                        #period_end='2019-03-01')\n",
    "\n",
    "try:\n",
    "    api_response = api_instance.post_doc_classify_api(payload)\n",
    "    api_ok = True\n",
    "except ApiException as e:\n",
    "    api_error = json.loads(e.body)\n",
    "    print('ERROR:', api_error['message'])\n",
    "    api_ok = False\n",
    "\n",
    "if api_ok:\n",
    "    print('Detailed Results')\n",
    "    print('    Docids:', api_response.result.detailed_results.docids)\n",
    "    print('    Exposure:', api_response.result.detailed_results.exposures)\n",
    "    print('    Estimated Category:', api_response.result.detailed_results.estimated_class)\n",
    "    print('    Actual Category:', api_response.result.detailed_results.true_class)\n",
    "    print('\\n')\n",
    "\n",
    "    print('Out-Sample Perf Metrics')\n",
    "    print('    Accuracy:', api_response.result.perf_metrics.hit_rate)\n",
    "    print('    Recall:', api_response.result.perf_metrics.recall)\n",
    "    print('    Precision:', api_response.result.perf_metrics.precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can move to the testing phase, detecting violent content and \"good\" content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = nucleus_api.DocClassifyModel(dataset=\"Social_media_feed\",\n",
    "                                        fixed_topics=fixed_topics,\n",
    "                                        metadata_selection=metadata_selection,\n",
    "                                        custom_stop_words=custom_stop_words,\n",
    "                                        validation_phase=False,\n",
    "                                        period_start='2019-03-01',\n",
    "                                        period_end='2019-06-01')\n",
    "\n",
    "try:\n",
    "    api_response = api_instance.post_doc_classify_api(payload)\n",
    "    api_ok = True\n",
    "except ApiException as e:\n",
    "    api_error = json.loads(e.body)\n",
    "    print('ERROR:', api_error['message'])\n",
    "    api_ok = False\n",
    "\n",
    "if api_ok:\n",
    "    print('Detailed Results')\n",
    "    print('    Docids:', api_response.result.detailed_results.docids)\n",
    "    print('    Exposure:', api_response.result.detailed_results.exposures)\n",
    "    print('    Estimated Category:', api_response.result.detailed_results.estimated_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.\tFine Tuning\n",
    "\n",
    "#### a. Specifying the metadata_selection for your contrasted topic\n",
    "\n",
    "-     Contrasting documents that contain different keywords \n",
    "\n",
    "    This can be useful to detect certain expressions that come up frequently in specific contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_selection = {\"content\": \"kill hate torture\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-     Contrasting documents that come from different authors\n",
    "\n",
    "    This can be useful to detect multiple accounts that link to the same actual person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_selection = {\"author\": \"@suspicious_author\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b.\tReducing noise in your low-quality content detection\n",
    "-\tSee whether some tailoring may be applied to your content classification by excluding certain topics considered not information-bearing for your end-user or your application. This is achieved by using the custom_stop_words parameter in input to the Contrasted Topic and Document Classify APIs\n",
    "\n",
    "\n",
    "-\tIdentify and Extract key topics on documents within your corpus and print the keywords of these topics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('------------- Get list of topics from dataset --------------')\n",
    "\n",
    "payload = nucleus_api.Topics(dataset='Social_media_feed',                         \n",
    "                            query='',                       \n",
    "                            num_topics=20, \n",
    "                            num_keywords=8,\n",
    "                            metadata_selection=metadata_selection)\n",
    "\n",
    "try:\n",
    "    api_response = api_instance.post_topic_api(payload)\n",
    "    api_ok = True\n",
    "except ApiException as e:\n",
    "    api_error = json.loads(e.body)\n",
    "    print('ERROR:', api_error['message'])\n",
    "    api_ok = False\n",
    "\n",
    "if api_ok:\n",
    "    for i, res in enumerate(api_response.result.topics):\n",
    "        print('Topic', i, ' keywords: ', res.keywords)    \n",
    "        print('---------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using your domain expertise / client input / advisor input, you can determine whether certain of those topics or keywords are not differentiated enough to contribute to low-quality content detection. \n",
    "\n",
    "You can then tailor the low-quality content detection by creating a custom_stop_words variable that contains those words. Initialize the variable as follows, for instance, and pass it in the payload of the main code of section 2: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_stop_words = [\"tough dude\",\"bad boy\"] # str | List of stop words. (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c. Focusing the content detection on specific subjects potentially discussed in your corpus\n",
    "**query**: You can refine the content detection by leveraging the query variable of the Contrasted Topic and Document Classify APIs.\n",
    "\n",
    "Rerun any of these 2 APIs on the content from your corpus that mentions a specific theme. Create a variable query and pass it in to the payload:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '(LOL OR league of legends OR WOW OR world of warcraft)' # str | Fulltext query, using mysql MATCH boolean query format. Example: \"(word1 OR word2) AND (word3 OR word4)\" (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) 2019 SumUp Analytics, Inc. All Rights Reserved.\n",
    "\n",
    "NOTICE: All information contained herein is, and remains the property of SumUp Analytics Inc. and its suppliers, if any. The intellectual and technical concepts contained herein are proprietary to SumUp Analytics Inc. and its suppliers and may be covered by U.S. and Foreign Patents, patents in process, and are protected by trade secret or copyright law.\n",
    "\n",
    "Dissemination of this information or reproduction of this material is strictly forbidden unless prior written permission is obtained from SumUp Analytics Inc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
