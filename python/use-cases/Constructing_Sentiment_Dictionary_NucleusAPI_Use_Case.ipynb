{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>  Constructing a Sentiment Dictionary - Nucleus APIs Use Cases</center></h1>\n",
    "\n",
    "\n",
    "<h1><center>  SumUp Analytics, Proprietary & Confidential</center></h1>\n",
    "\n",
    "\n",
    "<h1><center>  Disclaimers and Terms of Service available at www.sumup.ai</center></h1>\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "## Objective: \n",
    "-\tDevelop a pipeline to create a custom sentiment dictionary with contrast analysis\n",
    "  - Facilitate data labeling for sentiment modeling\n",
    "  - Define a programmatic approach to creating sentiment dictionaries on a corpus of user's choice\n",
    "\n",
    "**SumUp contrast analysis works on the premise of two distinct categories of documents within a corpus, defined by the user based on metadata or content**\n",
    "\n",
    "## Data:\n",
    "-\tAny collection of documents, where at least a subset of it is labeled with sentiment categories such as POSITIVE / NEUTRAL / NEGATIVE\n",
    "\n",
    "\n",
    "## Nucleus APIs:\n",
    "-\tDataset creation API\n",
    " - \t*api_instance.post_upload_file(file, dataset)*\n",
    " - \t*nucleus_helper.import_files(api_instance, dataset, file_iters, processes=1)*\n",
    "\n",
    "        nucleus_helper.import_files leverages api_instance.post_upload_file with parallel execution to speed-up the dataset creation\n",
    "\n",
    "\n",
    "-\tTopic Modeling API\n",
    " - \t*api_instance.post_topic_api(payload)*\n",
    "\n",
    "\n",
    "-\tContrasted Topic Modeling API\n",
    " - \t*api_instance.post_topic_contrast_api(payload)*\n",
    " \n",
    "\n",
    "-\tDocuments Classification API\n",
    " - \t*api_instance.post_doc_classify_api(payload)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach:\n",
    "\n",
    "### 1.\tTraining, Validation, Testing Dataset Preparation\n",
    "-\tCreate a Nucleus dataset containing all relevant documents\n",
    "\n",
    "\n",
    "-   The input documents must be labeled\n",
    "    - Either these documents are stored in a CSV or JSON and one column of the data corresponds to the sentiment label\n",
    "    - Or you need to specify the sentiment label as an extra metadata field when you construct your dataset in Nucleus\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('---- Train / Validate / Test dataset ----')\n",
    "folder = 'Sellside_research'         \n",
    "dataset = 'Sellside_research'# str | Destination dataset where the file will be inserted.\n",
    "\n",
    "# build file iterable from a folder recursively. \n",
    "# Each item in the iterable is in the format below:\n",
    "# {'filename': filename,   # filename to be uploaded. REQUIRED\n",
    "#  'metadata': {           # metadata for the file. Optional\n",
    "#      'key1': val1,       # keys can have arbiturary names as long as the names only\n",
    "#      'key2': val2        # contain alphanumeric (0-9|a-z|A-Z) and underscore (_)\n",
    "#   } \n",
    "# }\n",
    "\n",
    "# If the documents are not in a CSV or JSON, then you must specify sentiment labels in the file_iter object\n",
    "# as an extra metadata field.\n",
    "\n",
    "# If you are reading from a file where the sentiment label is already provided, \n",
    "# no need to pass the 'metadata' in the file_dict\n",
    "\n",
    "file_iter = []\n",
    "for root, dirs, files in os.walk(folder):\n",
    "    for file in files:\n",
    "        file_dict = {'filename': os.path.join(root, file),\n",
    "                     'metadata': {'sentiment': 'positive' # Here build some logic to decide how to assign POS / NEU / NEG\n",
    "                                }}\n",
    "        file_iter.append(file_dict)\n",
    "\n",
    "file_props = nucleus_helper.upload_files(api_instance, dataset, file_iter, processes=4)\n",
    "for fp in file_props:\n",
    "    print(fp.filename, '(', fp.size, 'bytes) has been added to dataset', dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.\tUnlabeled Dataset Preparation\n",
    "-\tCreate a Nucleus dataset containing all relevant documents\n",
    "\n",
    "\n",
    "-   The input documents are not labeled    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('---- Dataset to label ----')\n",
    "folder = 'Sellside_research_unlabeled'         \n",
    "dataset = 'Sellside_research_unlabeled'# str | Destination dataset where the file will be inserted.\n",
    "\n",
    "# build file iterable from a folder recursively. \n",
    "# Each item in the iterable is in the format below:\n",
    "# {'filename': filename,   # filename to be uploaded. REQUIRED }\n",
    "\n",
    "file_iter = []\n",
    "for root, dirs, files in os.walk(folder):\n",
    "    for file in files:\n",
    "        file_dict = {'filename': os.path.join(root, file)}\n",
    "        file_iter.append(file_dict)\n",
    "\n",
    "file_props = nucleus_helper.upload_files(api_instance, dataset, file_iter, processes=4)\n",
    "for fp in file_props:\n",
    "    print(fp.filename, '(', fp.size, 'bytes) has been added to dataset', dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Accelerating the Labeling of Data\n",
    "\n",
    "-     In this example, we define one category of documents as having positive sentiment. The second category has negative sentiment\n",
    "-     We extract the contrast topic on the training set that separates those two categories with TopicContrastModel\n",
    "-     We divide our dataset into training, validation and testing sets based on date created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_selection_contrast = {\"sentiment\": [\"positive\", \"negative\"]} # dict | Specifies the two categories of documents to contrast and summarize against each other\n",
    "\n",
    "query = '' # str | Dataset-language-specific fulltext query, using mysql MATCH boolean query format (optional)\n",
    "custom_stop_words = [\"\"] # List of stop words. (optional)\n",
    "excluded_docs = '' # str | List of document IDs that should be excluded from the analysis. Example, [\"docid1\", \"docid2\", ..., \"docidN\"]  (optional)\n",
    "syntax_variables = False # bool | Specifies whether to take into account syntax aspects of each category of documents to help with contrasting them (optional) (default to False)\n",
    "num_keywords = 20 # integer | Number of keywords for the contrasted topic that is extracted from the dataset. (optional) (default to 50)\n",
    "remove_redundancies = False # bool | If True, this option removes quasi-duplicates from the analysis. A quasi-duplicate would have the same NLP representation, but not necessarily the exact same text. (optional) (default False)\n",
    "\n",
    "payload = nucleus_api.TopicContrastModel(dataset='Sellside_research', \n",
    "                                        metadata_selection_contrast=metadata_selection_contrast,\n",
    "                                        custom_stop_words=custom_stop_words,\n",
    "                                        period_start='2017-01-01',\n",
    "                                        period_end='2018-01-01')\n",
    "api_response = api_instance.post_topic_contrast_api(payload)\n",
    "\n",
    "print('Contrasted Topic')\n",
    "print('    Keywords:', api_response.result.keywords)\n",
    "print('    Keywords Weight:', api_response.result.keywords_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-     Determine how well this contrasting topic performs at sentiment labeling of your corpus, on the validation dataset\n",
    "-     We detail further down how to fine-tune the sentiment labeler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_topics = {\"keywords\": api_response.result.keywords, \"weights\": api_response.result.keywords_weight} # dict | The contrasting topic used to separate the two categories of documents. Weights optional\n",
    "metadata_selection_contrast = {\"sentiment\": [\"positive\", \"negative\"]} # dict | Specifies the two categories of documents to contrast and summarize against each other\n",
    "\n",
    "query = '' # str | Dataset-language-specific fulltext query, using mysql MATCH boolean query format (optional)\n",
    "custom_stop_words = [\"\"] # List of stop words. (optional)\n",
    "excluded_docs = '' # str | List of document IDs that should be excluded from the analysis. Example, [\"docid1\", \"docid2\", ..., \"docidN\"]  (optional)\n",
    "syntax_variables = False # bool | If True, the classifier will include syntax-related variables on top of content variables (optional) (default to False)\n",
    "threshold = 0 # float | Threshold value for a document exposure to the contrastic topic, above which the document is assigned to class 1 specified through metadata_selection. (optional) (default to 0)\n",
    "remove_redundancies = False # bool | If True, this option removes quasi-duplicates from the analysis. A quasi-duplicate would have the same NLP representation, but not necessarily the exact same text. (optional) (default False)\n",
    "\n",
    "\n",
    "payload = nucleus_api.DocClassifyModel(dataset=\"Sellside_research\",\n",
    "                                        fixed_topics=fixed_topics,\n",
    "                                        metadata_selection_contrast=metadata_selection_contrast,\n",
    "                                        custom_stop_words=custom_stop_words,\n",
    "                                        validation_phase=True, # This argument tells the API that data is labeled, so produces perf metrics\n",
    "                                        period_start='2018-01-01',\n",
    "                                        period_end='2019-01-01')\n",
    "api_response = api_instance.post_doc_classify_api(payload)\n",
    "\n",
    "print('Detailed Results')\n",
    "print('    Docids:', api_response.result.detailed_results.docids)\n",
    "print('    Exposure:', api_response.result.detailed_results.exposures)\n",
    "print('    Estimated Category:', api_response.result.detailed_results.estimated_class)\n",
    "print('    Actual Category:', api_response.result.detailed_results.true_class)\n",
    "print('\\n')\n",
    "\n",
    "print('Perf Metrics')\n",
    "print('    Accuracy:', api_response.result.perf_metrics.hit_rate)\n",
    "print('    Recall:', api_response.result.perf_metrics.recall)\n",
    "print('    Precision:', api_response.result.perf_metrics.precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-     Once you are satified with your labeling model, you can apply it to the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_topics = {\"keywords\": api_response.result.keywords, \"weights\": api_response.result.keywords_weight} # dict | The contrasting topic used to separate the two categories of documents. Weights optional\n",
    "metadata_selection_contrast = {\"sentiment\": [\"positive\", \"negative\"]} # dict | Specifies the two categories of documents to contrast and summarize against each other\n",
    "\n",
    "query = '' # str | Dataset-language-specific fulltext query, using mysql MATCH boolean query format (optional)\n",
    "custom_stop_words = [\"\"] # List of stop words. (optional)\n",
    "excluded_docs = '' # str | List of document IDs that should be excluded from the analysis. Example, [\"docid1\", \"docid2\", ..., \"docidN\"]  (optional)\n",
    "syntax_variables = False # bool | If True, the classifier will include syntax-related variables on top of content variables (optional) (default to False)\n",
    "threshold = 0 # float | Threshold value for a document exposure to the contrastic topic, above which the document is assigned to class 1 specified through metadata_selection. (optional) (default to 0)\n",
    "remove_redundancies = False # bool | If True, this option removes quasi-duplicates from the analysis. A quasi-duplicate would have the same NLP representation, but not necessarily the exact same text. (optional) (default False)\n",
    "\n",
    "\n",
    "payload = nucleus_api.DocClassifyModel(dataset=\"Sellside_research\",\n",
    "                                        fixed_topics=fixed_topics,\n",
    "                                        metadata_selection_contrast=metadata_selection_contrast,\n",
    "                                        custom_stop_words=custom_stop_words,\n",
    "                                        validation_phase=True, # This argument tells the API that data is labeled, so produces perf metrics\n",
    "                                        period_start='2019-01-01',\n",
    "                                        period_end='2019-07-01')\n",
    "api_response = api_instance.post_doc_classify_api(payload)\n",
    "\n",
    "print('Detailed Results')\n",
    "print('    Docids:', api_response.result.detailed_results.docids)\n",
    "print('    Exposure:', api_response.result.detailed_results.exposures)\n",
    "print('    Estimated Category:', api_response.result.detailed_results.estimated_class)\n",
    "print('    Actual Category:', api_response.result.detailed_results.true_class)\n",
    "print('\\n')\n",
    "\n",
    "print('Perf Metrics')\n",
    "print('    Accuracy:', api_response.result.perf_metrics.hit_rate)\n",
    "print('    Recall:', api_response.result.perf_metrics.recall)\n",
    "print('    Precision:', api_response.result.perf_metrics.precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-     You are now ready to run the above model on the unlabeled dataset\n",
    "-     You will retrieve 'estimated_class' for each document, which completes your dataset labeling\n",
    "-     You can repeat this process for any pair of sentiment labels, and cross-validate the sentiment labels of the unlabeled dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_topics = {\"keywords\": api_response.result.keywords, \"weights\": api_response.result.keywords_weight} # dict | The contrasting topic used to separate the two categories of documents. Weights optional\n",
    "metadata_selection_contrast = {\"sentiment\": [\"positive\", \"negative\"]} # dict | Specifies the two categories of documents to contrast and summarize against each other\n",
    "\n",
    "query = '' # str | Dataset-language-specific fulltext query, using mysql MATCH boolean query format (optional)\n",
    "custom_stop_words = [\"\"] # List of stop words. (optional)\n",
    "excluded_docs = '' # str | List of document IDs that should be excluded from the analysis. Example, [\"docid1\", \"docid2\", ..., \"docidN\"]  (optional)\n",
    "syntax_variables = False # bool | If True, the classifier will include syntax-related variables on top of content variables (optional) (default to False)\n",
    "threshold = 0 # float | Threshold value for a document exposure to the contrastic topic, above which the document is assigned to class 1 specified through metadata_selection. (optional) (default to 0)\n",
    "remove_redundancies = False # bool | If True, this option removes quasi-duplicates from the analysis. A quasi-duplicate would have the same NLP representation, but not necessarily the exact same text. (optional) (default False)\n",
    "\n",
    "\n",
    "payload = nucleus_api.DocClassifyModel(dataset=\"Sellside_research_unlabeled\",\n",
    "                                        fixed_topics=fixed_topics,\n",
    "                                        metadata_selection_contrast=metadata_selection_contrast,\n",
    "                                        custom_stop_words=custom_stop_words,\n",
    "                                        validation_phase=False)\n",
    "api_response = api_instance.post_doc_classify_api(payload)\n",
    "\n",
    "print('Detailed Results')\n",
    "print('    Docids:', api_response.result.detailed_results.docids)\n",
    "print('    Estimated Category:', api_response.result.detailed_results.estimated_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Generating a Sentiment Dictionary\n",
    "\n",
    "-     Use the whole dataset: train/validate/test data + the data that was labeled in the previous step\n",
    "-     Generate topics that best contrast any two labels from above\n",
    "-     You can repeat this process for any pair of sentiment labels, and cross-validate the sentiment labels of each word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('---- Complete dataset ----')\n",
    "folder = 'Sellside_research_combined'         \n",
    "dataset = 'Sellside_research_combined'# str | Destination dataset where the file will be inserted.\n",
    "\n",
    "# build file iterable from a folder recursively. \n",
    "# Each item in the iterable is in the format below:\n",
    "# {'filename': filename,   # filename to be uploaded. REQUIRED\n",
    "#  'metadata': {           # metadata for the file. Optional\n",
    "#      'key1': val1,       # keys can have arbiturary names as long as the names only\n",
    "#      'key2': val2        # contain alphanumeric (0-9|a-z|A-Z) and underscore (_)\n",
    "#   } \n",
    "# }\n",
    "\n",
    "# If the documents are not in a CSV or JSON, then you must specify sentiment labels in the file_iter object\n",
    "# as an extra metadata field.\n",
    "\n",
    "# If you are reading from a file where the sentiment label is already provided, \n",
    "# no need to pass the 'metadata' in the file_dict\n",
    "\n",
    "file_iter = []\n",
    "for root, dirs, files in os.walk(folder):\n",
    "    for file in files:\n",
    "        file_dict = {'filename': os.path.join(root, file),\n",
    "                     'metadata': {'sentiment': 'positive' # Here pass in the labels obtained in the previous step\n",
    "                                }}\n",
    "        file_iter.append(file_dict)\n",
    "\n",
    "file_props = nucleus_helper.upload_files(api_instance, dataset, file_iter, processes=4)\n",
    "for fp in file_props:\n",
    "    print(fp.filename, '(', fp.size, 'bytes) has been added to dataset', dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_selection_contrast = {\"sentiment\": [\"positive\", \"negative\"]} # dict | Specifies the two categories of documents to contrast and summarize against each other\n",
    "\n",
    "query = '' # str | Dataset-language-specific fulltext query, using mysql MATCH boolean query format (optional)\n",
    "custom_stop_words = [\"\"] # List of stop words. (optional)\n",
    "excluded_docs = '' # str | List of document IDs that should be excluded from the analysis. Example, [\"docid1\", \"docid2\", ..., \"docidN\"]  (optional)\n",
    "syntax_variables = False # bool | Specifies whether to take into account syntax aspects of each category of documents to help with contrasting them (optional) (default to False)\n",
    "num_keywords = 20 # integer | Number of keywords for the contrasted topic that is extracted from the dataset. (optional) (default to 50)\n",
    "remove_redundancies = False # bool | If True, this option removes quasi-duplicates from the analysis. A quasi-duplicate would have the same NLP representation, but not necessarily the exact same text. (optional) (default False)\n",
    "\n",
    "payload = nucleus_api.TopicContrastModel(dataset='Sellside_research_combined', \n",
    "                                        metadata_selection_contrast=metadata_selection_contrast)\n",
    "api_response = api_instance.post_topic_contrast_api(payload)\n",
    "\n",
    "print('Contrasted Topic')\n",
    "print('    Keywords:', api_response.result.keywords)\n",
    "print('    Keywords Weight:', api_response.result.keywords_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.\tFine Tuning\n",
    "\n",
    "#### a.\tExcluding certain content from the contrast analysis\n",
    "\n",
    "-   Exclude irrelavant keywords / topics to tailor your contrast analysis by using the `custom_stop_words` parameter in the Contrast Analysis API\n",
    "\n",
    "\n",
    "-\tExtract keywords of the contrast topic on documents within your corpus and print the keywords of these topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('------------- Get list of topics from dataset --------------')\n",
    "\n",
    "payload = nucleus_api.Topics(dataset='Sellside_research',                         \n",
    "                            query='',                       \n",
    "                            num_topics=8, \n",
    "                            num_keywords=8,\n",
    "                            metadata_selection=metadata_selection_contrast)\n",
    "api_response = api_instance.post_topic_api(payload)        \n",
    "    \n",
    "for i, res in enumerate(api_response.result.topics):\n",
    "    print('Topic', i, ' keywords: ', res.keywords)    \n",
    "    print('---------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using your domain expertise or client / advisor input, you can determine if specific topics or keywords are not differentiated enough to contribute to contrast analysis. \n",
    "\n",
    "You can then tailor the contrast analysis by creating a `custom_stop_words` variable that contains those words. As shown below, initialize the variable and pass it in the payload of the main code of section 3: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_stop_words = [\"disclaimer\", \"disclosure\"] # str | List of stop words. (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. Specifying the metadata_selection_contrast for your contrasted topic\n",
    "\n",
    "-     Contrasting documents from two different entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_selection_contrast = {\"research_analyst\": [\"MS\", \"JPM\"]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-     Contrasting documents that contain different keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_selection_contrast = {\"content\": \"fundamentals\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c. Fine-tuning the contrasting topic\n",
    "\n",
    "**num_keywords**: The larger num_keywords, the more words will be retained in the contrasting topic, with increasingly less impact on separating the two categories of sentiment you work with\n",
    "\n",
    "**syntax_variables**: If True, then certain Part-of-Speech features are automatically included in the contrasting topic model. It may help if certain authors have vastly different writing styles. This is frequent with social media data and news. It is less likely to be in institutional publications\n",
    "\n",
    "**threshold**: This is the minimum exposure a document must have to the contrasting topic to be assigned to category_1 that you defined. A perfect model would have a threshold of 0, the default value. You may observe that higher performance metrics are obtained in validation from choosing a different value. This may be explained in particular in smaller samples for training and validation, or if there are generic words that appear as keywords in the contrasting topic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) 2019 SumUp Analytics, Inc. All Rights Reserved.\n",
    "\n",
    "NOTICE: All information contained herein is, and remains the property of SumUp Analytics Inc. and its suppliers, if any. The intellectual and technical concepts contained herein are proprietary to SumUp Analytics Inc. and its suppliers and may be covered by U.S. and Foreign Patents, patents in process, and are protected by trade secret or copyright law.\n",
    "\n",
    "Dissemination of this information or reproduction of this material is strictly forbidden unless prior written permission is obtained from SumUp Analytics Inc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
