{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>  News Tracking & Analysis - Nucleus APIs Use Cases</center></h1>\n",
    "\n",
    "\n",
    "<h1><center>  SumUp Analytics, Proprietary & Confidential</center></h1>\n",
    "\n",
    "\n",
    "<h1><center>  Disclaimers and Terms of Service available at www.sumup.ai</center></h1> \n",
    "\n",
    " \n",
    "\n",
    "\n",
    "## Objective: \n",
    "-\tDevelop a workflow to identify and track certain topics in news / social media, and to have the ability to analyze those topics in terms of key contributors and key takeaways\n",
    "\n",
    "\n",
    "## Data:\n",
    "-\tA collection of News Media RSS\n",
    "-   Social media feeds\n",
    "\n",
    "    **The Nucleus Datafeed can be leveraged for content from 200 News Media RSS**\n",
    "\n",
    "\n",
    "## Nucleus APIs used:\n",
    "-\tDataset creation API\n",
    " - \t*api_instance.post_upload_file(file, dataset)*\n",
    " - \t*nucleus_helper.import_files(api_instance, dataset, file_iters, processes=1)*\n",
    "\n",
    "        nucleus_helper.import_files leverages api_instance.post_upload_file with parallel execution to speed-up the dataset creation\n",
    "\n",
    "\n",
    "-\tTopic Modeling API\n",
    " - \t*api_instance.post_topic_api(payload)*\n",
    "\n",
    "\n",
    "-\tTopic Historical Analysis API\n",
    " - \t*api_instance.post_topic_historical_analysis_api(payload)*\n",
    "\n",
    "\n",
    "-\tAuthor Connectivity API\n",
    " - \t*api_instance.post_author_connectivity_api(payload)*\n",
    " \n",
    "\n",
    "-\tTopic Summary API\n",
    " -  *api_instance.post_topic_summary_api(payload)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach:\n",
    "\n",
    "### 1.\tDataset Preparation\n",
    "-\tCreate a Nucleus dataset containing all relevant documents over a chosen historical period\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leverage your own corpus\n",
    "print('---- Case 1: you are using your own corpus, coming from a local folder ----')\n",
    "folder = 'Twitter_feed'         \n",
    "dataset = 'Twitter_feed'# str | Destination dataset where the file will be inserted.\n",
    "\n",
    "# build file iterable from a folder recursively. \n",
    "# Each item in the iterable is in the format below:\n",
    "# {'filename': filename,   # filename to be uploaded. REQUIRED\n",
    "#  'metadata': {           # metadata for the file. Optional\n",
    "#      'key1': val1,       # keys can have arbiturary names as long as the names only\n",
    "#      'key2': val2        # contain alphanumeric (0-9|a-z|A-Z) and underscore (_)\n",
    "#   } \n",
    "# }\n",
    "file_iter = []\n",
    "for root, dirs, files in os.walk(folder):\n",
    "    for file in files:\n",
    "        #if Path(file).suffix == '.pdf': # .txt .doc .docx .rtf .html .csv also supported\n",
    "        file_dict = {'filename': os.path.join(root, file),\n",
    "                     'metadata': {'source': 'Tech Crunch',\n",
    "                                  'author': 'Sarah Moore'\n",
    "                                  'category': 'Media',\n",
    "                                  'date': '2019-01-01'}}\n",
    "        file_iter.append(file_dict)\n",
    "\n",
    "file_props = nucleus_helper.upload_files(api_instance, dataset, file_iter, processes=4)\n",
    "for fp in file_props:\n",
    "    print(fp.filename, '(', fp.size, 'bytes) has been added to dataset', dataset)\n",
    "\n",
    "    \n",
    "# Leverage a Nucleus embedded feed    \n",
    "print('---- Case 2: you are using an embedded datafeed ----')\n",
    "dataset = 'sumup/rss_feed_ai'# embedded datafeeds in Nucleus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-\tFor a given date in that period, retain only a subset of the documents published during a chosen lookback period\n",
    "\n",
    "**This can be done directly into the APIs that perform content analysis, see below**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.\tContent Analysis & Tracking\n",
    "-\tIdentify and Extract key topics over a recent period of your corpus \n",
    "\n",
    "\n",
    "-\tTrack how those key topics have evolved up until now: relevance, perception "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('------------ Get topics + historical analysis ----------------')\n",
    "\n",
    "payload = nucleus_api.TopicHistoryModel(dataset='Twitter_feed', \n",
    "                                    update_period='d', \n",
    "                                    query='',\n",
    "                                    num_topics=20, \n",
    "                                    num_keywords=8,\n",
    "                                    inc_step=1,\n",
    "                                    period_start=\"2018-12-01 00:00:00\",\n",
    "                                    period_end=\"2019-01-01 00:00:00\")\n",
    "api_response = api_instance.post_topic_historical_analysis_api(payload)\n",
    "\n",
    "print('Plotting historical metrics data...')\n",
    "historical_metrics = []\n",
    "for res in api_response.result:\n",
    "    # construct a list of historical metric' dictionaries for charting\n",
    "    historical_metrics.append({\n",
    "        'topic'    : res.topic,\n",
    "        'time_stamps' : np.array(res.time_stamps),\n",
    "        'strength' : np.array(res.strengths, dtype=np.float32),\n",
    "        'consensus': np.array(res.consensuses, dtype=np.float32), \n",
    "        'sentiment': np.array(res.sentiments, dtype=np.float32)})\n",
    "\n",
    "selected_topics = range(len(historical_metrics)) \n",
    "nucleus_helper.topic_charts_historical(historical_metrics, selected_topics, True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   For a few topics standing out as relevant to you, pull out key recent takeaways (summaries, best sources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('------------- Get the summaries of recent topics in your feed --------------')\n",
    "\n",
    "payload = nucleus_api.TopicSummaryModel(dataset='Twitter_feed',                         \n",
    "                            query='',                       \n",
    "                            num_topics=20, \n",
    "                            num_keywords=8,\n",
    "                            period_start=\"2018-12-31 00:00:00\",\n",
    "                            period_end=\"2019-01-01 00:00:00\")\n",
    "api_response = api_instance.post_topic_summary_api(payload)        \n",
    "    \n",
    "for res in api_response.result:\n",
    "    print('Topic', i, 'summary:')\n",
    "    print('    Keywords:', res.topic)\n",
    "    for j in range(len(res.summary)):\n",
    "        print(res.summary[j])\n",
    "        print('    Document ID:', res.summary[j].sourceid)\n",
    "        print('        Title:', res.summary[j].title)\n",
    "        print('        Sentences:', res.summary[j].sentences)\n",
    "        print('        Author:', res.summary[j].attribute['author'])\n",
    "        print('        Time:', datetime.datetime.fromtimestamp(float(res.summary[j].attribute['time'])))   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   You can drill down on some influencers or interesting emerging contributors by leveraging the author connectivity analysis: who is most similar to that person based on the topics they participate in and the nature of their participation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('----------------- Get author connectivity -------------------')\n",
    "\n",
    "payload = nucleus_api.AuthorConnection(dataset='Twitter_feed', \n",
    "                                        target_author='Yann LeCun', \n",
    "                                        query='',\n",
    "                                        period_start=\"2018-12-31 00:00:00\",\n",
    "                                        period_end=\"2019-01-01 00:00:00\")\n",
    "api_response = api_instance.post_author_connectivity_api(payload)    \n",
    "\n",
    "print('Mainstream connections:')\n",
    "for mc in api_response.result.mainstream_connections:\n",
    "    print('    Topic:', mc.keywords)\n",
    "    print('    Authors:', \" \".join(str(x) for x in mc.authors))\n",
    "    \n",
    "print('Niche connections:')\n",
    "for nc in api_response.result.niche_connections:\n",
    "    print('    Topic:', nc.keywords)\n",
    "    print('    Authors:', \" \".join(str(x) for x in nc.authors))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-\tFurther down, we discuss how to refine the content analysis by leveraging the different parameters available to the user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.\tFine Tuning\n",
    "\n",
    "#### a.\tTailoring the topics\n",
    "-\tSee whether some tailoring may be applied to your content analysis by excluding certain topics considered not differentiated. This is achieved by using the custom_stop_words parameter in input to the Topic Historical Analysis API\n",
    "\n",
    "\n",
    "-\tIdentify and Extract key topics on your corpus and print their keywords\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('------------- Get the recent topics in your feed --------------')\n",
    "\n",
    "payload = nucleus_api.Topics(dataset='Twitter_feed',                         \n",
    "                            query='',                       \n",
    "                            num_topics=20, \n",
    "                            num_keywords=8,\n",
    "                            period_start=\"2018-12-31 00:00:00\",\n",
    "                            period_end=\"2019-01-01 00:00:00\")\n",
    "api_response = api_instance.post_topic_api(payload)        \n",
    "    \n",
    "for i, res in enumerate(api_response.result.topics):\n",
    "    print('Topic', i, ' keywords: ', res.keywords)    \n",
    "    print('---------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can then tailor the content analysis by creating a custom_stop_words variable. Initialize the variable as follows, for instance, and pass it in the payload of the main code of section 2: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_stop_words = [\"supervised learning\", \"training\"] # str | List of stop words. (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b.\tFocusing the content analysis on certain subjects\n",
    "In case you decide to focus the content analysis, for instance on deep-learning subjects, simply substitute the query variable in the main code of section 2. with: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '(deep-learning OR LSTM OR RNN OR Neural network)' # str | Fulltext query, using mysql MATCH boolean query format. Example: \"(word1 OR word2) AND (word3 OR word4)\" (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c.\tExploring the impact of the type of documents, the lookback period, the number of topics being extracted\n",
    "**period_starts**: You can perform content analysis on any lookback you want, with granularities ranging from intraday to monthly. Depending on your objectives, such options give you the flexibility to slice the data in the most relevant time horizon.\n",
    "\n",
    "**Document types**: You can investigate how topics and their evolution over time change, based on the types of sources contributing to your content. Whether it is sources in different languages, or contributors from academia, the private sector or independent individuals, as long as your corpus has that info available, dicing and slicing is a piece of cake thanks to the metadata selector provided during the construction of the dataset. Rerun the main code of section 2. on a subset of the whole corpus. Create a variable metadata_selection and pass it in to the payload (works if using your docs or the Central Bank feed, News Media RSS feed doesn't have metadata that can be selected):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_selection = {\"category\": \"Academia\"}   # str | json object of {\\\"metadata_field\\\":[\\\"selected_values\\\"]} (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.\tNext Steps\n",
    "-\tPossible extension: build an internal BI report pulling out top topics and key take-aways\n",
    "\n",
    "\n",
    "-\tPossible extension: compare internal from your domain-expert teams and external discussions to support competitive landscape intelligence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) 2019 SumUp Analytics, Inc. All Rights Reserved.\n",
    "\n",
    "NOTICE: All information contained herein is, and remains the property of SumUp Analytics Inc. and its suppliers, if any. The intellectual and technical concepts contained herein are proprietary to SumUp Analytics Inc. and its suppliers and may be covered by U.S. and Foreign Patents, patents in process, and are protected by trade secret or copyright law.\n",
    "\n",
    "Dissemination of this information or reproduction of this material is strictly forbidden unless prior written permission is obtained from SumUp Analytics Inc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
