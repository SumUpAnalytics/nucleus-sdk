{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>  Single Names' ESG Scoring - Nucleus APIs Use Cases</center></h1>\n",
    "\n",
    "\n",
    "<h1><center>  SumUp Analytics, Proprietary & Confidential</center></h1>\n",
    "<h1><center>  Disclaimers and Terms of Service available at www.sumup.ai</center></h1>\n",
    "\n",
    "\n",
    "#  \n",
    "\n",
    " \n",
    "\n",
    "\n",
    "## Objective: \n",
    "-\tDevelop an ESG scoring methodology using content published by corporations\n",
    "\n",
    "\n",
    "## Data:\n",
    "-\tA chosen list of corporations, for instance within the same industry sector, or with similar market capitalization\n",
    " - \tCompany reports such as SEC filings\n",
    " - \tPress releases\n",
    " - \tEarning call transcripts\n",
    "\n",
    "\n",
    "\n",
    "## Nucleus APIs used:\n",
    "-\tDataset creation API\n",
    " - \t*api_instance.post_upload_file(file, dataset)*\n",
    " - \t*nucleus_helper.import_files(api_instance, dataset, file_iters, processes=1)*\n",
    "\n",
    "        nucleus_helper.import_files leverages api_instance.post_upload_file with parallel execution to speed-up the dataset creation\n",
    "\n",
    "\n",
    "-\tTopic Modeling API\n",
    " - \t*api_instance.post_topic_api(payload)*\n",
    "\n",
    "\n",
    "-\tTopic Sentiment API\n",
    " - \t*api_instance.post_topic_sentiment_api(payload)*\n",
    "\n",
    "\n",
    "-\tDocInfo API\n",
    " - \t*api_instance.post_doc_info(payload)*\n",
    "\n",
    "\n",
    "-\tDatasetInfo API\n",
    " - \t*api_instance.post_dataset_info(payload)*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach:\n",
    "\n",
    "### 1.\tDataset Preparation\n",
    "-\tCreate a Nucleus dataset containing all relevant documents over a chosen historical period\n",
    "-   Leverage the corporate filings' feed provided by Nucleus\n",
    "-   Focus on the Information Technology sector for this example\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import regex as re\n",
    "import numpy as np\n",
    "import datetime\n",
    "import nucleus_api.api.nucleus_api as nucleus_helper\n",
    "import nucleus_api\n",
    "from nucleus_api.rest import ApiException\n",
    "\n",
    "configuration = nucleus_api.Configuration()\n",
    "configuration.host = 'UPDATE-WITH-API-SERVER-HOSTNAME'\n",
    "configuration.api_key['x-api-key'] = 'UPDATE-WITH-API-KEY'\n",
    "\n",
    "# Create API instance\n",
    "api_instance = nucleus_api.NucleusApi(nucleus_api.ApiClient(configuration))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"Corporate_docs\" \n",
    "period_start = \"2015-01-01\" \n",
    "period_end= \"2019-06-01\"\n",
    "tickers = ['AAPL','MSFT','INTC','CSCO','MA','ORCL','IBM','CRM','PYPL','ACN US','ADBE','TXN','NVDA','INTU','QCOM','ADSK','CTSH','XLNX','HPQ','SPLK','TEL US','HPE','FISV','AMD','LRCX','MCHP','DXC','NOW','SYMC','ON','CDW','AKAM','FIS','NTAP','MXIM','DELL','ADS','VRSN','JNPR','LDOS','ANET','TER','GPN','TSS','IT','GDDY','CTXS','FTNT','DATA','ZBRA','WU','TYL','PAYC','CGNX','DOX']\n",
    "\n",
    "payload = nucleus_api.EdgarQuery(destination_dataset=dataset,\n",
    "                                tickers=tickers, \n",
    "                                filing_types=[\"10-K\", \"10-K/A\", \"10-Q\", \"10-Q/A\", \"8-K\", \"8-K/A\"], \n",
    "                                sections=[],\n",
    "                                period_start=period_start,\n",
    "                                period_end=period_end)\n",
    "\n",
    "api_response = api_instance.post_create_dataset_from_sec_filings(payload)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**You can subsequently work on specific time periods within your dataset directly in the APIs, as illustrated below**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Define ESG queries to focus the content analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_E = \"Biodiversity OR Carbon OR Cleantech OR Clean OR Climate OR Coal OR Conservation OR Ecosystem OR Emission OR Energy OR Fuel OR Green OR Land OR Natural OR Pollution OR (Raw AND materials) OR Renewable OR Resources OR Sustainability OR Sustainable OR Toxic OR Waste OR Water\"\n",
    "\n",
    "query_S = \"Accident OR Adult entertainment OR Alcohol OR Anti-personnel OR Behavior OR Charity OR Child Labor OR Community OR Controversial OR Controversy OR Discrimination OR Gambling OR Health OR (Human AND capital) OR (Human AND rights) OR Inclusion OR Injury OR Labor OR Munitions OR Opposition OR Pay OR Philanthropic OR Quality OR Responsible\"\n",
    "\n",
    "query_G = \"Advocacy OR Bribery OR Compensation OR Competitive OR Corruption OR Data breach OR Divestment OR Fraud OR Global Compact OR GRI OR Global Reporting Initiative OR Independent OR Justice OR Stability OR Stewardship OR Transparency\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.\tRank companies on each ESG subject\n",
    "- Identify and Extract key topics at a given point in time on each of the 3 ESG subjects\n",
    "\n",
    "\n",
    "- Measure the sentiment on each topic to classify all key topics into ‘good’ and ‘bad’ topics\n",
    "\n",
    "\n",
    "- Determine the exposure of each company to each topic\n",
    "\n",
    "\n",
    "- Aggregate the exposures of a given company across key topics based on the ‘good’ or ‘bad’ nature of the topics, to derive a ranking of the companies\n",
    " - The top company is the one with the most exposure to good topics and/or the least exposure to bad topics\n",
    " \n",
    " \n",
    "- Further down, we discuss how to refine this analysis by leveraging the different parameters available to the user\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine which companies are associated to the documents contributing to the topics\n",
    "company_list = tickers\n",
    "\n",
    "\n",
    "print('-------- Get topic sentiment and exposure per firm ----------------')\n",
    "\n",
    "payload = nucleus_api.TopicSentimentModel(dataset='Corporate_docs',          \n",
    "                                query=query_E,                   \n",
    "                                num_topics=8,\n",
    "                                num_keywords=8,\n",
    "                                period_start = \"2015-01-01\" \n",
    "                                period_end= \"2015-03-01\")\n",
    "try:\n",
    "    api_response = api_instance.post_topic_sentiment_api(payload)    \n",
    "    api_ok = True\n",
    "except ApiException as e:\n",
    "    api_error = json.loads(e.body)\n",
    "    print('ERROR:', api_error['message'])\n",
    "    api_ok = False\n",
    "\n",
    "if api_ok:\n",
    "    company_rankings = np.zeros([len(company_list), len(api_response.result)])\n",
    "    for i, res in enumerate(api_response.result):\n",
    "        print('Topic', i, 'sentiment:')\n",
    "        print('    Keywords:', res.keywords)\n",
    "\n",
    "        # Aggregate all document exposures within a topic into a company exposure, using the dataset metadata\n",
    "        payload = nucleus_api.DocInfo(dataset='Corporate_docs', doc_ids = res.doc_ids)\n",
    "        try:\n",
    "            api_response1 = api_instance.post_doc_info(payload)\n",
    "            api_ok = True\n",
    "        except ApiException as e:\n",
    "            api_error = json.loads(e.body)\n",
    "            print('ERROR:', api_error['message'])\n",
    "            api_ok = False\n",
    "\n",
    "        if api_ok:\n",
    "            company_sources = [] # This list might shorter than the whole dataset because not all companies necessarily contribute to a given topic\n",
    "            for res1 in api_response1.result:        \n",
    "                company_sources.append(re.split('\\s',res1.attribute['filename'])[0]) \n",
    "\n",
    "            company_contributions = np.zeros([len(company_list), 1])\n",
    "            for j in range(len(company_list)):\n",
    "                for k in range(len(company_sources)):\n",
    "                    if company_sources[k] == company_list[j]:\n",
    "                        company_contributions[j] += json.loads(res.doc_topic_exposures[0])[k]\n",
    "\n",
    "            company_rankings[:, i] = [x[0] for x in  float(res.strength) * float(res.sentiment) * company_contributions[:]]    \n",
    "\n",
    "            print('---------------')\n",
    "\n",
    "\n",
    "    # Add up the ranking of companies per topic into the final ESG score on the subject (E, S, G) currently analyzed\n",
    "    ESG_score = np.mean(company_rankings, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-\tRepeat the above tasks for each date in the historical period to get the complete history of your ESG scores\n",
    "\n",
    "-   Change the query used: query_E, query_S, query_G to get scores per company on each of the 3 sustainability pillars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('------------ Retrieve all companies found in the dataset ----------')\n",
    "\n",
    "company_list = tickers\n",
    "\n",
    "\n",
    "print('--------------- Retrieve the time range of the dataset -------------')\n",
    "\n",
    "payload = nucleus_api.DatasetInfo(dataset='Corporate_docs', query='')\n",
    "api_response = api_instance.post_dataset_info(payload)\n",
    "\n",
    "first_date = datetime.datetime.fromtimestamp(float(api_response.result.time_range[0]))\n",
    "last_date = datetime.datetime.fromtimestamp(float(api_response.result.time_range[1]))\n",
    "delta = last_date - first_date\n",
    "\n",
    "# Now loop through time and at each date, compute the ranking of companies\n",
    "T = 180 # The look-back period in days\n",
    "\n",
    "ESG_score = []\n",
    "for i in range(delta.days):  \n",
    "    if i == 0:\n",
    "        end_date = first_date + datetime.timedelta(days=T)\n",
    " \n",
    "    # first and last date used for the lookback period of T days\n",
    "    start_date = end_date - datetime.timedelta(days=T)\n",
    "    start_date_str = start_date.strftime(\"%Y-%m-%d 00:00:00\")\n",
    "\n",
    "    # We want a daily indicator\n",
    "    end_date = end_date + datetime.timedelta(days=1) \n",
    "    end_date_str = end_date.strftime(\"%Y-%m-%d 00:00:00\")\n",
    "\n",
    "    payload = nucleus_api.TopicSentimentModel(dataset=\"Corporate_docs\",      \n",
    "                                query=query_E,                   \n",
    "                                num_topics=8,\n",
    "                                num_keywords=8,\n",
    "                                period_start=start_date_str,\n",
    "                                period_end=end_date_str)\n",
    "    api_response = api_instance.post_topic_sentiment_api(payload)\n",
    "\n",
    "    company_rankings = np.zeros([len(company_list), len(api_response.result)])\n",
    "    for l, res in enumerate(api_response.result):\n",
    "        # Aggregate all document exposures within a topic into a company exposure, using the dataset metadata\n",
    "        payload = nucleus_api.DocInfo(dataset='Corporate_docs', doc_ids=res.doc_ids)\n",
    "        api_response1 = api_instance.post_doc_info(payload)\n",
    "\n",
    "        company_sources = [] # This list will be much shorter than the whole dataset because not all documents contribute to a given topic\n",
    "        for res1 in api_response1.result:        \n",
    "            company_sources.append(re.split('\\s',res1.attribute['filename'])[0]) \n",
    "\n",
    "        company_contributions = np.zeros([len(company_list), 1])\n",
    "        for j in range(len(company_list)):\n",
    "            for k in range(len(company_sources)):\n",
    "                if company_sources[k] == company_list[j]:\n",
    "                    company_contributions[j] += json.loads(res.doc_topic_exposures[0])[k]\n",
    "\n",
    "        company_rankings[:, l] = [x[0] for x in  float(res.strength) * float(res.sentiment) * company_contributions[:]]     \n",
    "\n",
    "    # Add up the ranking of companies per topic into the final ESG score on the subject (E, S, G) currently analyzed\n",
    "    ESG_score.append(np.mean(company_rankings, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.\tResults Interpretation\n",
    "-\tPlot the time series of companies' ESG score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.\tFine Tuning\n",
    "\n",
    "#### a.\tTailoring the topics\n",
    "-\tSee whether some tailoring may be applied to your single name screen by excluding certain topics considered not impactful. This is achieved by using the custom_stop_words parameter in input to the Topic Sentiment API\n",
    "\n",
    "\n",
    "-\tIdentify and Extract key topics on the corpus, for each of the 3 subjects (E, S, G) and print the corresponding keywords\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('------------- Get list of topics from dataset --------------')\n",
    "\n",
    "payload = nucleus_api.Topics(dataset='Corporate_docs',                       \n",
    "                                query=query_E,                       \n",
    "                                num_topics=20, \n",
    "                                num_keywords=8,\n",
    "                                period_start=\"2015-01-01\",\n",
    "                                period_end=\"2019-06-01\")\n",
    "try:\n",
    "    api_response = api_instance.post_topic_api(payload)        \n",
    "    api_ok = True\n",
    "except ApiException as e:\n",
    "    api_error = json.loads(e.body)\n",
    "    print('ERROR:', api_error['message'])\n",
    "    api_ok = False\n",
    "\n",
    "if api_ok:    \n",
    "    for i, res in enumerate(api_response.result.topics):\n",
    "        print('Topic', i, ' keywords: ', res.keywords)    \n",
    "        print('---------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can then tailor the scoring analysis by creating a custom_stop_words variable. Initialize the variable as follows, for instance, and pass it in the payload of the main code of section 2: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_stop_words = [\"call\",\"report\"] # str | List of stop words. (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b.\tExploring the impact of the type of documents, the lookback period, the number of topics being extracted\n",
    "**num_topics**: You can compute the companies' ESG score using different breadth of topics by changing the variable num_topics in the payload in the main code of section 2. A larger value will provide more breadth in establishing scores while a smaller value will provide a shallower measure. If num_topics is too large, some very marginal topics may bring in a lot of noise in measuring company ESG scores.\n",
    "\n",
    "**T**: You can compute the companies' ESG score with different speeds of propagation by changing the variable T (lookback) in the main code of section 2. A larger value will provide a slowly changing ESG score while a smaller value will lead to a very responsive scoring. If T is too small, too few documents may be used and this may lead to a lot of noise in scoring companies. If T is too long, the ESG scores won’t reflect quickly enough important new information. \n",
    "\n",
    "**Document types**: You can investigate how the companies' ESG score changes if it is measured using only one type of document among the different kinds of company filings by leveraging the metadata selector provided during the construction of the dataset. Rerun the main code of section 2. on a subset of the whole corpus. Create a variable metadata_selection and pass it in to the payload:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_selection = {\"category\": \"Report\"}   # str | json object of {\\\"metadata_field\\\":[\\\"selected_values\\\"]} (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.\tNext Steps\n",
    "-\tPossible extension: repeat the above tasks for different industry sectors\n",
    "\n",
    "-\tPossible extension: transform the raw ESG scores into a normalized metric\n",
    "\n",
    "        Score(Company i) = ( Rank(Company i) – Average(Ranks, [Companies]) ) / Std(Ranks, [Companies])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) 2019 SumUp Analytics, Inc. All Rights Reserved.\n",
    "\n",
    "NOTICE: All information contained herein is, and remains the property of SumUp Analytics Inc. and its suppliers, if any. The intellectual and technical concepts contained herein are proprietary to SumUp Analytics Inc. and its suppliers and may be covered by U.S. and Foreign Patents, patents in process, and are protected by trade secret or copyright law.\n",
    "\n",
    "Dissemination of this information or reproduction of this material is strictly forbidden unless prior written permission is obtained from SumUp Analytics Inc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
